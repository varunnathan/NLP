{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "LOCAL_DATA_ROOT = '/Users/varunn/Documents/projects/pretrained_word_vectors/'\n",
    "GLOVE_PATH = LOCAL_DATA_ROOT + 'glove.6B/'\n",
    "GLOVE_INP_FN = GLOVE_PATH + 'glove.6B.50d.txt'\n",
    "OUT_PATH = '/Users/varunn/Documents/NLP-data/'\n",
    "VOCAB_FILE = OUT_PATH+'vocabulary_file_w2v.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      "\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
      "\n",
      ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
      "\n",
      "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
      "\n",
      "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for line in open(GLOVE_INP_FN):\n",
    "    if count <= 5:\n",
    "        print(line)\n",
    "    else:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(object):\n",
    "    def __init__(self,vocab_file,vectors_file, vocab_flag=True):\n",
    "        if vocab_flag:\n",
    "            words = []\n",
    "            with open(vocab_file, 'r') as f:\n",
    "                lines = [x.rstrip().split('\\n') for x in f.readlines()]\n",
    "                lines = [x[0] for x in lines]\n",
    "                for line in lines:\n",
    "                    current_words = line.split(' ')\n",
    "                    words = list(set(words) | set(current_words))\n",
    "            \n",
    "\n",
    "        with open(vectors_file, 'r') as f:\n",
    "            vectors = {}\n",
    "            for line in f:\n",
    "                vals = line.rstrip().split(' ')\n",
    "                vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "        \n",
    "        \n",
    "        if not vocab_flag:\n",
    "            words = vectors.keys()\n",
    "        \n",
    "        vocab_size = len(words)\n",
    "        vocab = {w: idx for idx, w in enumerate(words)}\n",
    "        ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "        vector_dim = len(vectors[ivocab[0]])\n",
    "        W = np.zeros((vocab_size, vector_dim))\n",
    "        for word, v in vectors.items():\n",
    "            if (word == '<unk>') | (word not in vocab):\n",
    "                continue\n",
    "            W[vocab[word], :] = v\n",
    "            \n",
    "\n",
    "        # normalize each word vector to unit variance\n",
    "        W_norm = np.zeros(W.shape)\n",
    "        d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "        W_norm = (W.T / d).T\n",
    "        \n",
    "        if vocab_flag:\n",
    "            for i in range(W.shape[0]):\n",
    "                x = W[i, :]\n",
    "                if sum(x) == 0:\n",
    "                    W_norm[i, :] = W[i, :]\n",
    "\n",
    "        self.W = W_norm\n",
    "        self.vocab = vocab\n",
    "        self.ivocab = ivocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(embed,text,refs):\n",
    "\n",
    "    C = np.zeros((len(refs),embed.W.shape[1]))\n",
    "\n",
    "    for idx, term in enumerate(refs):\n",
    "        if term in embed.vocab:\n",
    "            C[idx,:] = embed.W[embed.vocab[term], :]\n",
    "\n",
    "\n",
    "    tokens = text.split(' ')\n",
    "    scores = [0.] * len(tokens)\n",
    "\n",
    "    for idx, term in enumerate(tokens):\n",
    "        if term in embed.vocab:\n",
    "            vec = embed.W[embed.vocab[term], :]\n",
    "            cosines = np.dot(C,vec.T)\n",
    "            score = np.mean(cosines)\n",
    "            scores[idx] = score\n",
    "    \n",
    "    print(scores)\n",
    "\n",
    "    return tokens[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"i am looking for a place in the north of town\",\n",
    "            \"looking for indian restaurants\",\n",
    "            \"Indian wants to go to an italian restaurant\",\n",
    "            \"show me chinese restaurants\",\n",
    "            \"show me chines restaurants in the north\",\n",
    "            \"show me a mexican place in the centre\",\n",
    "            \"i am looking for an indian spot called olaolaolaolaolaola\",\n",
    "            \"search for restaurants\",\n",
    "            \"anywhere in the west\",\n",
    "            \"anywhere near 18328\",\n",
    "            \"I am looking for asian fusion food\",\n",
    "            \"I am looking a restaurant in 29432\",\n",
    "            \"I am looking for mexican indian fusion\",\n",
    "            \"central indian restaurant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [x.lower() for x in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = open(OUT_PATH+'vocabulary_file_w2v.txt', 'w')\n",
    "for example in examples:\n",
    "    fn.write(example)\n",
    "    fn.write('\\n')\n",
    "fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(VOCAB_FILE, GLOVE_INP_FN, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(embed.W.shape)\n",
    "print(len(embed.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example1 = 'looking for spanish restaurants'\n",
    "test_example2 = 'looking for indian restaurants'\n",
    "test_example3 = 'looking for south indian restaurants'\n",
    "test_example4 = 'I want to find a chettinad restaurant'\n",
    "test_example5 = 'chinese man looking for a indian restaurant'\n",
    "refs = [\"mexican\",\"chinese\",\"french\",\"british\",\"american\"]\n",
    "threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  looking for spanish restaurants\n",
      "[0.5009760990196709, 0.5917668769198282, 0.651118669525621, 0.3510683709582437]\n",
      "spanish\n",
      "\n",
      "\n",
      "text:  looking for indian restaurants\n",
      "[0.5009760990196709, 0.5917668769198282, 0.5738119115027638, 0.3510683709582437]\n",
      "for\n",
      "\n",
      "\n",
      "text:  looking for south indian restaurants\n",
      "[0.5009760990196709, 0.5917668769198282, 0.5489459404006024, 0.5738119115027638, 0.3510683709582437]\n",
      "for\n",
      "\n",
      "\n",
      "text:  i want to find a chettinad restaurant\n",
      "[0.36844053629464035, 0.4487847824103165, 0.560535816434504, 0.4564699816910438, 0.5600945573541087, -0.18182957623293405, 0.3889199208317097]\n",
      "to\n",
      "\n",
      "\n",
      "text:  chinese man looking for a indian restaurant\n",
      "[0.6660608244765405, 0.512190629636525, 0.5009760990196709, 0.5917668769198282, 0.5600945573541087, 0.5738119115027638, 0.3889199208317097]\n",
      "chinese\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With stopwords\n",
    "for example in [test_example1, test_example2, test_example3,\n",
    "                test_example4, test_example5]:\n",
    "    example = example.lower()\n",
    "    print('text: ', example)\n",
    "    print(find_similar_words(embed,example,refs))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  looking for spanish restaurants\n",
      "[0.5009760990196709, 0.651118669525621, 0.3510683709582437]\n",
      "spanish\n",
      "\n",
      "\n",
      "text:  looking for indian restaurants\n",
      "[0.5009760990196709, 0.5738119115027638, 0.3510683709582437]\n",
      "indian\n",
      "\n",
      "\n",
      "text:  looking for south indian restaurants\n",
      "[0.5009760990196709, 0.5489459404006024, 0.5738119115027638, 0.3510683709582437]\n",
      "indian\n",
      "\n",
      "\n",
      "text:  I want to find a chettinad restaurant\n",
      "[0.36844053629464035, 0.4487847824103165, 0.4564699816910438, -0.18182957623293405, 0.3889199208317097]\n",
      "find\n",
      "\n",
      "\n",
      "text:  chinese man looking for a indian restaurant\n",
      "[0.6660608244765405, 0.512190629636525, 0.5009760990196709, 0.5738119115027638, 0.3889199208317097]\n",
      "chinese\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "for example in [test_example1, test_example2, test_example3,\n",
    "                test_example4, test_example5]:\n",
    "    print('text: ', example)\n",
    "    example = \" \".join([x.lower() for x in nltk.word_tokenize(example)\n",
    "               if x not in stop])\n",
    "    print(find_similar_words(embed,example,refs))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3736672532256827, 0.45149458923102903]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'food'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_words(embed, 'fish food', refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sum_vecs(embed,text):\n",
    "\n",
    "    tokens = text.split(' ')\n",
    "    vec = np.zeros(embed.W.shape[1])\n",
    "\n",
    "    for idx, term in enumerate(tokens):\n",
    "        if term in embed.vocab:\n",
    "            vec = vec + embed.W[embed.vocab[term], :]\n",
    "    return vec\n",
    "\n",
    "\n",
    "def get_centroid(embed,examples):\n",
    "\n",
    "    C = np.zeros((len(examples),embed.W.shape[1]))\n",
    "    for idx, text in enumerate(examples):\n",
    "        C[idx,:] = sum_vecs(embed,text)\n",
    "\n",
    "    centroid = np.mean(C,axis=0)\n",
    "    assert centroid.shape[0] == embed.W.shape[1]\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def get_intent(embed,text):\n",
    "    intents = ['deny', 'inform', 'greet']\n",
    "    vec = sum_vecs(embed,text)\n",
    "    scores = np.array([ np.linalg.norm(vec-data[label][\"centroid\"]) for label in intents ])\n",
    "    return intents[np.argmin(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "  \"greet\": {\n",
    "    \"examples\" : [\"hello\",\"hey there\",\"howdy\",\"hello\",\"hi\",\"hey\",\"hey ho\"],\n",
    "    \"centroid\" : None\n",
    "  },\n",
    "  \"inform\": {\n",
    "    \"examples\" : [\n",
    "      \"i'd like something asian\",\n",
    "      \"maybe korean\",\n",
    "      \"what mexican options do i have\",\n",
    "      \"what italian options do i have\",\n",
    "      \"i want korean food\",\n",
    "      \"i want german food\",\n",
    "      \"i want vegetarian food\",\n",
    "      \"i would like chinese food\",\n",
    "      \"i would like indian food\",\n",
    "      \"what japanese options do i have\",\n",
    "      \"korean please\",\n",
    "      \"what about indian\",\n",
    "      \"i want some vegan food\",\n",
    "      \"maybe thai\",\n",
    "      \"i'd like something vegetarian\",\n",
    "      \"show me french restaurants\",\n",
    "      \"show me a cool malaysian spot\"\n",
    "    ],\n",
    "    \"centroid\" : None\n",
    "  },\n",
    "  \"deny\": {\n",
    "    \"examples\" : [\n",
    "      \"nah\",\n",
    "      \"any other places ?\",\n",
    "      \"anything else\",\n",
    "      \"no thanks\"\n",
    "      \"not that one\",\n",
    "      \"i do not like that place\",\n",
    "      \"something else please\",\n",
    "      \"no please show other options\"\n",
    "    ],\n",
    "    \"centroid\" : None\n",
    "  }\n",
    "}\n",
    "intents = ['greet', 'inform', 'deny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for intent in intents:\n",
    "    examples = list(set(examples) | set(data[intent]['examples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what mexican options do i have',\n",
       " 'howdy',\n",
       " 'hey',\n",
       " 'hello',\n",
       " 'something else please',\n",
       " 'i do not like that place',\n",
       " 'show me a cool malaysian spot',\n",
       " 'hey there',\n",
       " 'anything else',\n",
       " 'hey ho',\n",
       " \"i'd like something asian\",\n",
       " 'i would like chinese food',\n",
       " 'maybe korean',\n",
       " 'no please show other options',\n",
       " 'what japanese options do i have',\n",
       " 'no thanksnot that one',\n",
       " 'what about indian',\n",
       " 'show me french restaurants',\n",
       " 'i want german food',\n",
       " 'i want korean food',\n",
       " 'hi',\n",
       " 'what italian options do i have',\n",
       " 'i would like indian food',\n",
       " \"i'd like something vegetarian\",\n",
       " 'i want vegetarian food',\n",
       " 'maybe thai',\n",
       " 'korean please',\n",
       " 'any other places ?',\n",
       " 'nah',\n",
       " 'i want some vegan food']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = open(VOCAB_FILE, 'w')\n",
    "for example in examples:\n",
    "    fn.write(example)\n",
    "    fn.write('\\n')\n",
    "fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(VOCAB_FILE, GLOVE_INP_FN, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in data.keys():\n",
    "    data[label][\"centroid\"] = get_centroid(embed,data[label][\"examples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greet': {'examples': ['hello',\n",
       "   'hey there',\n",
       "   'howdy',\n",
       "   'hello',\n",
       "   'hi',\n",
       "   'hey',\n",
       "   'hey ho'],\n",
       "  'centroid': array([-0.11797034,  0.08401492,  0.03852101,  0.00721597, -0.03044195,\n",
       "         -0.16339493, -0.05345686, -0.03382873, -0.14831921,  0.09320638,\n",
       "         -0.05309111,  0.13648676, -0.04040799,  0.04370132,  0.15911899,\n",
       "          0.06206383, -0.05080601,  0.18281922,  0.01270968,  0.00780746,\n",
       "         -0.03542216,  0.11279987,  0.11048912,  0.07726852,  0.23467205,\n",
       "         -0.23460631, -0.2877324 ,  0.07160929,  0.09418186, -0.34342254,\n",
       "          0.31013594,  0.13606241, -0.05841406,  0.2051403 , -0.09657553,\n",
       "         -0.11910098,  0.16374698, -0.15650952,  0.05164113, -0.07241123,\n",
       "         -0.0364207 , -0.0508396 , -0.06296611, -0.0796311 ,  0.19861918,\n",
       "         -0.04323929, -0.01389304, -0.17603198,  0.04052346,  0.23976296])},\n",
       " 'inform': {'examples': [\"i'd like something asian\",\n",
       "   'maybe korean',\n",
       "   'what mexican options do i have',\n",
       "   'what italian options do i have',\n",
       "   'i want korean food',\n",
       "   'i want german food',\n",
       "   'i want vegetarian food',\n",
       "   'i would like chinese food',\n",
       "   'i would like indian food',\n",
       "   'what japanese options do i have',\n",
       "   'korean please',\n",
       "   'what about indian',\n",
       "   'i want some vegan food',\n",
       "   'maybe thai',\n",
       "   \"i'd like something vegetarian\",\n",
       "   'show me french restaurants',\n",
       "   'show me a cool malaysian spot'],\n",
       "  'centroid': array([ 0.1555521 , -0.02108789, -0.14513978, -0.12259792,  0.3162827 ,\n",
       "         -0.17137175, -0.40437815, -0.09566605, -0.13236292,  0.10580138,\n",
       "         -0.0300831 ,  0.22453288, -0.04389567,  0.06529948,  0.56950135,\n",
       "          0.26703334,  0.13359032,  0.12993634,  0.02995608, -0.44238923,\n",
       "          0.03086497,  0.2495694 ,  0.2573245 ,  0.21948988,  0.2424272 ,\n",
       "         -1.22154919, -0.50629826,  0.04717438,  0.34704935, -0.28109232,\n",
       "          2.33564122,  0.33719759, -0.27630875,  0.0693121 , -0.03037955,\n",
       "         -0.05168706, -0.08064648,  0.12601397,  0.02131645, -0.16921442,\n",
       "         -0.14133434,  0.14087379,  0.1864337 ,  0.20517834,  0.34549346,\n",
       "          0.10357449, -0.22482242,  0.07841975, -0.00566505,  0.23514807])},\n",
       " 'deny': {'examples': ['nah',\n",
       "   'any other places ?',\n",
       "   'anything else',\n",
       "   'no thanksnot that one',\n",
       "   'i do not like that place',\n",
       "   'something else please',\n",
       "   'no please show other options'],\n",
       "  'centroid': array([ 0.20539887,  0.16482666,  0.03183876, -0.06549312,  0.33915183,\n",
       "         -0.02544313, -0.24000428, -0.02444102, -0.16088685,  0.07229542,\n",
       "         -0.13418871,  0.28559605, -0.19338645, -0.04858619,  0.52928517,\n",
       "          0.31854344,  0.19980334, -0.11265629,  0.03588895, -0.46018739,\n",
       "         -0.09121007,  0.16436104,  0.28606739,  0.18906707,  0.21662168,\n",
       "         -1.11397861, -0.53883262,  0.07454807,  0.46598367, -0.4066662 ,\n",
       "          2.06707704,  0.26613908, -0.29143802, -0.24212888,  0.02228303,\n",
       "         -0.06635804,  0.11575195,  0.00403351,  0.02738912, -0.0673154 ,\n",
       "         -0.0812974 ,  0.12898419,  0.09840715,  0.31666855,  0.06359715,\n",
       "          0.0550379 , -0.15188525,  0.05681737,  0.05487221,  0.25598127])}}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : 'hey you', predicted_label : 'greet'\n",
      "text : 'i am looking for chinese food', predicted_label : 'inform'\n",
      "text : 'not for me', predicted_label : 'deny'\n"
     ]
    }
   ],
   "source": [
    "for text in [\"hey you\",\"i am looking for chinese food\",\"not for me\"]:\n",
    "    print(\"text : '{0}', predicted_label : '{1}'\".format(text,get_intent(embed,text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : 'how do you do', predicted_label : 'deny'\n"
     ]
    }
   ],
   "source": [
    "text = \"how do you do\"\n",
    "print(\"text : '{0}', predicted_label : '{1}'\".format(text,get_intent(embed,text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
