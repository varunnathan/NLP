{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Optional, Text, Tuple\n",
    "\n",
    "from rasa_nlu.classifiers import INTENT_RANKING_LENGTH\n",
    "from rasa_nlu.components import Component\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if typing.TYPE_CHECKING:\n",
    "    import tensorflow as tf\n",
    "    from rasa_nlu.config import RasaNLUModelConfig\n",
    "    from rasa_nlu.training_data import TrainingData\n",
    "    from rasa_nlu.model import Metadata\n",
    "    from rasa_nlu.training_data import Message\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    tf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingIntentClassifier(Component):\n",
    "    \"\"\"Intent classifier using supervised embeddings.\n",
    "    The embedding intent classifier embeds user inputs\n",
    "    and intent labels into the same space.\n",
    "    Supervised embeddings are trained by maximizing similarity between them.\n",
    "    It also provides rankings of the labels that did not \"win\".\n",
    "    The embedding intent classifier needs to be preceded by\n",
    "    a featurizer in the pipeline.\n",
    "    This featurizer creates the features used for the embeddings.\n",
    "    It is recommended to use ``CountVectorsFeaturizer`` that\n",
    "    can be optionally preceded by ``SpacyNLP`` and ``SpacyTokenizer``.\n",
    "    Based on the starspace idea from: https://arxiv.org/abs/1709.03856.\n",
    "    However, in this implementation the `mu` parameter is treated differently\n",
    "    and additional hidden layers are added together with dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    provides = [\"intent\", \"intent_ranking\"]\n",
    "\n",
    "    requires = [\"text_features\"]\n",
    "\n",
    "    defaults = {\n",
    "        # nn architecture\n",
    "        # sizes of hidden layers before the embedding layer for input words\n",
    "        # the number of hidden layers is thus equal to the length of this list\n",
    "        \"hidden_layers_sizes_a\": [256, 128],\n",
    "        # sizes of hidden layers before the embedding layer for intent labels\n",
    "        # the number of hidden layers is thus equal to the length of this list\n",
    "        \"hidden_layers_sizes_b\": [],\n",
    "\n",
    "        # training parameters\n",
    "        # initial and final batch sizes - batch size will be\n",
    "        # linearly increased for each epoch\n",
    "        \"batch_size\": [64, 256],\n",
    "        # number of epochs\n",
    "        \"epochs\": 30,\n",
    "\n",
    "        # embedding parameters\n",
    "        # dimension size of embedding vectors\n",
    "        \"embed_dim\": 20,\n",
    "        # how similar the algorithm should try\n",
    "        # to make embedding vectors for correct intent labels\n",
    "        \"mu_pos\": 0.8,  # should be 0.0 < ... < 1.0 for 'cosine'\n",
    "        # maximum negative similarity for incorrect intent labels\n",
    "        \"mu_neg\": -0.4,  # should be -1.0 < ... < 1.0 for 'cosine'\n",
    "        # the type of the similarity\n",
    "        \"similarity_type\": 'cosine',  # string 'cosine' or 'inner'\n",
    "        # the number of incorrect intents, the algorithm will minimize\n",
    "        # their similarity to the input words during training\n",
    "        \"num_neg\": 20,\n",
    "        # flag: if true, only minimize the maximum similarity for\n",
    "        # incorrect intent labels\n",
    "        \"use_max_sim_neg\": True,\n",
    "        # set random seed to any int to get reproducible results\n",
    "        # try to change to another int if you are not getting good results\n",
    "        \"random_seed\": 1,\n",
    "\n",
    "        # regularization parameters\n",
    "        # the scale of L2 regularization\n",
    "        \"C2\": 0.002,\n",
    "        # the scale of how critical the algorithm should be of minimizing the\n",
    "        # maximum similarity between embeddings of different intent labels\n",
    "        \"C_emb\": 0.8,\n",
    "        # dropout rate for rnn\n",
    "        \"droprate\": 0.2,\n",
    "\n",
    "        # flag: if true, the algorithm will split the intent labels into tokens\n",
    "        #       and use bag-of-words representations for them\n",
    "        \"intent_tokenization_flag\": False,\n",
    "        # delimiter string to split the intent labels\n",
    "        \"intent_split_symbol\": '_',\n",
    "\n",
    "        # visualization of accuracy\n",
    "        # how often to calculate training accuracy\n",
    "        \"evaluate_every_num_epochs\": 10,  # small values may hurt performance\n",
    "        # how many examples to use for calculation of training accuracy\n",
    "        \"evaluate_on_num_examples\": 1000  # large values may hurt performance\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 component_config: Optional[Dict[Text, Any]] = None,\n",
    "                 inv_intent_dict: Optional[Dict[int, Text]] = None,\n",
    "                 encoded_all_intents: Optional[np.ndarray] = None,\n",
    "                 session: Optional['tf.Session'] = None,\n",
    "                 graph: Optional['tf.Graph'] = None,\n",
    "                 message_placeholder: Optional['tf.Tensor'] = None,\n",
    "                 intent_placeholder: Optional['tf.Tensor'] = None,\n",
    "                 similarity_op: Optional['tf.Tensor'] = None,\n",
    "                 word_embed: Optional['tf.Tensor'] = None,\n",
    "                 intent_embed: Optional['tf.Tensor'] = None\n",
    "                 ) -> None:\n",
    "        \"\"\"Declare instant variables with default values\"\"\"\n",
    "\n",
    "        self._check_tensorflow()\n",
    "        super(EmbeddingIntentClassifier, self).__init__(component_config)\n",
    "\n",
    "        self._load_params()\n",
    "\n",
    "        # transform numbers to intents\n",
    "        self.inv_intent_dict = inv_intent_dict\n",
    "        # encode all intents with numbers\n",
    "        self.encoded_all_intents = encoded_all_intents\n",
    "\n",
    "        # tf related instances\n",
    "        self.session = session\n",
    "        self.graph = graph\n",
    "        self.a_in = message_placeholder\n",
    "        self.b_in = intent_placeholder\n",
    "        self.sim_op = similarity_op\n",
    "\n",
    "        # persisted embeddings\n",
    "        self.word_embed = word_embed\n",
    "        self.intent_embed = intent_embed\n",
    "\n",
    "    # init helpers\n",
    "    def _load_nn_architecture_params(self, config: Dict[Text, Any]) -> None:\n",
    "        self.hidden_layer_sizes = {'a': config['hidden_layers_sizes_a'],\n",
    "                                   'b': config['hidden_layers_sizes_b']}\n",
    "\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.epochs = config['epochs']\n",
    "\n",
    "    def _load_embedding_params(self, config: Dict[Text, Any]) -> None:\n",
    "        self.embed_dim = config['embed_dim']\n",
    "        self.mu_pos = config['mu_pos']\n",
    "        self.mu_neg = config['mu_neg']\n",
    "        self.similarity_type = config['similarity_type']\n",
    "        self.num_neg = config['num_neg']\n",
    "        self.use_max_sim_neg = config['use_max_sim_neg']\n",
    "        self.random_seed = self.component_config['random_seed']\n",
    "\n",
    "    def _load_regularization_params(self, config: Dict[Text, Any]) -> None:\n",
    "        self.C2 = config['C2']\n",
    "        self.C_emb = config['C_emb']\n",
    "        self.droprate = config['droprate']\n",
    "\n",
    "    def _load_flag_if_tokenize_intents(self, config: Dict[Text, Any]) -> None:\n",
    "        self.intent_tokenization_flag = config['intent_tokenization_flag']\n",
    "        self.intent_split_symbol = config['intent_split_symbol']\n",
    "        if self.intent_tokenization_flag and not self.intent_split_symbol:\n",
    "            logger.warning(\"intent_split_symbol was not specified, \"\n",
    "                           \"so intent tokenization will be ignored\")\n",
    "            self.intent_tokenization_flag = False\n",
    "\n",
    "    def _load_visual_params(self, config: Dict[Text, Any]) -> None:\n",
    "        self.evaluate_every_num_epochs = config['evaluate_every_num_epochs']\n",
    "        if self.evaluate_every_num_epochs < 1:\n",
    "            self.evaluate_every_num_epochs = self.epochs\n",
    "\n",
    "        self.evaluate_on_num_examples = config['evaluate_on_num_examples']\n",
    "\n",
    "    def _load_params(self) -> None:\n",
    "\n",
    "        self._load_nn_architecture_params(self.component_config)\n",
    "        self._load_embedding_params(self.component_config)\n",
    "        self._load_regularization_params(self.component_config)\n",
    "        self._load_flag_if_tokenize_intents(self.component_config)\n",
    "        self._load_visual_params(self.component_config)\n",
    "\n",
    "    # package safety checks\n",
    "    @classmethod\n",
    "    def required_packages(cls) -> List[Text]:\n",
    "        return [\"tensorflow\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_tensorflow():\n",
    "        if tf is None:\n",
    "            raise ImportError(\n",
    "                'Failed to import `tensorflow`. '\n",
    "                'Please install `tensorflow`. '\n",
    "                'For example with `pip install tensorflow`.')\n",
    "\n",
    "    # training data helpers:\n",
    "    @staticmethod\n",
    "    def _create_intent_dict(training_data: 'TrainingData') -> Dict[Text, int]:\n",
    "        \"\"\"Create intent dictionary\"\"\"\n",
    "\n",
    "        distinct_intents = set([example.get(\"intent\")\n",
    "                                for example in training_data.intent_examples])\n",
    "        return {intent: idx\n",
    "                for idx, intent in enumerate(sorted(distinct_intents))}\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_intent_token_dict(intents: List[Text],\n",
    "                                  intent_split_symbol: Text) -> Dict[Text, int]:\n",
    "        \"\"\"Create intent token dictionary\"\"\"\n",
    "\n",
    "        distinct_tokens = set([token\n",
    "                               for intent in intents\n",
    "                               for token in intent.split(intent_split_symbol)])\n",
    "        return {token: idx\n",
    "                for idx, token in enumerate(sorted(distinct_tokens))}\n",
    "\n",
    "    def _create_encoded_intents(self,\n",
    "                                intent_dict: Dict[Text, int]) -> np.ndarray:\n",
    "        \"\"\"Create matrix with intents encoded in rows as bag of words.\n",
    "        If intent_tokenization_flag is off, returns identity matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.intent_tokenization_flag:\n",
    "            intent_token_dict = self._create_intent_token_dict(\n",
    "                list(intent_dict.keys()), self.intent_split_symbol)\n",
    "\n",
    "            encoded_all_intents = np.zeros((len(intent_dict),\n",
    "                                            len(intent_token_dict)))\n",
    "            for key, idx in intent_dict.items():\n",
    "                for t in key.split(self.intent_split_symbol):\n",
    "                    encoded_all_intents[idx, intent_token_dict[t]] = 1\n",
    "\n",
    "            return encoded_all_intents\n",
    "        else:\n",
    "            return np.eye(len(intent_dict))\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def _create_all_Y(self, size: int) -> np.ndarray:\n",
    "        \"\"\"Stack encoded_all_intents on top of each other\n",
    "        to create candidates for training examples and\n",
    "        to calculate training accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        return np.stack([self.encoded_all_intents] * size)\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def _prepare_data_for_training(\n",
    "            self,\n",
    "            training_data: 'TrainingData',\n",
    "            intent_dict: Dict[Text, int]\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "\n",
    "        X = np.stack([e.get(\"text_features\")\n",
    "                      for e in training_data.intent_examples])\n",
    "\n",
    "        intents_for_X = np.array([intent_dict[e.get(\"intent\")]\n",
    "                                  for e in training_data.intent_examples])\n",
    "\n",
    "        Y = np.stack([self.encoded_all_intents[intent_idx]\n",
    "                      for intent_idx in intents_for_X])\n",
    "\n",
    "        return X, Y, intents_for_X\n",
    "\n",
    "    # tf helpers:\n",
    "    def _create_tf_embed_nn(self, x_in: 'tf.Tensor', is_training: 'tf.Tensor',\n",
    "                            layer_sizes: List[int], name: Text) -> 'tf.Tensor':\n",
    "        \"\"\"Create nn with hidden layers and name\"\"\"\n",
    "\n",
    "        reg = tf.contrib.layers.l2_regularizer(self.C2)\n",
    "        x = x_in\n",
    "        for i, layer_size in enumerate(layer_sizes):\n",
    "            x = tf.layers.dense(inputs=x,\n",
    "                                units=layer_size,\n",
    "                                activation=tf.nn.relu,\n",
    "                                kernel_regularizer=reg,\n",
    "                                name='hidden_layer_{}_{}'.format(name, i))\n",
    "            x = tf.layers.dropout(x, rate=self.droprate, training=is_training)\n",
    "\n",
    "        x = tf.layers.dense(inputs=x,\n",
    "                            units=self.embed_dim,\n",
    "                            kernel_regularizer=reg,\n",
    "                            name='embed_layer_{}'.format(name))\n",
    "        return x\n",
    "\n",
    "    def _create_tf_embed(self,\n",
    "                         a_in: 'tf.Tensor',\n",
    "                         b_in: 'tf.Tensor',\n",
    "                         is_training: 'tf.Tensor'\n",
    "                         ) -> Tuple['tf.Tensor', 'tf.Tensor']:\n",
    "        \"\"\"Create tf graph for training\"\"\"\n",
    "\n",
    "        emb_a = self._create_tf_embed_nn(a_in, is_training,\n",
    "                                         self.hidden_layer_sizes['a'],\n",
    "                                         name='a')\n",
    "        emb_b = self._create_tf_embed_nn(b_in, is_training,\n",
    "                                         self.hidden_layer_sizes['b'],\n",
    "                                         name='b')\n",
    "        return emb_a, emb_b\n",
    "\n",
    "    def _tf_sim(self,\n",
    "                a: 'tf.Tensor',\n",
    "                b: 'tf.Tensor') -> Tuple['tf.Tensor', 'tf.Tensor']:\n",
    "        \"\"\"Define similarity\n",
    "        in two cases:\n",
    "            sim: between embedded words and embedded intent labels\n",
    "            sim_emb: between individual embedded intent labels only\n",
    "        \"\"\"\n",
    "\n",
    "        if self.similarity_type == 'cosine':\n",
    "            # normalize embedding vectors for cosine similarity\n",
    "            a = tf.nn.l2_normalize(a, -1)\n",
    "            b = tf.nn.l2_normalize(b, -1)\n",
    "\n",
    "        if self.similarity_type in {'cosine', 'inner'}:\n",
    "            sim = tf.reduce_sum(tf.expand_dims(a, 1) * b, -1)\n",
    "            sim_emb = tf.reduce_sum(b[:, 0:1, :] * b[:, 1:, :], -1)\n",
    "\n",
    "            return sim, sim_emb\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Wrong similarity type {}, \"\n",
    "                             \"should be 'cosine' or 'inner'\"\n",
    "                             \"\".format(self.similarity_type))\n",
    "\n",
    "    def _tf_loss(self, sim: 'tf.Tensor', sim_emb: 'tf.Tensor') -> 'tf.Tensor':\n",
    "        \"\"\"Define loss\"\"\"\n",
    "\n",
    "        # loss for maximizing similarity with correct action\n",
    "        loss = tf.maximum(0., self.mu_pos - sim[:, 0])\n",
    "\n",
    "        if self.use_max_sim_neg:\n",
    "            # minimize only maximum similarity over incorrect actions\n",
    "            max_sim_neg = tf.reduce_max(sim[:, 1:], -1)\n",
    "            loss += tf.maximum(0., self.mu_neg + max_sim_neg)\n",
    "        else:\n",
    "            # minimize all similarities with incorrect actions\n",
    "            max_margin = tf.maximum(0., self.mu_neg + sim[:, 1:])\n",
    "            loss += tf.reduce_sum(max_margin, -1)\n",
    "\n",
    "        # penalize max similarity between intent embeddings\n",
    "        max_sim_emb = tf.maximum(0., tf.reduce_max(sim_emb, -1))\n",
    "        loss += max_sim_emb * self.C_emb\n",
    "\n",
    "        # average the loss over the batch and add regularization losses\n",
    "        loss = (tf.reduce_mean(loss) + tf.losses.get_regularization_loss())\n",
    "        return loss\n",
    "\n",
    "    # training helpers:\n",
    "    def _create_batch_b(self, batch_pos_b: np.ndarray,\n",
    "                        intent_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create batch of intents.\n",
    "        Where the first is correct intent\n",
    "        and the rest are wrong intents sampled randomly\n",
    "        \"\"\"\n",
    "\n",
    "        batch_pos_b = batch_pos_b[:, np.newaxis, :]\n",
    "\n",
    "        # sample negatives\n",
    "        batch_neg_b = np.zeros((batch_pos_b.shape[0], self.num_neg,\n",
    "                                batch_pos_b.shape[-1]))\n",
    "        for b in range(batch_pos_b.shape[0]):\n",
    "            # create negative indexes out of possible ones\n",
    "            # except for correct index of b\n",
    "            negative_indexes = [i for i in\n",
    "                                range(self.encoded_all_intents.shape[0])\n",
    "                                if i != intent_ids[b]]\n",
    "            negs = np.random.choice(negative_indexes, size=self.num_neg)\n",
    "\n",
    "            batch_neg_b[b] = self.encoded_all_intents[negs]\n",
    "\n",
    "        return np.concatenate([batch_pos_b, batch_neg_b], 1)\n",
    "\n",
    "    def _linearly_increasing_batch_size(self, epoch: int) -> int:\n",
    "        \"\"\"Linearly increase batch size with every epoch.\n",
    "        The idea comes from https://arxiv.org/abs/1711.00489\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(self.batch_size, list):\n",
    "            return int(self.batch_size)\n",
    "\n",
    "        if self.epochs > 1:\n",
    "            return int(self.batch_size[0] +\n",
    "                       epoch * (self.batch_size[1] -\n",
    "                                self.batch_size[0]) / (self.epochs - 1))\n",
    "        else:\n",
    "            return int(self.batch_size[0])\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def _train_tf(self,\n",
    "                  X: np.ndarray,\n",
    "                  Y: np.ndarray,\n",
    "                  intents_for_X: np.ndarray,\n",
    "                  loss: 'tf.Tensor',\n",
    "                  is_training: 'tf.Tensor',\n",
    "                  train_op: 'tf.Tensor'\n",
    "                  ) -> None:\n",
    "        \"\"\"Train tf graph\"\"\"\n",
    "\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.evaluate_on_num_examples:\n",
    "            logger.info(\"Accuracy is updated every {} epochs\"\n",
    "                        \"\".format(self.evaluate_every_num_epochs))\n",
    "\n",
    "        pbar = tqdm(range(self.epochs), desc=\"Epochs\")\n",
    "        train_acc = 0\n",
    "        last_loss = 0\n",
    "        for ep in pbar:\n",
    "            indices = np.random.permutation(len(X))\n",
    "\n",
    "            batch_size = self._linearly_increasing_batch_size(ep)\n",
    "            batches_per_epoch = (len(X) // batch_size +\n",
    "                                 int(len(X) % batch_size > 0))\n",
    "\n",
    "            ep_loss = 0\n",
    "            for i in range(batches_per_epoch):\n",
    "                end_idx = (i + 1) * batch_size\n",
    "                start_idx = i * batch_size\n",
    "                batch_a = X[indices[start_idx:end_idx]]\n",
    "                batch_pos_b = Y[indices[start_idx:end_idx]]\n",
    "                intents_for_b = intents_for_X[indices[start_idx:end_idx]]\n",
    "                # add negatives\n",
    "                batch_b = self._create_batch_b(batch_pos_b, intents_for_b)\n",
    "\n",
    "                sess_out = self.session.run(\n",
    "                    {'loss': loss, 'train_op': train_op},\n",
    "                    feed_dict={self.a_in: batch_a,\n",
    "                               self.b_in: batch_b,\n",
    "                               is_training: True}\n",
    "                )\n",
    "                ep_loss += sess_out.get('loss') / batches_per_epoch\n",
    "\n",
    "            if self.evaluate_on_num_examples:\n",
    "                if (ep == 0 or\n",
    "                        (ep + 1) % self.evaluate_every_num_epochs == 0 or\n",
    "                        (ep + 1) == self.epochs):\n",
    "                    train_acc = self._output_training_stat(X, intents_for_X,\n",
    "                                                           is_training)\n",
    "                    last_loss = ep_loss\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": \"{:.3f}\".format(ep_loss),\n",
    "                    \"acc\": \"{:.3f}\".format(train_acc)\n",
    "                })\n",
    "            else:\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": \"{:.3f}\".format(ep_loss)\n",
    "                })\n",
    "\n",
    "        if self.evaluate_on_num_examples:\n",
    "            logger.info(\"Finished training embedding classifier, \"\n",
    "                        \"loss={:.3f}, train accuracy={:.3f}\"\n",
    "                        \"\".format(last_loss, train_acc))\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def _output_training_stat(self,\n",
    "                              X: np.ndarray,\n",
    "                              intents_for_X: np.ndarray,\n",
    "                              is_training: 'tf.Tensor') -> np.ndarray:\n",
    "        \"\"\"Output training statistics\"\"\"\n",
    "\n",
    "        n = self.evaluate_on_num_examples\n",
    "        ids = np.random.permutation(len(X))[:n]\n",
    "        all_Y = self._create_all_Y(X[ids].shape[0])\n",
    "\n",
    "        train_sim = self.session.run(self.sim_op,\n",
    "                                     feed_dict={self.a_in: X[ids],\n",
    "                                                self.b_in: all_Y,\n",
    "                                                is_training: False})\n",
    "\n",
    "        train_acc = np.mean(np.argmax(train_sim, -1) == intents_for_X[ids])\n",
    "        return train_acc\n",
    "\n",
    "    def train(self,\n",
    "              training_data: 'TrainingData',\n",
    "              cfg: Optional['RasaNLUModelConfig'] = None,\n",
    "              **kwargs: Any) -> None:\n",
    "        \"\"\"Train the embedding intent classifier on a data set.\"\"\"\n",
    "\n",
    "        intent_dict = self._create_intent_dict(training_data)\n",
    "        if len(intent_dict) < 2:\n",
    "            logger.error(\"Can not train an intent classifier. \"\n",
    "                         \"Need at least 2 different classes. \"\n",
    "                         \"Skipping training of intent classifier.\")\n",
    "            return\n",
    "\n",
    "        self.inv_intent_dict = {v: k for k, v in intent_dict.items()}\n",
    "        self.encoded_all_intents = self._create_encoded_intents(\n",
    "            intent_dict)\n",
    "\n",
    "        # noinspection PyPep8Naming\n",
    "        X, Y, intents_for_X = self._prepare_data_for_training(\n",
    "            training_data, intent_dict)\n",
    "\n",
    "        # check if number of negatives is less than number of intents\n",
    "        logger.debug(\"Check if num_neg {} is smaller than \"\n",
    "                     \"number of intents {}, \"\n",
    "                     \"else set num_neg to the number of intents - 1\"\n",
    "                     \"\".format(self.num_neg,\n",
    "                               self.encoded_all_intents.shape[0]))\n",
    "        self.num_neg = min(self.num_neg,\n",
    "                           self.encoded_all_intents.shape[0] - 1)\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # set random seed\n",
    "            np.random.seed(self.random_seed)\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.a_in = tf.placeholder(tf.float32, (None, X.shape[-1]),\n",
    "                                       name='a')\n",
    "            self.b_in = tf.placeholder(tf.float32, (None, None, Y.shape[-1]),\n",
    "                                       name='b')\n",
    "\n",
    "            is_training = tf.placeholder_with_default(False, shape=())\n",
    "\n",
    "            (self.word_embed,\n",
    "             self.intent_embed) = self._create_tf_embed(self.a_in, self.b_in,\n",
    "                                                        is_training)\n",
    "\n",
    "            self.sim_op, sim_emb = self._tf_sim(self.word_embed,\n",
    "                                                self.intent_embed)\n",
    "            loss = self._tf_loss(self.sim_op, sim_emb)\n",
    "\n",
    "            train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "            # train tensorflow graph\n",
    "            self.session = tf.Session()\n",
    "\n",
    "            self._train_tf(X, Y, intents_for_X,\n",
    "                           loss, is_training, train_op)\n",
    "\n",
    "    # process helpers\n",
    "    # noinspection PyPep8Naming\n",
    "    def _calculate_message_sim(self,\n",
    "                               X: np.ndarray,\n",
    "                               all_Y: np.ndarray\n",
    "                               ) -> Tuple[np.ndarray, List[float]]:\n",
    "        \"\"\"Load tf graph and calculate message similarities\"\"\"\n",
    "\n",
    "        message_sim = self.session.run(self.sim_op,\n",
    "                                       feed_dict={self.a_in: X,\n",
    "                                                  self.b_in: all_Y})\n",
    "        message_sim = message_sim.flatten()  # sim is a matrix\n",
    "\n",
    "        intent_ids = message_sim.argsort()[::-1]\n",
    "        message_sim[::-1].sort()\n",
    "\n",
    "        if self.similarity_type == 'cosine':\n",
    "            # clip negative values to zero\n",
    "            message_sim[message_sim < 0] = 0\n",
    "        elif self.similarity_type == 'inner':\n",
    "            # normalize result to [0, 1] with softmax\n",
    "            message_sim = np.exp(message_sim)\n",
    "            message_sim /= np.sum(message_sim)\n",
    "\n",
    "        # transform sim to python list for JSON serializing\n",
    "        return intent_ids, message_sim.tolist()\n",
    "\n",
    "    def process(self, message: 'Message', **kwargs: Any) -> None:\n",
    "        \"\"\"Return the most likely intent and its similarity to the input.\"\"\"\n",
    "\n",
    "        intent = {\"name\": None, \"confidence\": 0.0}\n",
    "        intent_ranking = []\n",
    "\n",
    "        if self.session is None:\n",
    "            logger.error(\"There is no trained tf.session: \"\n",
    "                         \"component is either not trained or \"\n",
    "                         \"didn't receive enough training data\")\n",
    "\n",
    "        else:\n",
    "            # get features (bag of words) for a message\n",
    "            # noinspection PyPep8Naming\n",
    "            X = message.get(\"text_features\").reshape(1, -1)\n",
    "\n",
    "            # stack encoded_all_intents on top of each other\n",
    "            # to create candidates for test examples\n",
    "            # noinspection PyPep8Naming\n",
    "            all_Y = self._create_all_Y(X.shape[0])\n",
    "\n",
    "            # load tf graph and session\n",
    "            intent_ids, message_sim = self._calculate_message_sim(X, all_Y)\n",
    "\n",
    "            # if X contains all zeros do not predict some label\n",
    "            if X.any() and intent_ids.size > 0:\n",
    "                intent = {\"name\": self.inv_intent_dict[intent_ids[0]],\n",
    "                          \"confidence\": message_sim[0]}\n",
    "\n",
    "                ranking = list(zip(list(intent_ids), message_sim))\n",
    "                ranking = ranking[:INTENT_RANKING_LENGTH]\n",
    "                intent_ranking = [{\"name\": self.inv_intent_dict[intent_idx],\n",
    "                                   \"confidence\": score}\n",
    "                                  for intent_idx, score in ranking]\n",
    "\n",
    "        message.set(\"intent\", intent, add_to_output=True)\n",
    "        message.set(\"intent_ranking\", intent_ranking, add_to_output=True)\n",
    "\n",
    "    def persist(self,\n",
    "                file_name: Text,\n",
    "                model_dir: Text) -> Dict[Text, Any]:\n",
    "        \"\"\"Persist this model into the passed directory.\n",
    "        Return the metadata necessary to load the model again.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.session is None:\n",
    "            return {\"file\": None}\n",
    "\n",
    "        checkpoint = os.path.join(model_dir, file_name + \".ckpt\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(checkpoint))\n",
    "        except OSError as e:\n",
    "            # be happy if someone already created the path\n",
    "            import errno\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "        with self.graph.as_default():\n",
    "            self.graph.clear_collection('message_placeholder')\n",
    "            self.graph.add_to_collection('message_placeholder',\n",
    "                                         self.a_in)\n",
    "\n",
    "            self.graph.clear_collection('intent_placeholder')\n",
    "            self.graph.add_to_collection('intent_placeholder',\n",
    "                                         self.b_in)\n",
    "\n",
    "            self.graph.clear_collection('similarity_op')\n",
    "            self.graph.add_to_collection('similarity_op',\n",
    "                                         self.sim_op)\n",
    "\n",
    "            self.graph.clear_collection('word_embed')\n",
    "            self.graph.add_to_collection('word_embed',\n",
    "                                         self.word_embed)\n",
    "            self.graph.clear_collection('intent_embed')\n",
    "            self.graph.add_to_collection('intent_embed',\n",
    "                                         self.intent_embed)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(self.session, checkpoint)\n",
    "\n",
    "        with io.open(os.path.join(\n",
    "                model_dir,\n",
    "                file_name + \"_inv_intent_dict.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.inv_intent_dict, f)\n",
    "        with io.open(os.path.join(\n",
    "                model_dir,\n",
    "                file_name + \"_encoded_all_intents.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.encoded_all_intents, f)\n",
    "\n",
    "        return {\"file\": file_name}\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls,\n",
    "             meta: Dict[Text, Any],\n",
    "             model_dir: Text = None,\n",
    "             model_metadata: 'Metadata' = None,\n",
    "             cached_component: Optional['EmbeddingIntentClassifier'] = None,\n",
    "             **kwargs: Any\n",
    "             ) -> 'EmbeddingIntentClassifier':\n",
    "\n",
    "        if model_dir and meta.get(\"file\"):\n",
    "            file_name = meta.get(\"file\")\n",
    "            checkpoint = os.path.join(model_dir, file_name + \".ckpt\")\n",
    "            graph = tf.Graph()\n",
    "            with graph.as_default():\n",
    "                sess = tf.Session()\n",
    "                saver = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "\n",
    "                saver.restore(sess, checkpoint)\n",
    "\n",
    "                a_in = tf.get_collection('message_placeholder')[0]\n",
    "                b_in = tf.get_collection('intent_placeholder')[0]\n",
    "\n",
    "                sim_op = tf.get_collection('similarity_op')[0]\n",
    "\n",
    "                word_embed = tf.get_collection('word_embed')[0]\n",
    "                intent_embed = tf.get_collection('intent_embed')[0]\n",
    "\n",
    "            with io.open(os.path.join(\n",
    "                    model_dir,\n",
    "                    file_name + \"_inv_intent_dict.pkl\"), 'rb') as f:\n",
    "                inv_intent_dict = pickle.load(f)\n",
    "            with io.open(os.path.join(\n",
    "                    model_dir,\n",
    "                    file_name + \"_encoded_all_intents.pkl\"), 'rb') as f:\n",
    "                encoded_all_intents = pickle.load(f)\n",
    "\n",
    "            return cls(\n",
    "                component_config=meta,\n",
    "                inv_intent_dict=inv_intent_dict,\n",
    "                encoded_all_intents=encoded_all_intents,\n",
    "                session=sess,\n",
    "                graph=graph,\n",
    "                message_placeholder=a_in,\n",
    "                intent_placeholder=b_in,\n",
    "                similarity_op=sim_op,\n",
    "                word_embed=word_embed,\n",
    "                intent_embed=intent_embed\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            logger.warning(\"Failed to load nlu model. Maybe path {} \"\n",
    "                           \"doesn't exist\"\n",
    "                           \"\".format(os.path.abspath(model_dir)))\n",
    "            return cls(component_config=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import TrainingData, Message\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en')\n",
    "examples = [Message(\"anywhere in the west\", {\n",
    "                    \"intent\": \"restaurant_search\",\n",
    "                    \"entities\": [{\"start\": 16, \"end\": 20,\n",
    "                                  \"value\": \"west\", \"entity\": \"location\"}],\n",
    "                    \"spacy_doc\": spacy_nlp(\"anywhere in the west\")\n",
    "                    }),\n",
    "            Message(\"central indian restaurant\", {\n",
    "                    \"intent\": \"restaurant_search\",\n",
    "                    \"entities\": [\n",
    "                     {\"start\": 0, \"end\": 7, \"value\": \"central\",\n",
    "                      \"entity\": \"location\",\n",
    "                      \"extractor\": \"random_extractor\"},\n",
    "                     {\"start\": 8, \"end\": 14, \"value\": \"indian\",\n",
    "                      \"entity\": \"cuisine\",\n",
    "                      \"extractor\": \"CRFEntityExtractor\"}\n",
    "                                 ],\n",
    "                    \"spacy_doc\": spacy_nlp(\"central indian restaurant\")\n",
    "                    }),\n",
    "            Message(\"hi there!\", {\"intent\": \"greet\", \"entities\": [],\n",
    "                                  \"spacy_doc\": spacy_nlp(\"hi there!\")}),\n",
    "            Message(\"good morning\", {\"intent\": \"greet\", \"entities\": [],         \n",
    "                                     \"spacy_doc\": spacy_nlp(\"good morning\")}),\n",
    "            Message(\"thank you\", {\"intent\": \"thanks\", \"entities\": [],\n",
    "                                  \"spacy_doc\": spacy_nlp(\"thank you\")}),\n",
    "            Message(\"good bye\", {\"intent\": \"thanks\", \"entities\": [],\n",
    "                                 \"spacy_doc\": spacy_nlp(\"good bye\")})        \n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/rasa_nlu/training_data/training_data.py:194: UserWarning: Entity 'cuisine' has only 1 training examples! minimum is 2, training may fail.\n",
      "  self.MIN_EXAMPLES_PER_ENTITY))\n"
     ]
    }
   ],
   "source": [
    "training_data = TrainingData(training_examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.featurizers.count_vectors_featurizer import CountVectorsFeaturizer\n",
    "ftr = CountVectorsFeaturizer({\"token_pattern\": r'(?u)\\b\\w+\\b'})\n",
    "ftr.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EmbeddingIntentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 300/300 [00:01<00:00, 209.18it/s, loss=0.096, acc=1.000]\n"
     ]
    }
   ],
   "source": [
    "classifier.train(training_data=training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = Message(\"show me some indian restuarants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'show me some indian restuarants'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr.process(test_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_features': array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'text': 'show me some indian restuarants'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.process(test_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_features': array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'intent': {'name': 'restaurant_search', 'confidence': 0.8687058687210083},\n",
       " 'intent_ranking': [{'name': 'restaurant_search',\n",
       "   'confidence': 0.8687058687210083},\n",
       "  {'name': 'greet', 'confidence': 0.35430455207824707},\n",
       "  {'name': 'thanks', 'confidence': 0.0}],\n",
       " 'text': 'show me some indian restuarants'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model using json input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data = {\n",
    "  \"intent\":{\n",
    "   \"restaurant_search\": [\"anywhere in the west\", \"central indian restaurant\", \"looking for french hotels\",\n",
    "\t\t\t\t\t\t\"show me some south indian places to eat in the north\", \"looking for exotic eating spots\",\n",
    "\t\t\t\t\t\t\"any north indian hotels near me\", \"show me chinese restaurants\", \"want to go to an italian restaurant\",\n",
    "\t\t\t\t\t\t\"i'm looking for a place in the north of town\", \"show me a mexican place in the centre\",\n",
    "\t\t\t\t\t\t\"hi, show me some authentic tamilian hotels\", \"hello, i need some fish food\", \"good morning, i'm looking for spanish or mexican hotels, thanks\"],\n",
    "   \"greet\": [\"hi\", \"hello\", \"good day\", \"hi!\", \"hey\", \"good morning\", \"good evening\", \"hey there!\", \"hey, how are you?\"],\n",
    "   \"thanks\": [\"thanks\", \"thank you\", \"thanx\", \"this is helpful, thanks\", \"regards\", \"good bye\", \"bye\", \"thanks & regards\", \"thanks and regards\",\n",
    "             \"with regards\"]\n",
    "  },\n",
    " \"entities\": {\n",
    "  \"restaurant_search\": [[{\"value\": \"west\", \"entity\": \"location\"}], [{\"value\": \"central\", \"entity\": \"location\"}, {\"value\": \"indian\", \"entity\": \"cuisine\"}], [{\"value\": \"french\", \"entity\": \"cuisine\"}], [{\"value\": \"south indian\", \"entity\": \"cuisine\"}, {\"value\": \"north\", \"entity\": \"location\"}], [], [{\"value\": \"north indian\", \"entity\": \"cuisine\"}], [{\"value\": \"chinese\", \"entity\": \"cuisine\"}], [{\"value\": \"italian\", \"entity\": \"cuisine\"}], [{\"value\": \"north\", \"entity\": \"location\"}], [{\"value\": \"mexican\", \"entity\": \"cuisine\"}, {\"value\": \"centre\", \"entity\": \"location\"}], [{\"value\": \"tamilian\", \"entity\": \"cuisine\"}], [{\"value\": \"fish\", \"entity\": \"cuisine\"}], [{\"value\": \"spanish\", \"entity\": \"cuisine\"}, {\"value\": \"mexican\", \"entity\": \"cuisine\"}]],\n",
    "  \"greet\": [[], [], [], [], [], [], [], [], []],\n",
    "  \"thanks\": [[], [], [], [], [], [], [], [], [], []]\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"i had / i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i shall / i will\",\n",
    "\"i'll've\": \"i shall have / i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent\n",
      "restaurant_search\n",
      "greet\n",
      "thanks\n",
      "entities\n",
      "restaurant_search\n",
      "greet\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "# preprocessing input data\n",
    "# lowering, contractions, regex to remove special characters\n",
    "import re\n",
    "def regex_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\x01', ' ')\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    text = re.sub(r\"http[s]?\\S+\", \" \", text)\n",
    "    text = re.sub(r\"xx+\", \" \", text)\n",
    "    text = re.sub(r\"x{2,}\", \"\", text)\n",
    "    text = re.sub(r\"www\\S+\\.com\", \"\", text)\n",
    "    text = re.sub(r\"\\S+fresh[a-zA-Z]+.com\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[-()._\\#/@&%;*:<>{}+=~!|?,]\", \" \", text)\n",
    "    text = re.sub(r\"[\\[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"`\", \"\", text)\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    # lower\n",
    "    text = str(text).lower()\n",
    "    # contractions\n",
    "    text = \" \".join(CONTRACTIONS[y.lower()].split('/')[0]\n",
    "                    if y.lower() in CONTRACTIONS else y\n",
    "                    for y in str(text).split())\n",
    "    # regex cleaning\n",
    "    text = regex_cleaning(text)\n",
    "    return text\n",
    "    \n",
    "inp_data_pre = {}\n",
    "for key, value in inp_data.items():\n",
    "    print(key)\n",
    "    inp_data_pre[key] = {}\n",
    "    for key1, value1 in value.items():\n",
    "        print(key1)\n",
    "        if key == \"intent\":\n",
    "            value1_new = [preprocess(x) for x in value1]\n",
    "        elif key == \"entities\":\n",
    "            value1_new = []\n",
    "            for item in value1:\n",
    "                new_item = []\n",
    "                for item1 in item:\n",
    "                    if type(item1['value']) == str:\n",
    "                        item1['value'] = preprocess(item1['value'])\n",
    "                    new_item.append(item1)\n",
    "                value1_new.append(new_item)\n",
    "        inp_data_pre[key][key1] = value1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'restaurant_search': ['anywhere in the west',\n",
       "   'central indian restaurant',\n",
       "   'looking for french hotels',\n",
       "   'show me some south indian places to eat in the north',\n",
       "   'looking for exotic eating spots',\n",
       "   'any north indian hotels near me',\n",
       "   'show me chinese restaurants',\n",
       "   'want to go to an italian restaurant',\n",
       "   'i am looking for a place in the north of town',\n",
       "   'show me a mexican place in the centre',\n",
       "   'hi  show me some authentic tamilian hotels',\n",
       "   'hello  i need some fish food',\n",
       "   'good morning  i am looking for spanish or mexican hotels  thanks'],\n",
       "  'greet': ['hi',\n",
       "   'hello',\n",
       "   'good day',\n",
       "   'hi ',\n",
       "   'hey',\n",
       "   'good morning',\n",
       "   'good evening',\n",
       "   'hey there ',\n",
       "   'hey  how are you '],\n",
       "  'thanks': ['thanks',\n",
       "   'thank you',\n",
       "   'thanx',\n",
       "   'this is helpful  thanks',\n",
       "   'regards',\n",
       "   'good bye',\n",
       "   'bye',\n",
       "   'thanks   regards',\n",
       "   'thanks and regards',\n",
       "   'with regards']},\n",
       " 'entities': {'restaurant_search': [[{'value': 'west', 'entity': 'location'}],\n",
       "   [{'value': 'central', 'entity': 'location'},\n",
       "    {'value': 'indian', 'entity': 'cuisine'}],\n",
       "   [{'value': 'french', 'entity': 'cuisine'}],\n",
       "   [{'value': 'south indian', 'entity': 'cuisine'},\n",
       "    {'value': 'north', 'entity': 'location'}],\n",
       "   [],\n",
       "   [{'value': 'north indian', 'entity': 'cuisine'}],\n",
       "   [{'value': 'chinese', 'entity': 'cuisine'}],\n",
       "   [{'value': 'italian', 'entity': 'cuisine'}],\n",
       "   [{'value': 'north', 'entity': 'location'}],\n",
       "   [{'value': 'mexican', 'entity': 'cuisine'},\n",
       "    {'value': 'centre', 'entity': 'location'}],\n",
       "   [{'value': 'tamilian', 'entity': 'cuisine'}],\n",
       "   [{'value': 'fish', 'entity': 'cuisine'}],\n",
       "   [{'value': 'spanish', 'entity': 'cuisine'},\n",
       "    {'value': 'mexican', 'entity': 'cuisine'}]],\n",
       "  'greet': [[], [], [], [], [], [], [], [], []],\n",
       "  'thanks': [[], [], [], [], [], [], [], [], [], []]}}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_data_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import TrainingData, Message\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inp_data for training\n",
    "intents = inp_data_pre['intent']\n",
    "entities = inp_data_pre['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_pos_entity(utterance, entity):\n",
    "    start_pos = utterance.index(entity)\n",
    "    end_pos = start_pos + len(entity)\n",
    "    return start_pos, end_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant_search\n",
      "greet\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for intent, intent_values in intents.items():\n",
    "    print(intent)\n",
    "    entity_values = entities[intent]\n",
    "    for i, intent_value in enumerate(intent_values):\n",
    "        out = {}\n",
    "        entity_value = entity_values[i]\n",
    "        if entity_value:\n",
    "            entity_value_new = []\n",
    "            for ent in entity_value:\n",
    "                start_pos, end_pos = get_start_end_pos_entity(intent_value,\n",
    "                                                              ent['value'])\n",
    "                ent['start'] = start_pos\n",
    "                ent['end'] = end_pos\n",
    "                entity_value_new.append(ent)\n",
    "        else:\n",
    "            entity_value_new = entity_value\n",
    "        # update out dict\n",
    "        out['intent'] = intent\n",
    "        out['entities'] = entity_value_new\n",
    "        out['spacy_doc']= spacy_nlp(intent_value)\n",
    "        examples.append(Message(intent_value, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TrainingData(training_examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data.intent_examples) == len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.featurizers.count_vectors_featurizer import CountVectorsFeaturizer\n",
    "ftr = CountVectorsFeaturizer({\"token_pattern\": r'(?u)\\b\\w+\\b'})\n",
    "ftr.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EmbeddingIntentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 30/30 [00:00<00:00, 65.45it/s, loss=0.285, acc=1.000]\n"
     ]
    }
   ],
   "source": [
    "classifier.train(training_data=training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'embedding_intent_classifier_v1'}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "classifier.persist(file_name=\"embedding_intent_classifier_v1\", model_dir=\"/Users/varunn/Documents/projects/Intent_and_Entity_Extraction/models/\")\n",
    "#import pickle\n",
    "#pickle.dump(ftr, open('/Users/varunn/Documents/projects/Intent_and_Entity_Extraction/models/count_vectorizer_v1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = \"hi, looking for chinese hotels\"\n",
    "test_inp_pre = preprocess(test_inp)\n",
    "test_inp_pre_m = Message(test_inp_pre)\n",
    "ftr.process(test_inp_pre_m)\n",
    "classifier.process(test_inp_pre_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'restaurant_search', 'confidence': 0.7194852828979492}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp_pre_m.as_dict()['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'embedding_intent_classifier_v1'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.persist(file_name=\"embedding_intent_classifier_v1\", model_dir=\"/Users/varunn/Documents/projects/Intent_and_Entity_Extraction/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pickle (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pickle\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'CountVectorsFeaturizer.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-807cc7dd67c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/varunn/Documents/projects/Intent_and_Entity_Extraction/models/count_vectorizer_v1.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'CountVectorsFeaturizer.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "!pip3 install pickle\n",
    "import pickle\n",
    "pickle.dump(ftr, open('/Users/varunn/Documents/projects/Intent_and_Entity_Extraction/models/count_vectorizer_v1.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File ./temp_starspace_model.ckpt.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-5c751008c007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'temp_starspace_model'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingIntentClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0fa617cc355e>\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, meta, model_dir, model_metadata, cached_component, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   1673\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[0;32m-> 1674\u001b[0;31m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1685\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File ./temp_starspace_model.ckpt.meta does not exist."
     ]
    }
   ],
   "source": [
    "meta = {'file': 'temp_starspace_model'}\n",
    "classifier1 = EmbeddingIntentClassifier.load(meta=meta, model_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = \"looking for chinese hotels\"\n",
    "test_inp_pre = preprocess(test_inp)\n",
    "test_inp_pre_m = Message(test_inp_pre)\n",
    "ftr.process(test_inp_pre_m)\n",
    "classifier1.process(test_inp_pre_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_features': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'intent': {'name': 'restaurant_search', 'confidence': 0.8807507157325745},\n",
       " 'intent_ranking': [{'name': 'restaurant_search',\n",
       "   'confidence': 0.8807507157325745},\n",
       "  {'name': 'thanks', 'confidence': 0.17054714262485504},\n",
       "  {'name': 'greet', 'confidence': 0.0}],\n",
       " 'text': 'looking for chinese hotels'}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp_pre_m.as_dict()['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
