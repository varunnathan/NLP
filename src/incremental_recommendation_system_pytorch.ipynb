{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.csv':\n",
    "            files[filename.stem] = pd.read_csv(filename)\n",
    "        elif filename.suffix == '.dat':\n",
    "            if filename.stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(filename, sep='::', names=columns, engine='python')\n",
    "            files[filename.stem] = data\n",
    "    return files['ratings'], files['movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = '/Users/varunn/Documents/NLP-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, movies = read_data(Path(os.path.join(download_path, 'ml-1m')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ratings.shape)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform users and movies to categorical features\n",
    "ratings['userId'] = ratings['userId'].astype('category')\n",
    "ratings['movieId'] = ratings['movieId'].astype('category')\n",
    "\n",
    "# use the category codes to avoid creating separate vocabularies\n",
    "ratings['user_code'] = ratings['userId'].cat.codes.astype(int)\n",
    "ratings['movie_code'] = ratings['movieId'].cat.codes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users:  6040\n",
      "# movies:  3706\n"
     ]
    }
   ],
   "source": [
    "n_users = ratings['user_code'].max() + 1\n",
    "n_movies = ratings['movie_code'].max() + 1\n",
    "\n",
    "print('# users: ', n_users)\n",
    "print('# movies: ', n_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = ratings.sort_values(by='timestamp')\n",
    "data_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users:  6040\n",
      "# movies:  3706\n"
     ]
    }
   ],
   "source": [
    "user_ids = data_df['userId'].unique().tolist()\n",
    "movie_ids = data_df['movieId'].unique().tolist()\n",
    "user_id_mapping = {value: i for i, value in enumerate(user_ids)}\n",
    "movie_id_mapping = {value: i for i, value in enumerate(movie_ids)}\n",
    "\n",
    "n_users = len(user_id_mapping)\n",
    "n_movies = len(movie_id_mapping)\n",
    "\n",
    "print('# users: ', n_users)\n",
    "print('# movies: ', n_movies)\n",
    "\n",
    "data_df['new_userId'] = data_df['userId'].apply(\n",
    "    lambda x: user_id_mapping[x])\n",
    "data_df['new_movieId'] = data_df['movieId'].apply(\n",
    "    lambda x: movie_id_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_code</th>\n",
       "      <th>movie_code</th>\n",
       "      <th>new_userId</th>\n",
       "      <th>new_movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6040</td>\n",
       "      <td>858</td>\n",
       "      <td>4</td>\n",
       "      <td>956703932</td>\n",
       "      <td>6039</td>\n",
       "      <td>802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6040</td>\n",
       "      <td>2384</td>\n",
       "      <td>4</td>\n",
       "      <td>956703954</td>\n",
       "      <td>6039</td>\n",
       "      <td>2191</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6040</td>\n",
       "      <td>593</td>\n",
       "      <td>5</td>\n",
       "      <td>956703954</td>\n",
       "      <td>6039</td>\n",
       "      <td>579</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6040</td>\n",
       "      <td>1961</td>\n",
       "      <td>4</td>\n",
       "      <td>956703977</td>\n",
       "      <td>6039</td>\n",
       "      <td>1781</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6040</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>956703977</td>\n",
       "      <td>6039</td>\n",
       "      <td>1839</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userId movieId  rating  timestamp  user_code  movie_code new_userId  \\\n",
       "0   6040     858       4  956703932       6039         802          0   \n",
       "1   6040    2384       4  956703954       6039        2191          0   \n",
       "2   6040     593       5  956703954       6039         579          0   \n",
       "3   6040    1961       4  956703977       6039        1781          0   \n",
       "4   6040    2019       5  956703977       6039        1839          0   \n",
       "\n",
       "  new_movieId  \n",
       "0           0  \n",
       "1           1  \n",
       "2           2  \n",
       "3           3  \n",
       "4           4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226310, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_code</th>\n",
       "      <th>movie_code</th>\n",
       "      <th>new_userId</th>\n",
       "      <th>new_movieId</th>\n",
       "      <th>preference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6040</td>\n",
       "      <td>593</td>\n",
       "      <td>5</td>\n",
       "      <td>956703954</td>\n",
       "      <td>6039</td>\n",
       "      <td>579</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6040</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>956703977</td>\n",
       "      <td>6039</td>\n",
       "      <td>1839</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6040</td>\n",
       "      <td>213</td>\n",
       "      <td>5</td>\n",
       "      <td>956704056</td>\n",
       "      <td>6039</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6040</td>\n",
       "      <td>3111</td>\n",
       "      <td>5</td>\n",
       "      <td>956704056</td>\n",
       "      <td>6039</td>\n",
       "      <td>2895</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6040</td>\n",
       "      <td>2503</td>\n",
       "      <td>5</td>\n",
       "      <td>956704191</td>\n",
       "      <td>6039</td>\n",
       "      <td>2309</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId movieId  rating  timestamp  user_code  movie_code new_userId  \\\n",
       "2    6040     593       5  956703954       6039         579          0   \n",
       "4    6040    2019       5  956703977       6039        1839          0   \n",
       "6    6040     213       5  956704056       6039         207          0   \n",
       "7    6040    3111       5  956704056       6039        2895          0   \n",
       "11   6040    2503       5  956704191       6039        2309          0   \n",
       "\n",
       "   new_movieId  preference  \n",
       "2            2           1  \n",
       "4            4           1  \n",
       "6            6           1  \n",
       "7            7           1  \n",
       "11          11           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more than 4 -> 1, less than 5 -> 0\n",
    "data_df['preference'] = np.where(data_df['rating'] > 4, 1, 0)\n",
    "\n",
    "# keep only ones and discard the others\n",
    "data_df_cleaned = data_df[(data_df['preference'] == 1)]\n",
    "print(data_df_cleaned.shape)\n",
    "data_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6039, 6038, 6037, ...,  348, 2909, 2946])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['user_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function normal_ in module torch.nn.init:\n",
      "\n",
      "normal_(tensor, mean=0.0, std=1.0)\n",
      "    Fills the input Tensor with values drawn from the normal\n",
      "    distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\n",
      "    \n",
      "    Args:\n",
      "        tensor: an n-dimensional `torch.Tensor`\n",
      "        mean: the mean of the normal distribution\n",
      "        std: the standard deviation of the normal distribution\n",
      "    \n",
      "    Examples:\n",
      "        >>> w = torch.empty(3, 5)\n",
      "        >>> nn.init.normal_(w)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.init.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(n_movies, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5789,  0.8428,  1.2063,  ...,  1.2183, -0.8995,  0.8056],\n",
       "        [ 0.9189,  0.4767, -1.4105,  ..., -0.1423, -0.7803,  0.5009],\n",
       "        [ 0.5333,  1.4665,  0.7449,  ...,  0.1174, -0.7504, -1.4096],\n",
       "        ...,\n",
       "        [-1.9330, -0.5867,  1.1772,  ...,  0.0532,  0.9101, -0.5424],\n",
       "        [-0.0422,  0.6056, -0.4763,  ...,  1.3144, -0.0459,  0.9182],\n",
       "        [ 1.4965, -0.0470,  0.2065,  ...,  1.0852, -0.7818,  0.6645]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7677,  1.0578, -1.7912,  ..., -0.3434, -0.3171,  0.0614],\n",
       "        [ 1.2797, -0.1544, -1.2456,  ..., -0.2985,  1.5844, -1.0965],\n",
       "        [ 1.3789, -0.4688,  0.1579,  ...,  1.2603,  0.7772, -1.5381],\n",
       "        ...,\n",
       "        [-0.3001, -0.3375, -0.6527,  ...,  0.4167, -2.0371, -0.6036],\n",
       "        [ 1.8442,  0.9047, -2.2226,  ...,  1.9515,  0.8001, -1.7166],\n",
       "        [ 0.6924, -1.9342,  1.5229,  ..., -0.0569,  1.4001, -0.0700]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = nn.Embedding(n_movies, 1, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9953],\n",
       "        [-0.6603],\n",
       "        [-0.6756],\n",
       "        ...,\n",
       "        [ 1.1043],\n",
       "        [-0.3431],\n",
       "        [ 1.1553]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1364],\n",
       "        [0.6402],\n",
       "        [0.5094],\n",
       "        ...,\n",
       "        [0.0621],\n",
       "        [0.6678],\n",
       "        [0.5436]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(bias.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "\n",
    "class SimpleCF(nn.Module):\n",
    "    def __init__(self, n_users: int, n_items: int, factors: int = 16,\n",
    "                 user_embeddings: torch.tensor = None,\n",
    "                 freeze_users: bool = False,\n",
    "                 item_embeddings: torch.tensor = None,\n",
    "                 freeze_items: bool = False,\n",
    "                 init: torch.nn.init = torch.nn.init.normal_,\n",
    "                 binary: bool =False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.binary = binary\n",
    "\n",
    "        self.user_embeddings = self._create_embedding(\n",
    "            n_users, factors, user_embeddings, freeze_users,\n",
    "            init, **kwargs)\n",
    "        self.item_embeddings = self._create_embedding(\n",
    "            n_items, factors, item_embeddings, freeze_items,\n",
    "            init, **kwargs)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, u: torch.tensor, i: torch.tensor) -> torch.tensor:\n",
    "        user_embedding = self.user_embeddings(u)\n",
    "        user_embedding = user_embedding[:, None, :]\n",
    "        item_embedding = self.item_embeddings(i)\n",
    "        item_embedding = item_embedding[:, None, :]\n",
    "        rating = torch.matmul(user_embedding, item_embedding.transpose(\n",
    "            1, 2))\n",
    "        if self.binary:\n",
    "            return self.sigmoid(rating)\n",
    "        return rating\n",
    "\n",
    "    def _create_embedding(self, n_items, factors, weights, freeze,\n",
    "                          init, **kwargs):\n",
    "        embedding = nn.Embedding(n_items, factors)\n",
    "        init(embedding.weight.data, **kwargs)\n",
    "\n",
    "        if weights is not None:\n",
    "            embedding.load_state_dict({'weight': weights})\n",
    "        if freeze:\n",
    "            embedding.weight.requires_grad = False\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "class BaseModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Base module for explicit matrix factorization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_factors=40,\n",
    "                 dropout_p=0,\n",
    "                 sparse=False,\n",
    "                 user_embeddings: torch.tensor = None,\n",
    "                 user_biases: torch.tensor = None,\n",
    "                 freeze_users: bool = False,\n",
    "                 item_embeddings: torch.tensor = None,\n",
    "                 item_biases: torch.tensor = None,\n",
    "                 freeze_items: bool = False,\n",
    "                 init: torch.nn.init = torch.nn.init.normal_,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_users : int\n",
    "            Number of users\n",
    "        n_items : int\n",
    "            Number of items\n",
    "        n_factors : int\n",
    "            Number of latent factors (or embeddings or whatever you want to\n",
    "            call it).\n",
    "        dropout_p : float\n",
    "            p in nn.Dropout module. Probability of dropout.\n",
    "        sparse : bool\n",
    "            Whether or not to treat embeddings as sparse. NOTE: cannot use\n",
    "            weight decay on the optimizer if sparse=True. Also, can only use\n",
    "            Adagrad.\n",
    "        \"\"\"\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "        self.user_embeddings, self.user_biases = self._create_embedding(\n",
    "            n_users, n_factors, user_embeddings, user_biases,\n",
    "            freeze_users, init, sparse, **kwargs)\n",
    "        self.item_embeddings, self.item_biases = self._create_embedding(\n",
    "            n_items, n_factors, item_embeddings, item_biases,\n",
    "            freeze_items, init, sparse, **kwargs)\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "        self.sparse = sparse\n",
    "        \n",
    "    def forward(self, users, items):\n",
    "        \"\"\"\n",
    "        Forward pass through the model. For a single user and item, this\n",
    "        looks like:\n",
    "        user_bias + item_bias + user_embeddings.dot(item_embeddings)\n",
    "        Parameters\n",
    "        ----------\n",
    "        users : np.ndarray\n",
    "            Array of user indices\n",
    "        items : np.ndarray\n",
    "            Array of item indices\n",
    "        Returns\n",
    "        -------\n",
    "        preds : np.ndarray\n",
    "            Predicted ratings.\n",
    "        \"\"\"\n",
    "        ues = self.user_embeddings(users)\n",
    "        uis = self.item_embeddings(items)\n",
    "\n",
    "        preds = self.user_biases(users)\n",
    "        preds += self.item_biases(items)\n",
    "        preds += (self.dropout(ues) * self.dropout(uis)).sum(\n",
    "            dim=1, keepdim=True)\n",
    "\n",
    "        return preds.squeeze()\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def predict(self, users, items):\n",
    "        return self.forward(users, items)\n",
    "    \n",
    "    def _create_embedding(self, n_items, n_factors, pre_weights,\n",
    "                          pre_biases, freeze, init, sparse, **kwargs):\n",
    "        \n",
    "        bias = nn.Embedding(n_items, 1, sparse=sparse)\n",
    "        embedding = nn.Embedding(n_items, n_factors, sparse=sparse)\n",
    "        init(bias.weight.data, **kwargs)\n",
    "        init(embedding.weight.data, **kwargs)\n",
    "\n",
    "        if pre_weights is not None:\n",
    "            embedding.load_state_dict({'weight': pre_weights})\n",
    "            \n",
    "        if pre_biases is not None:\n",
    "            bias.load_state_dict({'weight': pre_biases})\n",
    "        \n",
    "        if freeze:\n",
    "            embedding.weight.requires_grad = False\n",
    "            bias.weight.requires_grad = False\n",
    "\n",
    "        return embedding, bias\n",
    "\n",
    "\n",
    "def bpr_loss(preds, vals):\n",
    "    sig = nn.Sigmoid()\n",
    "    return (1.0 - sig(preds)).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class Interactions(data.Dataset):\n",
    "    \"\"\"\n",
    "    Hold data in the form of an interactions matrix.\n",
    "    Typical use-case is like a ratings matrix:\n",
    "    - Users are the rows\n",
    "    - Items are the columns\n",
    "    - Elements of the matrix are the ratings given by a user for an item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mat):\n",
    "        self.mat = mat.astype(np.float32).tocoo()\n",
    "        self.n_users = self.mat.shape[0]\n",
    "        self.n_items = self.mat.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.mat.row[index]\n",
    "        col = self.mat.col[index]\n",
    "        val = self.mat.data[index]\n",
    "        return (row, col), val\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mat.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from abc import ABCMeta\n",
    "from abc import abstractmethod\n",
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class StepBase:\n",
    "    \"\"\"Defines the interface that all step models here expose.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_fit(self, data_loader: torch.utils.data.DataLoader, epochs: int):\n",
    "        \"\"\"Trains the model on a batch of user-item interactions.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, user: torch.tensor, item: torch.tensor,\n",
    "             rating: torch.tensor, preference: torch.tensor):\n",
    "        \"\"\"Trains the model incrementally.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, user: torch.tensor, k: int):\n",
    "        \"\"\"Recommends the top-k items to a specific user.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Saves the model parameters to the given path.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Loads the model parameters from a given path.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Step(StepBase):\n",
    "    \"\"\"Incremental and batch training of recommender systems.\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module,\n",
    "                 loss_function=torch.nn.MSELoss(reduction='sum'),\n",
    "                 optimizer = torch.optim.Adam,\n",
    "                 lr = 0.01, weight_decay = 0.):\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimizer = optimizer(self.model.parameters(),\n",
    "                                   lr=self.lr,\n",
    "                                   weight_decay=self.weight_decay)\n",
    "        self.losses = []\n",
    "\n",
    "        # check if the user has provided user and item embeddings\n",
    "        assert self.model.user_embeddings, 'User embedding matrix could not be found.'\n",
    "        assert self.model.item_embeddings, 'Item embedding matrix could not be found.'\n",
    "\n",
    "    @property\n",
    "    def user_embeddings(self):\n",
    "        return self.model.user_embeddings\n",
    "\n",
    "    @property\n",
    "    def item_embeddings(self):\n",
    "        return self.model.item_embeddings\n",
    "    \n",
    "    @property\n",
    "    def user_biases(self):\n",
    "        return self.model.user_biases\n",
    "    \n",
    "    @property\n",
    "    def item_biases(self):\n",
    "        return self.model.item_biases\n",
    "\n",
    "    def batch_fit(self, data_loader: torch.utils.data.DataLoader,\n",
    "                  data_size: int, epochs: int = 1):\n",
    "        \"\"\"Trains the model on a batch of user-item interactions.\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = torch.Tensor([0])\n",
    "            with tqdm(total=len(data_loader)) as pbar:\n",
    "                for _, ((row, col), val) in enumerate(data_loader):\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    row = row.long()\n",
    "                    # TODO: turn this into a collate_fn like the data_loader\n",
    "                    if isinstance(col, list):\n",
    "                        col = tuple(c.long() for c in col)\n",
    "                    else:\n",
    "                        col = col.long()\n",
    "                    val = val.float()\n",
    "\n",
    "                    preds = self.model(row, col)\n",
    "                    loss = self.loss_function(preds, val)\n",
    "                    loss.backward()\n",
    "\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    batch_loss = loss.item() / row.size()[0]\n",
    "\n",
    "                    pbar.update(1)\n",
    "                \n",
    "            total_loss /= data_size\n",
    "            self.losses.append(total_loss)\n",
    "            \n",
    "    def _validation_loss(self, data_loader: torch.utils.data.DataLoader,\n",
    "                         data_size: int):\n",
    "        self.model.eval()\n",
    "        total_loss = torch.Tensor([0])\n",
    "        for _, ((row, col), val) in enumerate(data_loader):\n",
    "            row = row.long()\n",
    "            if isinstance(col, list):\n",
    "                col = tuple(c.long() for c in col)\n",
    "            else:\n",
    "                col = col.long()\n",
    "            val = val.float()\n",
    "\n",
    "            preds = self.model(row, col)\n",
    "            loss = self.loss_function(preds, val)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= data_size\n",
    "        return total_loss[0]\n",
    "\n",
    "    def step(self, user: torch.tensor, item: torch.tensor,\n",
    "             rating: torch.tensor = None):\n",
    "        \"\"\"Trains the model incrementally.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        pred = self.model(user, item)\n",
    "        loss = self.loss_function(pred, rating)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        return batch_loss\n",
    "\n",
    "    def recommend(self, user: torch.tensor, k:int = 10) -> torch.tensor:\n",
    "        \"\"\"Recommends the top-k items to a specific user.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        u_embed_one = self.user_embeddings(user)\n",
    "        u_embed_one_reshaped = u_embed_one.reshape((\n",
    "            1, u_embed_one.shape[0]))\n",
    "        m_embed = self.item_embeddings.weight\n",
    "        u_bias_one = self.user_biases(user)\n",
    "        u_bias_one_reshaped = u_bias_one.reshape((\n",
    "            1, u_bias_one.shape[0]))\n",
    "        m_bias = self.item_biases.weight\n",
    "        \n",
    "        bias_sum = u_bias_one_reshaped + m_bias\n",
    "        bias_sum = bias_sum.reshape((bias_sum.shape[1],\n",
    "                                     bias_sum.shape[0]))\n",
    "\n",
    "        preds = torch.matmul(u_embed_one_reshaped, m_embed.t())+bias_sum\n",
    "\n",
    "        return preds.squeeze().argsort()[-k:]\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Saves the model parameters to the given path.\"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Loads the model parameters from a given path.\"\"\"\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = nn.Embedding(n_users, 128, sparse=False)\n",
    "\n",
    "user_biases = nn.Embedding(n_users, 1, sparse=False)\n",
    "\n",
    "movie_embeddings = nn.Embedding(n_movies, 128, sparse=False)\n",
    "\n",
    "movie_biases = nn.Embedding(n_movies, 1, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "user = 0\n",
    "user = torch.tensor(user)\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_embed_one = user_embeddings(user)\n",
    "m_embed = movie_embeddings.weight\n",
    "u_bias_one = user_biases(user)\n",
    "m_bias = movie_biases.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([3706, 128])\n",
      "torch.Size([128, 3706])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 3706])\n",
      "torch.Size([3706, 1])\n"
     ]
    }
   ],
   "source": [
    "print(u_embed_one.shape)\n",
    "u_embed_one_reshaped = u_embed_one.reshape((1, u_embed_one.shape[0]))\n",
    "print(u_embed_one_reshaped.shape)\n",
    "#print(u_embed_one.transpose(0, 1))\n",
    "print(m_embed.shape)\n",
    "print(m_embed.t().shape)\n",
    "print(u_bias_one.shape)\n",
    "u_bias_one_reshaped = u_bias_one.reshape((1, u_bias_one.shape[0]))\n",
    "print(u_bias_one_reshaped.shape)\n",
    "bias_sum = u_bias_one_reshaped + m_bias\n",
    "bias_sum = bias_sum.reshape((bias_sum.shape[1], bias_sum.shape[0]))\n",
    "print(bias_sum.shape)\n",
    "print(m_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3706])\n",
      "tensor([[ -7.5282,  15.1134,   5.4389,  ..., -13.9357, -14.3046, -21.9863]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "tmp = torch.matmul(u_embed_one_reshaped, m_embed.t())\n",
    "print(tmp.shape)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tmp + bias_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3706])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3706])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.view(preds.shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3701, 2744, 1224, 2863, 3224, 1756,  393, 1635, 3496, 3248])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.squeeze().argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data_df into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, test = train_test_split(data_df, test_size=0.1, random_state=1)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900188, 9)\n",
      "(100021, 9)\n",
      "num users train:  6040\n",
      "num movies train:  3698\n",
      "num users test:  5953\n",
      "num movies test:  3303\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('num users train: ', train['userId'].nunique())\n",
    "print('num movies train: ', train['movieId'].nunique())\n",
    "\n",
    "print('num users test: ', test['userId'].nunique())\n",
    "print('num movies test: ', test['movieId'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360075, 9)\n",
      "(540113, 9)\n",
      "  userId movieId  rating  timestamp  user_code  movie_code new_userId  \\\n",
      "0   6040     593       5  956703954       6039         579          0   \n",
      "1   6040    2384       4  956703954       6039        2191          0   \n",
      "2   6040    2019       5  956703977       6039        1839          0   \n",
      "3   6040    3111       5  956704056       6039        2895          0   \n",
      "4   6040     213       5  956704056       6039         207          0   \n",
      "\n",
      "  new_movieId  preference  \n",
      "0           2           1  \n",
      "1           1           0  \n",
      "2           4           1  \n",
      "3           7           1  \n",
      "4           6           1  \n",
      "  userId movieId  rating  timestamp  user_code  movie_code new_userId  \\\n",
      "0   3380    1240       5  967588202       3379        1148       2660   \n",
      "1   3380    2130       4  967588214       3379        1949       2660   \n",
      "2   3377    1343       5  967588226       3376        1245       2663   \n",
      "3   3377    2118       4  967588226       3376        1937       2663   \n",
      "4   3377     707       4  967588226       3376         677       2663   \n",
      "\n",
      "  new_movieId  preference  \n",
      "0         454           1  \n",
      "1         654           0  \n",
      "2        1039           1  \n",
      "3         929           0  \n",
      "4        1422           0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# divide train into train1 and train2\n",
    "train.sort_values('timestamp', ascending=True, inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "bootstrap_perc = 0.4\n",
    "pct = int(train.shape[0] * bootstrap_perc)\n",
    "train1 = train[:pct]\n",
    "train2 = train[pct:]\n",
    "\n",
    "train1.reset_index(drop=True, inplace=True)\n",
    "train2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(train1.shape)\n",
    "print(train2.shape)\n",
    "print(train1.head())\n",
    "print(train2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interactions matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def get_interaction_matrix(df, n_users, n_movies, user2index,\n",
    "                           item2index):\n",
    "    interactions = np.zeros((n_users, n_movies))\n",
    "    for row in df.itertuples():\n",
    "        interactions[user2index[row[1]], item2index[row[2]]] = row[3]\n",
    "    \n",
    "    return sp.coo_matrix(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_interactions = get_interaction_matrix(train1, n_users, n_movies,\n",
    "                                             user_id_mapping,\n",
    "                                             movie_id_mapping)\n",
    "train2_interactions = get_interaction_matrix(train2, n_users, n_movies,\n",
    "                                             user_id_mapping,\n",
    "                                             movie_id_mapping)\n",
    "test_interactions = get_interaction_matrix(test, n_users, n_movies,\n",
    "                                           user_id_mapping,\n",
    "                                           movie_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 3706)\n",
      "(6040, 3706)\n",
      "(6040, 3706)\n",
      "360075\n"
     ]
    }
   ],
   "source": [
    "print(train1_interactions.shape)\n",
    "print(train2_interactions.shape)\n",
    "print(test_interactions.shape)\n",
    "print(train1_interactions.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "1. Train on train1 (model1) and validate on test\n",
    "2. Update model1 on train2 and validate on test\n",
    "3. Incremental training of model1 on train2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - train on train1 and validate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_loader = data.DataLoader(Interactions(train1_interactions),\n",
    "                                batch_size=512, shuffle=False)\n",
    "test_loader = data.DataLoader(Interactions(test_interactions),\n",
    "                              batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 360075, 704)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train1_loader), train1_interactions.nnz, 360448//512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BaseModule(n_users, n_movies, n_factors=128, dropout_p=0.02)\n",
    "\n",
    "model = Step(net, lr=0.02, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:11<00:00, 54.79it/s]\n",
      "100%|██████████| 704/704 [00:13<00:00, 52.24it/s]\n",
      "100%|██████████| 704/704 [00:23<00:00, 29.55it/s]\n",
      "100%|██████████| 704/704 [00:23<00:00, 29.93it/s]\n",
      "100%|██████████| 704/704 [00:22<00:00, 30.94it/s]\n",
      "100%|██████████| 704/704 [00:22<00:00, 30.99it/s]\n",
      "100%|██████████| 704/704 [00:23<00:00, 29.91it/s]\n",
      "100%|██████████| 704/704 [00:24<00:00, 29.30it/s]\n",
      "100%|██████████| 704/704 [00:23<00:00, 29.71it/s]\n",
      "100%|██████████| 704/704 [00:22<00:00, 29.83it/s]\n",
      "100%|██████████| 704/704 [00:22<00:00, 32.89it/s]\n",
      "100%|██████████| 704/704 [00:23<00:00, 29.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 258.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model.batch_fit(train1_loader, train1_interactions.nnz, epochs=12)\n",
    "\n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([11.2205]),\n",
       " tensor([2.0262]),\n",
       " tensor([1.3138]),\n",
       " tensor([1.3812]),\n",
       " tensor([1.2426]),\n",
       " tensor([1.2377]),\n",
       " tensor([1.1876]),\n",
       " tensor([1.1864]),\n",
       " tensor([1.1596]),\n",
       " tensor([1.1610]),\n",
       " tensor([1.1479]),\n",
       " tensor([1.1544])]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of model on test:  tensor(2.6761)\n"
     ]
    }
   ],
   "source": [
    "print('loss of model on test: ',\n",
    "      model._validation_loss(test_loader, test_interactions.nnz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_train1_E12.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Step(net, lr=6e-3, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load('./model_train1_E12.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of model1 on test:  tensor(2.6761)\n"
     ]
    }
   ],
   "source": [
    "print('loss of model1 on test: ',\n",
    "      model1._validation_loss(test_loader, test_interactions.nnz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Update model1 on train2 and validate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_loader = data.DataLoader(Interactions(train2_interactions),\n",
    "                                batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54012/54012 [25:13<00:00, 35.68it/s]\n",
      "100%|██████████| 54012/54012 [28:20<00:00, 27.23it/s]\n",
      "100%|██████████| 54012/54012 [28:10<00:00, 31.95it/s]\n",
      "100%|██████████| 54012/54012 [27:02<00:00, 34.97it/s]\n",
      "100%|██████████| 54012/54012 [27:25<00:00, 35.42it/s]\n",
      "100%|██████████| 54012/54012 [25:27<00:00, 35.36it/s]\n",
      "100%|██████████| 54012/54012 [25:43<00:00, 35.00it/s]\n",
      "100%|██████████| 54012/54012 [40:20<00:00, 22.32it/s]  \n",
      "100%|██████████| 54012/54012 [26:40<00:00, 33.74it/s]\n",
      "100%|██████████| 54012/54012 [25:23<00:00, 35.66it/s]\n",
      "100%|██████████| 54012/54012 [25:43<00:00, 34.99it/s]\n",
      "100%|██████████| 54012/54012 [26:14<00:00, 34.31it/s]\n",
      "100%|██████████| 54012/54012 [26:42<00:00, 33.71it/s]\n",
      "100%|██████████| 54012/54012 [26:43<00:00, 33.67it/s]\n",
      "100%|██████████| 54012/54012 [26:05<00:00, 34.49it/s]\n",
      "100%|██████████| 54012/54012 [25:40<00:00, 35.03it/s]\n",
      "100%|██████████| 54012/54012 [26:01<00:00, 34.59it/s]\n",
      "100%|██████████| 54012/54012 [25:47<00:00, 35.19it/s]\n",
      "100%|██████████| 54012/54012 [25:53<00:00, 34.77it/s]\n",
      "100%|██████████| 54012/54012 [25:53<00:00, 34.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8h 52min 45s, sys: 4min 7s, total: 8h 56min 53s\n",
      "Wall time: 9h 33s\n"
     ]
    }
   ],
   "source": [
    "%time model1.batch_fit(train2_loader, train2_interactions.nnz, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of model1_updated on test:  tensor(11.1351)\n"
     ]
    }
   ],
   "source": [
    "print('loss of model1_updated on test: ',\n",
    "      model1._validation_loss(test_loader, test_interactions.nnz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([7.1437]),\n",
       " tensor([7.1831]),\n",
       " tensor([7.1775]),\n",
       " tensor([7.1770]),\n",
       " tensor([7.1773]),\n",
       " tensor([7.1770]),\n",
       " tensor([7.1711]),\n",
       " tensor([7.1765]),\n",
       " tensor([7.1772]),\n",
       " tensor([7.1762]),\n",
       " tensor([7.1806]),\n",
       " tensor([7.1812]),\n",
       " tensor([7.1707]),\n",
       " tensor([7.1694]),\n",
       " tensor([7.1783]),\n",
       " tensor([7.1762]),\n",
       " tensor([7.1769]),\n",
       " tensor([7.1772]),\n",
       " tensor([7.1719]),\n",
       " tensor([7.1771])]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interactions = get_interaction_matrix(train, n_users, n_movies,\n",
    "                                            user_id_mapping,\n",
    "                                            movie_id_mapping)\n",
    "train_loader = data.DataLoader(Interactions(train_interactions),\n",
    "                               batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BaseModule(n_users, n_movies, n_factors=128, dropout_p=0.02)\n",
    "\n",
    "model2 = Step(net, lr=0.02, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1759/1759 [00:33<00:00, 52.20it/s]\n",
      "100%|██████████| 1759/1759 [00:37<00:00, 47.03it/s]\n",
      "100%|██████████| 1759/1759 [00:39<00:00, 44.59it/s]\n",
      "100%|██████████| 1759/1759 [00:42<00:00, 41.18it/s]\n",
      "100%|██████████| 1759/1759 [00:51<00:00, 34.31it/s]\n",
      "100%|██████████| 1759/1759 [00:55<00:00, 31.73it/s]\n",
      "100%|██████████| 1759/1759 [01:00<00:00, 34.35it/s]\n",
      "100%|██████████| 1759/1759 [00:49<00:00, 35.37it/s]\n",
      "100%|██████████| 1759/1759 [00:44<00:00, 39.32it/s]\n",
      "100%|██████████| 1759/1759 [00:48<00:00, 35.96it/s]\n",
      "100%|██████████| 1759/1759 [00:44<00:00, 39.92it/s]\n",
      "100%|██████████| 1759/1759 [00:43<00:00, 40.68it/s]\n",
      "100%|██████████| 1759/1759 [00:42<00:00, 41.83it/s]\n",
      "100%|██████████| 1759/1759 [00:41<00:00, 42.44it/s]\n",
      "100%|██████████| 1759/1759 [00:41<00:00, 42.10it/s]\n",
      "100%|██████████| 1759/1759 [00:42<00:00, 41.02it/s]\n",
      "100%|██████████| 1759/1759 [00:43<00:00, 36.03it/s]\n",
      "100%|██████████| 1759/1759 [00:44<00:00, 39.70it/s]\n",
      "100%|██████████| 1759/1759 [00:44<00:00, 39.97it/s]\n",
      "100%|██████████| 1759/1759 [00:42<00:00, 41.17it/s]\n",
      "100%|██████████| 1759/1759 [00:45<00:00, 34.71it/s]\n",
      "100%|██████████| 1759/1759 [00:44<00:00, 41.50it/s]\n",
      "100%|██████████| 1759/1759 [00:43<00:00, 40.79it/s]\n",
      "100%|██████████| 1759/1759 [00:46<00:00, 38.12it/s]\n",
      "100%|██████████| 1759/1759 [00:44<00:00, 39.19it/s]\n",
      "100%|██████████| 1759/1759 [00:45<00:00, 38.44it/s]\n",
      "100%|██████████| 1759/1759 [00:43<00:00, 40.16it/s]\n",
      "100%|██████████| 1759/1759 [00:43<00:00, 40.12it/s]\n",
      "100%|██████████| 1759/1759 [00:44<00:00, 39.44it/s]\n",
      "100%|██████████| 1759/1759 [00:42<00:00, 43.31it/s]\n",
      "100%|██████████| 1759/1759 [00:42<00:00, 41.36it/s]\n",
      "100%|██████████| 1759/1759 [00:42<00:00, 41.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 32s, sys: 11.1 s, total: 23min 43s\n",
      "Wall time: 23min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time model2.batch_fit(train_loader, train_interactions.nnz, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([5.5935]),\n",
       " tensor([1.4669]),\n",
       " tensor([1.3905]),\n",
       " tensor([1.3773]),\n",
       " tensor([1.3657]),\n",
       " tensor([1.3668]),\n",
       " tensor([1.3621]),\n",
       " tensor([1.3666]),\n",
       " tensor([1.3626]),\n",
       " tensor([1.3672]),\n",
       " tensor([1.3633]),\n",
       " tensor([1.3676]),\n",
       " tensor([1.3640]),\n",
       " tensor([1.3679]),\n",
       " tensor([1.3646]),\n",
       " tensor([1.3682]),\n",
       " tensor([1.3648]),\n",
       " tensor([1.3684]),\n",
       " tensor([1.3651]),\n",
       " tensor([1.3685]),\n",
       " tensor([1.3653]),\n",
       " tensor([1.3687]),\n",
       " tensor([1.3654]),\n",
       " tensor([1.3688]),\n",
       " tensor([1.3654]),\n",
       " tensor([1.3688]),\n",
       " tensor([1.3654]),\n",
       " tensor([1.3689]),\n",
       " tensor([1.3655]),\n",
       " tensor([1.3688]),\n",
       " tensor([1.3656]),\n",
       " tensor([1.3689])]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of model1_updated on test:  tensor(1.2263)\n"
     ]
    }
   ],
   "source": [
    "print('loss of model2_updated on test: ',\n",
    "      model2._validation_loss(test_loader, test_interactions.nnz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(Interactions(train_interactions),\n",
    "                               batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t [[tensor([0, 0], dtype=torch.int32), tensor([1, 2], dtype=torch.int32)], tensor([4., 5.])]\n",
      "1 \t [[tensor([0, 0], dtype=torch.int32), tensor([4, 5], dtype=torch.int32)], tensor([5., 3.])]\n",
      "2 \t [[tensor([0, 0], dtype=torch.int32), tensor([6, 7], dtype=torch.int32)], tensor([5., 5.])]\n",
      "3 \t [[tensor([0, 0], dtype=torch.int32), tensor([11, 12], dtype=torch.int32)], tensor([5., 5.])]\n",
      "4 \t [[tensor([0, 0], dtype=torch.int32), tensor([13, 14], dtype=torch.int32)], tensor([5., 5.])]\n",
      "5 \t [[tensor([0, 0], dtype=torch.int32), tensor([15, 16], dtype=torch.int32)], tensor([5., 5.])]\n",
      "6 \t [[tensor([0, 0], dtype=torch.int32), tensor([17, 18], dtype=torch.int32)], tensor([4., 5.])]\n",
      "7 \t [[tensor([0, 0], dtype=torch.int32), tensor([19, 20], dtype=torch.int32)], tensor([5., 4.])]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(islice(train_loader, 8)):\n",
    "    print(i, '\\t', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
