{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/varunn/.pytorch_pretrained_bert')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, delimiter='\\t', quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter=delimiter, quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing MRPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cabinet: /Users/varunn/Documents/NLP-data/MRPC/MSRParaphraseCorpus.msi\n",
      "  extracting MRPC/_2D65ED66D69C42A28B021C3E24C1D8C0\n",
      "  extracting MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D\n",
      "  extracting MRPC/_63DE49D9E7214609BE7E38DD145D8081\n",
      "  extracting MRPC/_B3CFEFE1C368459BA1D1B8A2FA07A16D\n",
      "  extracting MRPC/_C5BC91AAB1554DF3AF5E4105DE57C85A\n",
      "  extracting MRPC/_D18B15DC041F43D7925309EFFCFE0236\n",
      "  extracting MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61\n",
      "\n",
      "All done, no errors.\n"
     ]
    }
   ],
   "source": [
    "!cabextract /Users/varunn/Documents/NLP-data/MRPC/MSRParaphraseCorpus.msi -d MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $'\\r' > MRPC/msr_paraphrase_train.txt\n",
    "!cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $'\\r' > MRPC/msr_paraphrase_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: MRPC/_*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "### not required\n",
    "!rm MRPC/_*\n",
    "!rm MRPC/MSRParaphraseCorpus.msi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Quality\t#1 ID\t#2 ID\t#1 String\t#2 String\n",
      "\n",
      "1\t1089874\t1089925\tPCCW's chief operating officer, Mike Butcher, and Alex Arena, the chief financial officer, will report directly to Mr So.\tCurrent Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for line in open('MRPC/msr_paraphrase_test.txt'):\n",
    "    if count <= 2:\n",
    "        print(line)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MRPC/dev_ids.tsv', <http.client.HTTPMessage at 0x11d390208>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "mrpc_dir = 'MRPC'\n",
    "dev_mrpc_path = 'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc'\n",
    "urllib.request.urlretrieve(dev_mrpc_path, os.path.join(mrpc_dir, \"dev_ids.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train File\n",
      "Test File\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "mrpc_train_file = 'MRPC/msr_paraphrase_train.txt'\n",
    "mrpc_test_file = 'MRPC/msr_paraphrase_test.txt'\n",
    "dev_ids = []\n",
    "with io.open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding='utf-8') as ids_fh:\n",
    "    for row in ids_fh:\n",
    "        dev_ids.append(row.strip().split('\\t'))\n",
    "print('Train File')\n",
    "with io.open(mrpc_train_file, encoding='utf-8') as data_fh, \\\n",
    "io.open(os.path.join(mrpc_dir, \"train.tsv\"), 'w', encoding='utf-8') as train_fh, \\\n",
    "io.open(os.path.join(mrpc_dir, \"dev.tsv\"), 'w', encoding='utf-8') as dev_fh:\n",
    "    header = data_fh.readline()\n",
    "    train_fh.write(header)\n",
    "    dev_fh.write(header)\n",
    "    for row in data_fh:\n",
    "        label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "        if [id1, id2] in dev_ids:\n",
    "            dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "        else:\n",
    "            train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "print('Test File')\n",
    "with io.open(mrpc_test_file, encoding='utf-8') as data_fh, \\\n",
    "io.open(os.path.join(mrpc_dir, \"test.tsv\"), 'w', encoding='utf-8') as test_fh:\n",
    "    header = data_fh.readline()\n",
    "    test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "    for idx, row in enumerate(data_fh):\n",
    "        label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "        test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataProcessor._read_tsv('MRPC/dev.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1355540',\n",
       " '1355592',\n",
       " \"He said the foodservice pie business doesn't fit the company's long-term growth strategy.\",\n",
       " '\"The foodservice pie business does not fit our long-term growth strategy.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = line[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'MRPC'\n",
    "bert_model = 'bert-base-uncased'\n",
    "task_name = 'mrpc'\n",
    "output_dir = 'MRPC_Bert_Predictions'\n",
    "max_seq_length = 128\n",
    "do_train = True\n",
    "do_eval = True\n",
    "do_lower_case = True\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 3.0\n",
    "warmup_proportion = 0.1\n",
    "no_cuda = True\n",
    "local_rank = -1\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 1\n",
    "fp16 = False\n",
    "loss_scale = 0\n",
    "\n",
    "processors = {\n",
    "        \"mrpc\": MrpcProcessor,\n",
    "    }\n",
    "\n",
    "num_labels_task = {\n",
    "    \"cola\": 2,\n",
    "    \"mnli\": 3,\n",
    "    \"mrpc\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:07 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "if local_rank == -1 or no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "    device, n_gpu, bool(local_rank != -1), fp16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), '\\t', 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device, '\\t', n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gradient_accumulation_steps < 1:\n",
    "    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                     gradient_accumulation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = int(train_batch_size / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if not do_train and not do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "task_name = task_name.lower()\n",
    "\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "processor = processors[task_name]()\n",
    "num_labels = num_labels_task[task_name]\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:14 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/varunn/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:15 - INFO - __main__ -   LOOKING AT MRPC/train.tsv\n"
     ]
    }
   ],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if do_train:\n",
    "    train_examples = processor.get_train_examples(data_dir)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668, '\\t', 343, '\\t', 32, '\\t', 3.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples), '\\t', num_train_steps, '\\t', train_batch_size, '\\t', num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(3668/32 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:20 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/varunn/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/02/2019 13:00:20 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/varunn/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_y/2dty3nzx05zdf0lpd_r9bj1jbr9qbr/T/tmp3xej9ris\n",
      "04/02/2019 13:00:25 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/02/2019 13:00:28 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2019 13:00:28 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model,\n",
    "          cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n",
    "          num_labels = num_labels)\n",
    "if fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "if local_rank != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    model = DDP(model)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Apex to use distributed and fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 113, done.\u001b[K\n",
      "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
      "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
      "remote: Total 2580 (delta 58), reused 68 (delta 33), pack-reused 2467\u001b[K\n",
      "Receiving objects: 100% (2580/2580), 7.94 MiB | 759.00 KiB/s, done.\n",
      "Resolving deltas: 100% (1585/1585), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/apex.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==0.4\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\",)': /packages/6c/41/5454da652b9cf75f4b5706ef71404352b6131484bb966f05c7899e6ac25d/torch-0.4.0-cp36-cp36m-macosx_10_7_x86_64.whl\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/41/5454da652b9cf75f4b5706ef71404352b6131484bb966f05c7899e6ac25d/torch-0.4.0-cp36-cp36m-macosx_10_7_x86_64.whl (8.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.2MB 1.9MB/s ta 0:00:011\n",
      "\u001b[31mpytorch-pretrained-bert 0.3.0 has requirement torch>=0.4.1, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 1.0.42 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Found existing installation: torch 1.0.0\n",
      "    Uninstalling torch-1.0.0:\n",
      "      Successfully uninstalled torch-1.0.0\n",
      "Successfully installed torch-0.4.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "('Warning: Torch did not find available GPUs on this system.\\n', 'If your intention is to cross-compile, this is not an error.')\n",
      "('torch.__version__  = ', '0.3.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"apex/setup.py\", line 16, in <module>\n",
      "    \"The latest stable release can be obtained from https://pytorch.org/\")\n",
      "RuntimeError: APEx requires Pytorch 0.4 or newer.\n",
      "The latest stable release can be obtained from https://pytorch.org/\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "if local_rank != -1:\n",
    "    t_total = t_total // torch.distributed.get_world_size()\n",
    "if fp16:\n",
    "    try:\n",
    "        from apex.optimizers import FP16_Optimizer\n",
    "        from apex.optimizers import FusedAdam\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                          lr=learning_rate,\n",
    "                          bias_correction=False,\n",
    "                          max_grad_norm=1.0)\n",
    "    if loss_scale == 0:\n",
    "        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "    else:\n",
    "        optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n",
    "\n",
    "else:\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=learning_rate,\n",
    "                         warmup=warmup_proportion,\n",
    "                         t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-1\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-2\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] yu ##ca ##ip ##a owned dominic ##k ' s before selling the chain to safe ##way in 1998 for $ 2 . 5 billion . [SEP] yu ##ca ##ip ##a bought dominic ##k ' s in 1995 for $ 69 ##3 million and sold it to safe ##way for $ 1 . 8 billion in 1998 . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-3\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] they had published an advertisement on the internet on june 10 , offering the cargo for sale , he added . [SEP] on june 10 , the ship ' s owners had published an advertisement on the internet , offering the explosives for sale . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-4\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] around 03 ##35 gm ##t , tab shares were up 19 cents , or 4 . 4 % , at a $ 4 . 56 , having earlier set a record high of a $ 4 . 57 . [SEP] tab shares jumped 20 cents , or 4 . 6 % , to set a record closing high at a $ 4 . 57 . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-5\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] the stock rose $ 2 . 11 , or about 11 percent , to close friday at $ 21 . 51 on the new york stock exchange . [SEP] pg & e corp . shares jumped $ 1 . 63 or 8 percent to $ 21 . 03 on the new york stock exchange on friday . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2572,\n",
       " 3217,\n",
       " 5831,\n",
       " 5496,\n",
       " 2010,\n",
       " 2567,\n",
       " 1010,\n",
       " 3183,\n",
       " 2002,\n",
       " 2170,\n",
       " 1000,\n",
       " 1996,\n",
       " 7409,\n",
       " 1000,\n",
       " 1010,\n",
       " 1997,\n",
       " 9969,\n",
       " 4487,\n",
       " 23809,\n",
       " 3436,\n",
       " 2010,\n",
       " 3350,\n",
       " 1012,\n",
       " 102,\n",
       " 7727,\n",
       " 2000,\n",
       " 2032,\n",
       " 2004,\n",
       " 2069,\n",
       " 1000,\n",
       " 1996,\n",
       " 7409,\n",
       " 1000,\n",
       " 1010,\n",
       " 2572,\n",
       " 3217,\n",
       " 5831,\n",
       " 5496,\n",
       " 2010,\n",
       " 2567,\n",
       " 1997,\n",
       " 9969,\n",
       " 4487,\n",
       " 23809,\n",
       " 3436,\n",
       " 2010,\n",
       " 3350,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
       "         2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
       "         3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
       "         1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
       "         2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2107, 1037,  ...,    0,    0,    0],\n",
      "        [ 101, 2021, 2485,  ...,    0,    0,    0],\n",
      "        [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1037, 3764,  ...,    0,    0,    0],\n",
      "        [ 101, 2002, 2056,  ...,    0,    0,    0],\n",
      "        [ 101, 9530, 8490,  ...,    0,    0,    0]])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "    print(input_ids)\n",
    "    print(input_ids.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-1\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-2\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] yu ##ca ##ip ##a owned dominic ##k ' s before selling the chain to safe ##way in 1998 for $ 2 . 5 billion . [SEP] yu ##ca ##ip ##a bought dominic ##k ' s in 1995 for $ 69 ##3 million and sold it to safe ##way for $ 1 . 8 billion in 1998 . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-3\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] they had published an advertisement on the internet on june 10 , offering the cargo for sale , he added . [SEP] on june 10 , the ship ' s owners had published an advertisement on the internet , offering the explosives for sale . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-4\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] around 03 ##35 gm ##t , tab shares were up 19 cents , or 4 . 4 % , at a $ 4 . 56 , having earlier set a record high of a $ 4 . 57 . [SEP] tab shares jumped 20 cents , or 4 . 6 % , to set a record closing high at a $ 4 . 57 . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-5\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] the stock rose $ 2 . 11 , or about 11 percent , to close friday at $ 21 . 51 on the new york stock exchange . [SEP] pg & e corp . shares jumped $ 1 . 63 or 8 percent to $ 21 . 03 on the new york stock exchange on friday . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:01:21 - INFO - __main__ -   ***** Running training *****\n",
      "04/02/2019 13:01:21 - INFO - __main__ -     Num examples = 3668\n",
      "04/02/2019 13:01:21 - INFO - __main__ -     Batch size = 32\n",
      "04/02/2019 13:01:21 - INFO - __main__ -     Num steps = 343\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/115 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 1/115 [00:37<1:10:23, 37.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 2/115 [01:10<1:07:50, 36.02s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 3/115 [01:46<1:07:12, 36.00s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/115 [02:23<1:07:01, 36.23s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 5/115 [03:00<1:07:06, 36.61s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 6/115 [03:30<1:02:34, 34.44s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 7/115 [03:59<59:18, 32.95s/it]  \u001b[A\n",
      "Iteration:   7%|▋         | 8/115 [04:28<56:37, 31.75s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 9/115 [04:57<54:39, 30.94s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 10/115 [05:27<53:21, 30.49s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 11/115 [05:56<52:12, 30.12s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/115 [06:26<51:30, 30.01s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 13/115 [06:55<50:53, 29.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 14/115 [07:25<50:02, 29.73s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 15/115 [07:54<49:31, 29.72s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/115 [08:24<49:04, 29.74s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 17/115 [08:54<48:31, 29.71s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 18/115 [09:24<48:02, 29.72s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 19/115 [09:53<47:26, 29.66s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/115 [10:23<47:11, 29.81s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/115 [10:53<46:52, 29.92s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 22/115 [11:25<47:04, 30.37s/it]\u001b[A\n",
      "Iteration:  20%|██        | 23/115 [11:56<46:50, 30.55s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/115 [12:27<46:31, 30.68s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 25/115 [12:57<45:51, 30.57s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 26/115 [13:27<45:07, 30.42s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 27/115 [13:57<44:30, 30.34s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/115 [14:27<43:54, 30.28s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 29/115 [14:58<43:18, 30.21s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 30/115 [15:28<42:50, 30.25s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 31/115 [15:58<42:18, 30.22s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/115 [16:28<41:54, 30.30s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 33/115 [16:59<41:18, 30.23s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 34/115 [17:29<40:58, 30.35s/it]\u001b[A\n",
      "Iteration:  30%|███       | 35/115 [17:59<40:22, 30.28s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 36/115 [18:30<39:58, 30.37s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 37/115 [19:00<39:24, 30.31s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 38/115 [19:30<38:53, 30.31s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 39/115 [20:01<38:33, 30.44s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 40/115 [20:31<37:40, 30.15s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 41/115 [21:00<36:52, 29.90s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 42/115 [21:29<36:12, 29.77s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 43/115 [21:59<35:34, 29.65s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/115 [22:29<35:16, 29.80s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 45/115 [23:00<35:10, 30.15s/it]\u001b[A\n",
      "Iteration:  40%|████      | 46/115 [23:30<34:34, 30.07s/it]\u001b[A\n",
      "Iteration:  41%|████      | 47/115 [23:59<33:50, 29.86s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 48/115 [24:29<33:14, 29.76s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 49/115 [24:58<32:29, 29.54s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 50/115 [25:27<31:57, 29.50s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 51/115 [25:56<31:22, 29.42s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 52/115 [26:26<30:53, 29.42s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 53/115 [26:56<30:35, 29.61s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 54/115 [27:26<30:18, 29.82s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 55/115 [27:56<29:54, 29.90s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 56/115 [28:27<29:41, 30.19s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 57/115 [28:57<29:03, 30.06s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 58/115 [29:27<28:36, 30.11s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 59/115 [29:57<28:08, 30.14s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/115 [30:27<27:28, 29.98s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 61/115 [30:56<26:47, 29.77s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 62/115 [31:26<26:13, 29.69s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 63/115 [31:55<25:37, 29.57s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 64/115 [32:24<25:06, 29.54s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 65/115 [32:54<24:39, 29.60s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 66/115 [33:24<24:17, 29.74s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 67/115 [33:54<23:53, 29.87s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 68/115 [34:24<23:28, 29.96s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 69/115 [34:55<23:02, 30.06s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 70/115 [35:24<22:22, 29.84s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 71/115 [35:53<21:44, 29.66s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 72/115 [36:23<21:14, 29.63s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 73/115 [36:52<20:41, 29.55s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 74/115 [37:22<20:08, 29.48s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 75/115 [37:51<19:36, 29.41s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/115 [38:21<19:15, 29.64s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 77/115 [38:51<18:46, 29.64s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 78/115 [39:21<18:21, 29.76s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 79/115 [39:51<17:55, 29.89s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 80/115 [40:20<17:21, 29.77s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 81/115 [40:50<16:47, 29.64s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 82/115 [41:19<16:15, 29.57s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 83/115 [41:48<15:42, 29.45s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 84/115 [42:18<15:17, 29.61s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 85/115 [42:48<14:50, 29.68s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 86/115 [43:18<14:22, 29.75s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 87/115 [43:48<13:54, 29.79s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 88/115 [44:18<13:29, 29.98s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 89/115 [44:48<13:00, 30.04s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 90/115 [45:19<12:36, 30.25s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 91/115 [45:49<12:04, 30.18s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 92/115 [46:20<11:36, 30.27s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 93/115 [46:51<11:13, 30.60s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 94/115 [47:22<10:42, 30.58s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 95/115 [47:51<10:03, 30.18s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/115 [48:26<10:00, 31.63s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 97/115 [48:57<09:25, 31.41s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 98/115 [49:28<08:50, 31.22s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 99/115 [49:57<08:12, 30.75s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 100/115 [50:27<07:38, 30.55s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 101/115 [50:57<07:05, 30.38s/it]\u001b[A\n",
      "Iteration:  89%|████████▊ | 102/115 [51:27<06:34, 30.31s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 103/115 [51:58<06:04, 30.35s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 104/115 [52:29<05:35, 30.49s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 105/115 [52:59<05:03, 30.31s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 106/115 [53:28<04:30, 30.04s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 107/115 [53:57<03:58, 29.82s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 108/115 [54:27<03:28, 29.76s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 109/115 [54:56<02:57, 29.62s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 110/115 [55:26<02:28, 29.61s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 111/115 [55:55<01:57, 29.48s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/115 [56:25<01:28, 29.54s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 113/115 [56:54<00:59, 29.55s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 114/115 [57:24<00:29, 29.54s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 115/115 [57:42<00:00, 26.30s/it]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [57:42<1:55:25, 3462.99s/it]\n",
      "Iteration:   0%|          | 0/115 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/115 [00:29<56:19, 29.65s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 2/115 [00:59<56:02, 29.76s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 3/115 [01:29<55:30, 29.74s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/115 [01:58<54:41, 29.57s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 5/115 [02:27<54:01, 29.47s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 6/115 [02:57<53:24, 29.40s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 7/115 [03:26<52:50, 29.36s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/115 [03:55<52:20, 29.35s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 9/115 [04:24<51:49, 29.34s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 10/115 [04:54<51:17, 29.31s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 11/115 [05:23<50:45, 29.28s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/115 [05:52<50:21, 29.33s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 13/115 [06:23<50:24, 29.65s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  12%|█▏        | 14/115 [06:54<50:29, 30.00s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 15/115 [07:23<49:49, 29.89s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/115 [07:53<49:12, 29.82s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 17/115 [08:23<48:40, 29.80s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 18/115 [08:53<48:20, 29.90s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 19/115 [09:22<47:32, 29.71s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/115 [09:52<46:59, 29.68s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/115 [10:21<46:35, 29.74s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 22/115 [10:51<46:11, 29.80s/it]\u001b[A\n",
      "Iteration:  20%|██        | 23/115 [11:21<45:48, 29.87s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/115 [11:52<45:26, 29.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 25/115 [12:22<44:59, 29.99s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 26/115 [12:52<44:38, 30.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 27/115 [13:22<43:54, 29.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/115 [13:51<43:08, 29.75s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 29/115 [14:20<42:31, 29.67s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 30/115 [14:50<41:49, 29.52s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 31/115 [15:19<41:28, 29.63s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/115 [15:49<40:59, 29.64s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 33/115 [16:19<40:30, 29.65s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 34/115 [16:49<40:05, 29.70s/it]\u001b[A\n",
      "Iteration:  30%|███       | 35/115 [17:18<39:37, 29.72s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 36/115 [17:48<39:13, 29.79s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 37/115 [18:18<38:42, 29.78s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 38/115 [18:48<38:20, 29.87s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 39/115 [19:18<37:51, 29.89s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 40/115 [19:48<37:27, 29.97s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 41/115 [20:18<37:02, 30.04s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 42/115 [20:56<39:17, 32.30s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 43/115 [21:28<38:47, 32.33s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/115 [22:04<39:23, 33.29s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 45/115 [22:37<38:41, 33.16s/it]\u001b[A\n",
      "Iteration:  40%|████      | 46/115 [23:10<38:06, 33.14s/it]\u001b[A\n",
      "Iteration:  41%|████      | 47/115 [23:49<39:36, 34.95s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 48/115 [24:20<37:31, 33.60s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 49/115 [24:51<36:23, 33.08s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 50/115 [25:22<35:09, 32.45s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 51/115 [25:59<36:02, 33.78s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 52/115 [26:33<35:27, 33.77s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 53/115 [27:03<33:37, 32.55s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 54/115 [27:32<31:59, 31.47s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 55/115 [28:01<30:47, 30.80s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 56/115 [28:30<29:55, 30.42s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 57/115 [29:03<30:01, 31.06s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 58/115 [29:32<28:51, 30.38s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 59/115 [30:00<27:53, 29.89s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/115 [30:32<27:48, 30.34s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 61/115 [31:03<27:26, 30.50s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 62/115 [31:34<27:03, 30.62s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 63/115 [32:12<28:27, 32.83s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 64/115 [32:42<27:23, 32.22s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 65/115 [33:17<27:19, 32.79s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 66/115 [33:49<26:45, 32.76s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 67/115 [34:30<28:13, 35.28s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 68/115 [35:01<26:35, 33.95s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 69/115 [35:34<25:50, 33.71s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 70/115 [36:08<25:17, 33.73s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 71/115 [36:40<24:16, 33.10s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 72/115 [37:10<23:02, 32.15s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 73/115 [37:39<21:56, 31.33s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 74/115 [38:08<20:52, 30.55s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 75/115 [38:37<20:02, 30.06s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/115 [39:10<20:12, 31.09s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 77/115 [39:41<19:40, 31.07s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 78/115 [40:15<19:33, 31.71s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 79/115 [40:52<20:02, 33.40s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 80/115 [41:27<19:50, 34.02s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 81/115 [41:58<18:39, 32.92s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 82/115 [42:28<17:36, 32.01s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 83/115 [42:59<17:01, 31.92s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 84/115 [43:32<16:32, 32.03s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 85/115 [44:05<16:15, 32.53s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 86/115 [44:39<15:56, 32.99s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 87/115 [45:09<14:51, 31.85s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 88/115 [45:39<14:04, 31.29s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 89/115 [46:10<13:37, 31.43s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 90/115 [46:42<13:09, 31.58s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 91/115 [47:14<12:37, 31.55s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 92/115 [47:46<12:11, 31.79s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 93/115 [48:20<11:53, 32.45s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 94/115 [48:51<11:14, 32.11s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 95/115 [49:31<11:26, 34.34s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/115 [50:06<10:58, 34.66s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 97/115 [50:39<10:13, 34.11s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 98/115 [51:11<09:30, 33.55s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 99/115 [51:46<09:00, 33.81s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 100/115 [52:16<08:10, 32.71s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 101/115 [52:49<07:39, 32.83s/it]\u001b[A\n",
      "Iteration:  89%|████████▊ | 102/115 [53:20<07:00, 32.34s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 103/115 [53:50<06:19, 31.62s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 104/115 [54:23<05:52, 32.09s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 105/115 [54:56<05:22, 32.20s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 106/115 [55:27<04:46, 31.79s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 107/115 [55:57<04:09, 31.22s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 108/115 [56:26<03:35, 30.82s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 109/115 [56:59<03:07, 31.31s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 110/115 [57:33<02:41, 32.21s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 111/115 [58:08<02:11, 32.99s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/115 [58:43<01:40, 33.63s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 113/115 [59:19<01:08, 34.38s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 114/115 [59:51<00:33, 33.56s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 115/115 [1:00:11<00:00, 29.62s/it]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [1:57:54<58:27, 3507.65s/it]\n",
      "Iteration:   0%|          | 0/115 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/115 [00:31<1:00:23, 31.79s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 2/115 [01:02<59:27, 31.57s/it]  \u001b[A\n",
      "Iteration:   3%|▎         | 3/115 [01:35<59:26, 31.84s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/115 [02:07<58:52, 31.82s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 5/115 [02:39<58:29, 31.91s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 6/115 [03:08<56:44, 31.23s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 7/115 [03:38<55:10, 30.65s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/115 [04:09<54:58, 30.83s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 9/115 [04:38<53:48, 30.46s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 10/115 [05:08<53:02, 30.31s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 11/115 [05:40<53:04, 30.62s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/115 [06:10<52:19, 30.48s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 13/115 [06:42<52:28, 30.86s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 14/115 [07:17<54:21, 32.30s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 15/115 [07:50<54:03, 32.44s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/115 [08:22<53:09, 32.22s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 17/115 [08:57<54:06, 33.12s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 18/115 [09:33<55:05, 34.07s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 19/115 [10:06<53:39, 33.54s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/115 [10:38<52:30, 33.17s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/115 [11:15<53:35, 34.21s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 22/115 [11:47<52:01, 33.57s/it]\u001b[A\n",
      "Iteration:  20%|██        | 23/115 [12:19<51:07, 33.34s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/115 [12:51<49:31, 32.65s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 25/115 [13:23<48:43, 32.48s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 26/115 [13:53<47:20, 31.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  23%|██▎       | 27/115 [14:26<47:02, 32.08s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/115 [14:56<45:55, 31.67s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 29/115 [15:27<44:47, 31.25s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 30/115 [15:59<44:50, 31.66s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 31/115 [16:30<44:04, 31.49s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/115 [17:01<43:11, 31.23s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 33/115 [17:35<43:42, 31.98s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 34/115 [18:11<44:52, 33.24s/it]\u001b[A\n",
      "Iteration:  30%|███       | 35/115 [18:44<44:24, 33.31s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 36/115 [19:15<42:51, 32.56s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 37/115 [19:46<41:32, 31.95s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 38/115 [20:17<40:51, 31.84s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 39/115 [20:50<40:42, 32.13s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 40/115 [21:23<40:34, 32.46s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 41/115 [21:57<40:18, 32.69s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 42/115 [22:30<40:07, 32.97s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 43/115 [23:10<42:05, 35.08s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/115 [23:49<43:00, 36.34s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 45/115 [24:21<40:42, 34.89s/it]\u001b[A\n",
      "Iteration:  40%|████      | 46/115 [25:00<41:32, 36.13s/it]\u001b[A\n",
      "Iteration:  41%|████      | 47/115 [25:32<39:37, 34.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 48/115 [26:04<37:56, 33.97s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 49/115 [26:34<36:08, 32.85s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 50/115 [27:07<35:30, 32.77s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 51/115 [27:48<37:41, 35.34s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 52/115 [28:20<36:03, 34.33s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 53/115 [28:52<34:37, 33.51s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 54/115 [29:23<33:26, 32.89s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 55/115 [29:55<32:35, 32.59s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 56/115 [30:28<32:11, 32.73s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 57/115 [30:59<31:00, 32.08s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 58/115 [31:31<30:30, 32.11s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 59/115 [32:04<30:23, 32.56s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/115 [32:36<29:42, 32.41s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 61/115 [33:13<30:19, 33.70s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 62/115 [33:43<28:46, 32.58s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 63/115 [34:12<27:20, 31.55s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 64/115 [34:44<26:47, 31.53s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 65/115 [35:20<27:31, 33.03s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 66/115 [35:56<27:32, 33.73s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 67/115 [36:33<27:52, 34.84s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 68/115 [37:02<25:52, 33.02s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 69/115 [37:31<24:21, 31.77s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 70/115 [37:59<23:04, 30.76s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 71/115 [38:28<22:06, 30.14s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 72/115 [38:56<21:16, 29.68s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 73/115 [39:25<20:34, 29.38s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 74/115 [39:54<19:55, 29.15s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 75/115 [40:23<19:26, 29.17s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/115 [40:59<20:23, 31.38s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 77/115 [41:32<20:04, 31.70s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 78/115 [42:02<19:10, 31.11s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 79/115 [42:31<18:19, 30.54s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 80/115 [43:00<17:33, 30.10s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 81/115 [43:32<17:22, 30.65s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 82/115 [44:02<16:42, 30.38s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 83/115 [44:32<16:10, 30.32s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 84/115 [45:01<15:27, 29.93s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 85/115 [45:30<14:52, 29.75s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 86/115 [46:01<14:28, 29.96s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 87/115 [46:30<13:55, 29.85s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 88/115 [46:59<13:20, 29.63s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 89/115 [47:28<12:45, 29.44s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 90/115 [47:58<12:16, 29.45s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 91/115 [48:28<11:49, 29.56s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 92/115 [48:59<11:29, 29.99s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 93/115 [49:30<11:07, 30.33s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 94/115 [50:00<10:39, 30.46s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 95/115 [50:31<10:07, 30.39s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/115 [51:00<09:30, 30.02s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 97/115 [51:29<08:55, 29.74s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 98/115 [51:58<08:21, 29.50s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 99/115 [52:28<07:53, 29.58s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 100/115 [52:57<07:23, 29.55s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 101/115 [53:26<06:52, 29.47s/it]\u001b[A\n",
      "Iteration:  89%|████████▊ | 102/115 [53:55<06:21, 29.34s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 103/115 [54:25<05:51, 29.29s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 104/115 [54:55<05:26, 29.65s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 105/115 [55:25<04:56, 29.69s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 106/115 [55:54<04:26, 29.58s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 107/115 [56:23<03:55, 29.46s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 108/115 [56:52<03:25, 29.35s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 109/115 [57:22<02:55, 29.29s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 110/115 [57:51<02:26, 29.20s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 111/115 [58:20<01:56, 29.24s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/115 [58:49<01:27, 29.24s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 113/115 [59:18<00:58, 29.24s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 114/115 [59:48<00:29, 29.21s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 115/115 [1:00:06<00:00, 26.03s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [2:58:01<00:00, 3537.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# training begins\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "if do_train:\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if local_rank == -1:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = DistributedSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            if fp16:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "if do_train:\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 16:18:42 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/varunn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/02/2019 16:18:42 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/varunn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_y/2dty3nzx05zdf0lpd_r9bj1jbr9qbr/T/tmppphemfdg\n",
      "04/02/2019 16:18:46 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/02/2019 16:18:49 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2019 16:18:49 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file, map_location=device)\n",
    "model1 = BertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
    "model1.load_state_dict(model_state_dict)\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "print(model1.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-1\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] \" the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-2\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] magna ##relli said ra ##cic ##ot hated the iraqi regime and looked forward to using his long years of training in the war . [SEP] his wife said he was \" 100 percent behind george bush \" and looked forward to using his years of training in the war . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-3\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] the dollar was at 116 . 92 yen against the yen , flat on the session , and at 1 . 289 ##1 against the swiss fran ##c , also flat . [SEP] the dollar was at 116 . 78 yen jp ##y = , virtually flat on the session , and at 1 . 287 ##1 against the swiss fran ##c ch ##f = , down 0 . 1 percent . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-4\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] the afl - ci ##o is waiting until october to decide if it will end ##ors ##e a candidate . [SEP] the afl - ci ##o announced wednesday that it will decide in october whether to end ##ors ##e a candidate before the primaries . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-5\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] no dates have been set for the civil or the criminal trial . [SEP] no dates have been set for the criminal or civil cases , but shan ##ley has pleaded not guilty . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2019 16:19:42 - INFO - __main__ -     Num examples = 408\n",
      "04/02/2019 16:19:42 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating:   2%|▏         | 1/51 [00:04<04:03,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1818,  2.7929],\n",
      "        [ 1.3366, -1.0892],\n",
      "        [ 0.9410, -0.9200],\n",
      "        [-2.9937,  2.3675],\n",
      "        [ 1.3348, -1.0914],\n",
      "        [-2.2992,  1.5050],\n",
      "        [-1.2822,  0.4767],\n",
      "        [-3.1211,  2.5602]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   4%|▍         | 2/51 [00:09<03:59,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5136,  1.7666],\n",
      "        [-2.7615,  2.0549],\n",
      "        [-3.1807,  2.7941],\n",
      "        [ 1.1995, -1.0859],\n",
      "        [ 1.2775, -1.0279],\n",
      "        [-3.1848,  2.6668],\n",
      "        [-3.1569,  2.7890],\n",
      "        [-2.3192,  1.5652]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   6%|▌         | 3/51 [00:14<03:46,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2211,  2.7893],\n",
      "        [ 1.2416, -1.0223],\n",
      "        [-3.1625,  2.7746],\n",
      "        [ 1.3238, -1.0293],\n",
      "        [ 1.3748, -1.0547],\n",
      "        [-1.9296,  1.0845],\n",
      "        [ 1.3315, -1.0549],\n",
      "        [-3.2023,  2.7855]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   8%|▊         | 4/51 [00:18<03:31,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1205,  1.3320],\n",
      "        [-1.7488,  0.9182],\n",
      "        [-1.0321,  0.2851],\n",
      "        [-3.1517,  2.7674],\n",
      "        [-2.0852,  1.2828],\n",
      "        [-3.0831,  2.4896],\n",
      "        [-0.1843, -0.2877],\n",
      "        [-3.1298,  2.7600]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  10%|▉         | 5/51 [00:22<03:26,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0837,  1.2951],\n",
      "        [-1.9656,  1.1345],\n",
      "        [-3.2019,  2.7478],\n",
      "        [-2.3388,  1.5938],\n",
      "        [ 1.1030, -1.0202],\n",
      "        [ 1.3979, -1.0844],\n",
      "        [-3.1980,  2.7318],\n",
      "        [-3.1456,  2.7712]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  12%|█▏        | 6/51 [00:27<03:29,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4076, -0.1481],\n",
      "        [-2.8235,  2.1746],\n",
      "        [ 0.1338, -0.4460],\n",
      "        [ 1.3643, -1.0839],\n",
      "        [ 1.3143, -1.0564],\n",
      "        [-3.2090,  2.7757],\n",
      "        [-3.2254,  2.7678],\n",
      "        [ 1.3299, -1.0902]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  14%|█▎        | 7/51 [00:32<03:25,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2129,  2.7667],\n",
      "        [-3.1985,  2.7089],\n",
      "        [-2.3067,  1.5358],\n",
      "        [-1.5383,  0.7170],\n",
      "        [-3.0297,  2.3977],\n",
      "        [-3.1982,  2.7742],\n",
      "        [-2.9573,  2.3057],\n",
      "        [-3.2050,  2.7270]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  16%|█▌        | 8/51 [00:36<03:18,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1743, -1.0269],\n",
      "        [-3.2017,  2.7285],\n",
      "        [-3.1944,  2.7932],\n",
      "        [-3.0926,  2.4764],\n",
      "        [-1.0581,  0.2911],\n",
      "        [ 1.2766, -1.0675],\n",
      "        [-3.1875,  2.6808],\n",
      "        [-2.7346,  2.0415]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  18%|█▊        | 9/51 [00:41<03:09,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0669,  2.5105],\n",
      "        [ 1.3140, -1.0426],\n",
      "        [-3.1558,  2.7663],\n",
      "        [-3.1909,  2.7767],\n",
      "        [-1.9124,  1.0681],\n",
      "        [-3.2033,  2.7411],\n",
      "        [-3.2060,  2.7892],\n",
      "        [ 0.8407, -0.8689]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  20%|█▉        | 10/51 [00:45<02:59,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2109,  2.8005],\n",
      "        [-2.1449,  1.3296],\n",
      "        [-2.8536,  2.1929],\n",
      "        [-1.5405,  0.6902],\n",
      "        [-3.0333,  2.3964],\n",
      "        [-3.1767,  2.7695],\n",
      "        [-3.2149,  2.7159],\n",
      "        [-3.0994,  2.5121]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  22%|██▏       | 11/51 [00:49<02:56,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5099,  0.6397],\n",
      "        [-1.9535,  1.1056],\n",
      "        [-3.1567,  2.7229],\n",
      "        [-0.7058,  0.0464],\n",
      "        [-3.1954,  2.7818],\n",
      "        [-0.9743,  0.1974],\n",
      "        [-3.0007,  2.3635],\n",
      "        [-1.2765,  0.4788]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  24%|██▎       | 12/51 [00:54<02:53,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1968,  2.7983],\n",
      "        [-3.1400,  2.7710],\n",
      "        [-0.1323, -0.3073],\n",
      "        [-3.2106,  2.7798],\n",
      "        [-3.0609,  2.4845],\n",
      "        [-2.7962,  2.0339],\n",
      "        [-3.0390,  2.4558],\n",
      "        [-3.1879,  2.7819]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  25%|██▌       | 13/51 [00:59<02:54,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0179, -0.9779],\n",
      "        [-3.0278,  2.3962],\n",
      "        [-3.0475,  2.4421],\n",
      "        [-2.5764,  1.8436],\n",
      "        [-3.1079,  2.5155],\n",
      "        [-0.8198,  0.1153],\n",
      "        [-2.2475,  1.4614],\n",
      "        [-3.2229,  2.7572]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  27%|██▋       | 14/51 [01:03<02:45,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3043, -0.5447],\n",
      "        [-3.1994,  2.7313],\n",
      "        [-1.8163,  0.9421],\n",
      "        [ 1.3918, -1.0574],\n",
      "        [ 1.1817, -1.0261],\n",
      "        [-3.1716,  2.6331],\n",
      "        [ 0.3933, -0.6101],\n",
      "        [-3.1454,  2.5975]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  29%|██▉       | 15/51 [01:07<02:41,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4248,  1.7043],\n",
      "        [-3.1404,  2.7663],\n",
      "        [ 0.1137, -0.4335],\n",
      "        [ 1.2449, -1.0316],\n",
      "        [-2.8445,  2.1942],\n",
      "        [-3.1880,  2.7827],\n",
      "        [-3.1856,  2.7871],\n",
      "        [-3.1935,  2.7730]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  31%|███▏      | 16/51 [01:12<02:40,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2100,  2.6788],\n",
      "        [-1.2290,  0.4393],\n",
      "        [ 1.3714, -1.0560],\n",
      "        [-3.1639,  2.6606],\n",
      "        [-3.1450,  2.6107],\n",
      "        [-3.2163,  2.7279],\n",
      "        [-3.0142,  2.3958],\n",
      "        [ 1.3420, -1.0953]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  33%|███▎      | 17/51 [01:17<02:36,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1783,  2.7930],\n",
      "        [-2.5962,  1.8597],\n",
      "        [-1.9497,  1.1282],\n",
      "        [ 1.1526, -1.0123],\n",
      "        [-3.0561,  2.4247],\n",
      "        [-0.0360, -0.3656],\n",
      "        [-1.7644,  0.9708],\n",
      "        [-1.4805,  0.6539]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  35%|███▌      | 18/51 [01:22<02:37,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5021,  1.7792],\n",
      "        [ 1.2467, -1.0385],\n",
      "        [-3.1753,  2.7889],\n",
      "        [-2.9068,  2.2818],\n",
      "        [-3.1693,  2.6497],\n",
      "        [ 0.1489, -0.4795],\n",
      "        [ 1.3980, -1.0752],\n",
      "        [-3.1631,  2.6243]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  37%|███▋      | 19/51 [01:27<02:35,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3493, -1.0718],\n",
      "        [-0.0705, -0.3544],\n",
      "        [-3.2117,  2.7914],\n",
      "        [-0.8289,  0.1991],\n",
      "        [ 0.4038, -0.6140],\n",
      "        [-1.6331,  0.7842],\n",
      "        [ 1.3563, -1.0830],\n",
      "        [-2.4526,  1.7108]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  39%|███▉      | 20/51 [01:31<02:25,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9536,  1.1252],\n",
      "        [-3.2088,  2.7216],\n",
      "        [-1.3331,  0.5007],\n",
      "        [-3.0982,  2.5204],\n",
      "        [-3.1132,  2.5583],\n",
      "        [-2.5822,  1.8298],\n",
      "        [ 0.9970, -0.9455],\n",
      "        [-2.0234,  1.2269]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  41%|████      | 21/51 [01:36<02:18,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7242,  2.0196],\n",
      "        [-2.9499,  2.2998],\n",
      "        [-3.1997,  2.7970],\n",
      "        [-3.1317,  2.7522],\n",
      "        [-3.2071,  2.7130],\n",
      "        [-3.1903,  2.7440],\n",
      "        [-1.5830,  0.7405],\n",
      "        [ 0.2505, -0.5091]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  43%|████▎     | 22/51 [01:40<02:11,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4642,  0.6133],\n",
      "        [ 1.1757, -1.0473],\n",
      "        [ 0.3019, -0.5543],\n",
      "        [-1.6345,  0.7953],\n",
      "        [-0.9621,  0.2114],\n",
      "        [-2.9112,  2.2258],\n",
      "        [-1.0607,  0.2863],\n",
      "        [-1.0961,  0.3208]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  45%|████▌     | 23/51 [01:44<02:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8694,  1.0229],\n",
      "        [ 0.7884, -0.8344],\n",
      "        [-3.1739,  2.7778],\n",
      "        [-2.8865,  2.2046],\n",
      "        [-1.6830,  0.8193],\n",
      "        [-2.3050,  1.5067],\n",
      "        [-2.5492,  1.7894],\n",
      "        [-3.1890,  2.7614]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  47%|████▋     | 24/51 [01:49<01:58,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4889,  0.6661],\n",
      "        [-2.4364,  1.6658],\n",
      "        [ 1.3435, -1.0801],\n",
      "        [-0.9731,  0.2438],\n",
      "        [ 1.3693, -1.0814],\n",
      "        [-2.3653,  1.6406],\n",
      "        [-3.2158,  2.7622],\n",
      "        [ 1.3600, -1.0969]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  49%|████▉     | 25/51 [01:53<01:54,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5511, -0.0622],\n",
      "        [-3.1615,  2.7676],\n",
      "        [-1.4607,  0.6183],\n",
      "        [-2.1695,  1.3731],\n",
      "        [-3.1919,  2.7752],\n",
      "        [ 0.4148, -0.6010],\n",
      "        [-2.5478,  1.8654],\n",
      "        [-2.0784,  1.2436]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  51%|█████     | 26/51 [01:58<01:57,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1308,  1.3970],\n",
      "        [-1.5608,  0.7371],\n",
      "        [ 1.3607, -1.0632],\n",
      "        [-2.5188,  1.7784],\n",
      "        [-1.6605,  0.8169],\n",
      "        [ 1.3465, -1.0766],\n",
      "        [-3.1250,  2.6092],\n",
      "        [-2.1401,  1.3606]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  53%|█████▎    | 27/51 [02:03<01:52,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7743,  0.0793],\n",
      "        [ 0.8036, -0.8407],\n",
      "        [ 0.8722, -0.8987],\n",
      "        [-3.1756,  2.6570],\n",
      "        [-0.7237,  0.0412],\n",
      "        [ 1.3261, -1.0507],\n",
      "        [-3.0141,  2.4124],\n",
      "        [-3.1564,  2.7652]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  55%|█████▍    | 28/51 [02:08<01:47,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7211,  0.8675],\n",
      "        [-2.9397,  2.2926],\n",
      "        [ 1.2882, -1.0855],\n",
      "        [-3.0499,  2.4557],\n",
      "        [-2.2374,  1.4764],\n",
      "        [-2.9414,  2.3583],\n",
      "        [-3.1988,  2.6888],\n",
      "        [ 1.3338, -1.0734]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  57%|█████▋    | 29/51 [02:12<01:42,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1733,  2.7753],\n",
      "        [-3.1840,  2.7741],\n",
      "        [-3.1533,  2.7741],\n",
      "        [-1.5058,  0.5978],\n",
      "        [-1.9600,  1.1219],\n",
      "        [-1.9383,  1.1369],\n",
      "        [-2.8081,  2.1412],\n",
      "        [-2.3521,  1.6039]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  59%|█████▉    | 30/51 [02:17<01:34,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2722e-01, -1.3810e-01],\n",
      "        [-4.5669e-01, -1.2393e-01],\n",
      "        [-8.2717e-01,  1.2975e-01],\n",
      "        [-6.6382e-01,  1.1546e-03],\n",
      "        [-1.3805e+00,  5.2609e-01],\n",
      "        [ 1.0202e+00, -9.7470e-01],\n",
      "        [-2.0344e+00,  1.2020e+00],\n",
      "        [-1.5958e-01, -2.9451e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  61%|██████    | 31/51 [02:21<01:28,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0668,  2.5262],\n",
      "        [ 1.3566, -1.0945],\n",
      "        [ 0.3718, -0.5925],\n",
      "        [-2.5327,  1.8166],\n",
      "        [-2.3661,  1.6811],\n",
      "        [-1.9523,  1.1035],\n",
      "        [-2.7422,  2.0217],\n",
      "        [-2.0635,  1.2453]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  63%|██████▎   | 32/51 [02:25<01:23,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8425,  2.1759],\n",
      "        [-2.9399,  2.3148],\n",
      "        [-2.6125,  1.8908],\n",
      "        [ 0.6652, -0.7683],\n",
      "        [ 0.0450, -0.4019],\n",
      "        [-1.5507,  0.7113],\n",
      "        [ 1.1428, -1.0135],\n",
      "        [ 1.4285, -1.0767]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  65%|██████▍   | 33/51 [02:29<01:18,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2875,  1.5203],\n",
      "        [-2.5832,  1.8941],\n",
      "        [-2.2116,  1.4114],\n",
      "        [-0.4674, -0.0959],\n",
      "        [-1.9173,  1.1143],\n",
      "        [-3.0804,  2.4971],\n",
      "        [-2.7803,  2.0696],\n",
      "        [-1.3149,  0.5095]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  67%|██████▋   | 34/51 [02:34<01:13,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5956,  1.7718],\n",
      "        [-0.7749,  0.0955],\n",
      "        [ 0.8500, -0.8538],\n",
      "        [ 0.2823, -0.5431],\n",
      "        [ 1.3724, -1.0500],\n",
      "        [-2.3651,  1.5904],\n",
      "        [ 0.9246, -0.9213],\n",
      "        [-2.6482,  1.9284]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  69%|██████▊   | 35/51 [02:38<01:11,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1331,  2.5524],\n",
      "        [-3.1872,  2.7934],\n",
      "        [-3.1479,  2.7084],\n",
      "        [-3.1832,  2.7086],\n",
      "        [-3.1866,  2.7022],\n",
      "        [-0.6897,  0.0250],\n",
      "        [-2.6012,  1.8670],\n",
      "        [ 1.3289, -1.0675]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  71%|███████   | 36/51 [02:43<01:06,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1308,  1.3318],\n",
      "        [-0.8877,  0.1763],\n",
      "        [-2.1823,  1.4274],\n",
      "        [ 1.3378, -1.0816],\n",
      "        [ 0.2745, -0.5069],\n",
      "        [-3.1980,  2.7238],\n",
      "        [-2.7059,  1.9726],\n",
      "        [-2.1275,  1.3332]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  73%|███████▎  | 37/51 [02:48<01:03,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2078,  2.7776],\n",
      "        [ 1.2104, -1.0308],\n",
      "        [ 0.4212, -0.6100],\n",
      "        [ 1.2925, -1.0791],\n",
      "        [-3.1951,  2.7925],\n",
      "        [-2.0279,  1.2188],\n",
      "        [-2.7700,  2.0845],\n",
      "        [ 0.9598, -0.9308]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  75%|███████▍  | 38/51 [02:53<01:00,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3588, -1.0907],\n",
      "        [-1.1043,  0.3275],\n",
      "        [-3.1665,  2.7765],\n",
      "        [-1.3034,  0.5314],\n",
      "        [-3.1491,  2.7707],\n",
      "        [-3.1922,  2.6970],\n",
      "        [-3.0859,  2.5194],\n",
      "        [ 0.9119, -0.8955]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  76%|███████▋  | 39/51 [02:57<00:53,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1945,  2.7872],\n",
      "        [-3.1947,  2.7113],\n",
      "        [ 1.4139, -1.0936],\n",
      "        [-3.2007,  2.7759],\n",
      "        [ 1.3869, -1.0983],\n",
      "        [-0.9958,  0.2351],\n",
      "        [-3.0112,  2.3651],\n",
      "        [-3.2038,  2.7998]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  78%|███████▊  | 40/51 [03:01<00:48,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2421,  0.4883],\n",
      "        [ 1.2034, -1.0511],\n",
      "        [-3.1963,  2.7828],\n",
      "        [ 0.9999, -0.9657],\n",
      "        [-1.2790,  0.4466],\n",
      "        [-3.1735,  2.6451],\n",
      "        [ 1.2365, -1.0549],\n",
      "        [ 0.5568, -0.7051]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  80%|████████  | 41/51 [03:05<00:43,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3423, -1.0910],\n",
      "        [ 1.2792, -1.0619],\n",
      "        [ 1.2771, -1.0361],\n",
      "        [-2.0934,  1.3338],\n",
      "        [ 0.5900, -0.7255],\n",
      "        [-3.1457,  2.7758],\n",
      "        [-2.3611,  1.6411],\n",
      "        [-3.2081,  2.7919]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  82%|████████▏ | 42/51 [03:09<00:38,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.6679,  1.9419],\n",
      "        [-1.9656,  1.1518],\n",
      "        [-3.1301,  2.5764],\n",
      "        [-3.1795,  2.6576],\n",
      "        [-1.4228,  0.5777],\n",
      "        [-2.1648,  1.3741],\n",
      "        [-3.1820,  2.7828],\n",
      "        [-3.0221,  2.4067]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  84%|████████▍ | 43/51 [03:13<00:34,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2143,  2.7632],\n",
      "        [-2.9056,  2.2608],\n",
      "        [ 1.3182, -1.0793],\n",
      "        [-2.1765,  1.4223],\n",
      "        [-3.1154,  2.5835],\n",
      "        [-3.1728,  2.7677],\n",
      "        [ 1.3403, -1.0925],\n",
      "        [-0.5491, -0.0611]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  86%|████████▋ | 44/51 [03:18<00:30,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2153,  2.7373],\n",
      "        [-3.2096,  2.7976],\n",
      "        [-3.2036,  2.7591],\n",
      "        [-3.2162,  2.7959],\n",
      "        [-0.9563,  0.2273],\n",
      "        [-3.2012,  2.7447],\n",
      "        [ 1.1923, -1.0526],\n",
      "        [-3.1991,  2.7564]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  88%|████████▊ | 45/51 [03:23<00:26,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2668,  1.4232],\n",
      "        [-2.3256,  1.5842],\n",
      "        [-2.7625,  2.0707],\n",
      "        [ 1.3068, -1.0516],\n",
      "        [-2.4183,  1.6585],\n",
      "        [-2.5501,  1.8183],\n",
      "        [-2.9358,  2.3314],\n",
      "        [-2.3695,  1.6170]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  90%|█████████ | 46/51 [03:27<00:22,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9610, -0.9278],\n",
      "        [-2.9314,  2.2895],\n",
      "        [-3.1449,  2.7668],\n",
      "        [-1.8638,  1.0022],\n",
      "        [-2.7574,  2.0107],\n",
      "        [-3.2009,  2.7993],\n",
      "        [-3.1954,  2.7033],\n",
      "        [-1.6754,  0.8472]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  92%|█████████▏| 47/51 [03:32<00:18,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1476, -1.0601],\n",
      "        [ 1.3759, -1.0547],\n",
      "        [-0.9250,  0.1878],\n",
      "        [-3.1579,  2.6812],\n",
      "        [-2.6467,  1.9231],\n",
      "        [ 0.1745, -0.5052],\n",
      "        [ 1.3546, -1.0861],\n",
      "        [ 1.0821, -1.0020]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  94%|█████████▍| 48/51 [03:36<00:13,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3477, -0.5600],\n",
      "        [ 1.4137, -1.0883],\n",
      "        [-2.0451,  1.2514],\n",
      "        [-3.2081,  2.7173],\n",
      "        [-1.8683,  1.0134],\n",
      "        [-3.1964,  2.8001],\n",
      "        [-0.3271, -0.2212],\n",
      "        [-3.1510,  2.7615]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  96%|█████████▌| 49/51 [03:41<00:09,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9890,  2.3864],\n",
      "        [-0.3441, -0.1820],\n",
      "        [-2.5237,  1.7909],\n",
      "        [ 1.1096, -1.0121],\n",
      "        [-3.1888,  2.7937],\n",
      "        [-0.7072,  0.0301],\n",
      "        [-3.1906,  2.6910],\n",
      "        [-0.7787,  0.1050]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  98%|█████████▊| 50/51 [03:46<00:04,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2151,  2.7859],\n",
      "        [-3.1834,  2.6645],\n",
      "        [-2.0960,  1.2165],\n",
      "        [-0.1442, -0.2922],\n",
      "        [-3.2159,  2.7947],\n",
      "        [-0.9473,  0.2199],\n",
      "        [-2.5020,  1.7557],\n",
      "        [-3.1770,  2.6475]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 51/51 [03:51<00:00,  4.68s/it]\n",
      "04/02/2019 16:23:33 - INFO - __main__ -   ***** Eval results *****\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     eval_accuracy = 0.8602941176470589\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     eval_loss = 0.415484676977583\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     global_step = 345\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     loss = 0.18990675794041675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3128, -1.0338],\n",
      "        [-3.2089,  2.7509],\n",
      "        [-1.9044,  1.0529],\n",
      "        [ 1.4479, -1.0434],\n",
      "        [ 0.1515, -0.4591],\n",
      "        [-3.1568,  2.7844],\n",
      "        [ 1.2631, -1.0399],\n",
      "        [-2.4895,  1.8019]])\n"
     ]
    }
   ],
   "source": [
    "if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    eval_examples = processor.get_dev_examples(data_dir)\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "        print(logits)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss/nb_tr_steps if do_train else None\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "              'global_step': global_step,\n",
    "              'loss': loss}\n",
    "\n",
    "    output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.415484676977583,\n",
       " 'eval_accuracy': 0.8602941176470589,\n",
       " 'global_step': 345,\n",
       " 'loss': 0.18990675794041675}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
