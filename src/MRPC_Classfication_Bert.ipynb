{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/varunn/.pytorch_pretrained_bert')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, delimiter='\\t', quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter=delimiter, quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing MRPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cabinet: /Users/varunn/Documents/NLP-data/MRPC/MSRParaphraseCorpus.msi\n",
      "  extracting MRPC/_2D65ED66D69C42A28B021C3E24C1D8C0\n",
      "  extracting MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D\n",
      "  extracting MRPC/_63DE49D9E7214609BE7E38DD145D8081\n",
      "  extracting MRPC/_B3CFEFE1C368459BA1D1B8A2FA07A16D\n",
      "  extracting MRPC/_C5BC91AAB1554DF3AF5E4105DE57C85A\n",
      "  extracting MRPC/_D18B15DC041F43D7925309EFFCFE0236\n",
      "  extracting MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61\n",
      "\n",
      "All done, no errors.\n"
     ]
    }
   ],
   "source": [
    "!cabextract /Users/varunn/Documents/NLP-data/MRPC/MSRParaphraseCorpus.msi -d MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $'\\r' > MRPC/msr_paraphrase_train.txt\n",
    "!cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $'\\r' > MRPC/msr_paraphrase_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: MRPC/_*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "### not required\n",
    "!rm MRPC/_*\n",
    "!rm MRPC/MSRParaphraseCorpus.msi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Quality\t#1 ID\t#2 ID\t#1 String\t#2 String\n",
      "\n",
      "1\t1089874\t1089925\tPCCW's chief operating officer, Mike Butcher, and Alex Arena, the chief financial officer, will report directly to Mr So.\tCurrent Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for line in open('MRPC/msr_paraphrase_test.txt'):\n",
    "    if count <= 2:\n",
    "        print(line)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MRPC/dev_ids.tsv', <http.client.HTTPMessage at 0x11d390208>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "mrpc_dir = 'MRPC'\n",
    "dev_mrpc_path = 'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc'\n",
    "urllib.request.urlretrieve(dev_mrpc_path, os.path.join(mrpc_dir, \"dev_ids.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train File\n",
      "Test File\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "mrpc_train_file = 'MRPC/msr_paraphrase_train.txt'\n",
    "mrpc_test_file = 'MRPC/msr_paraphrase_test.txt'\n",
    "dev_ids = []\n",
    "with io.open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding='utf-8') as ids_fh:\n",
    "    for row in ids_fh:\n",
    "        dev_ids.append(row.strip().split('\\t'))\n",
    "print('Train File')\n",
    "with io.open(mrpc_train_file, encoding='utf-8') as data_fh, \\\n",
    "io.open(os.path.join(mrpc_dir, \"train.tsv\"), 'w', encoding='utf-8') as train_fh, \\\n",
    "io.open(os.path.join(mrpc_dir, \"dev.tsv\"), 'w', encoding='utf-8') as dev_fh:\n",
    "    header = data_fh.readline()\n",
    "    train_fh.write(header)\n",
    "    dev_fh.write(header)\n",
    "    for row in data_fh:\n",
    "        label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "        if [id1, id2] in dev_ids:\n",
    "            dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "        else:\n",
    "            train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "print('Test File')\n",
    "with io.open(mrpc_test_file, encoding='utf-8') as data_fh, \\\n",
    "io.open(os.path.join(mrpc_dir, \"test.tsv\"), 'w', encoding='utf-8') as test_fh:\n",
    "    header = data_fh.readline()\n",
    "    test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "    for idx, row in enumerate(data_fh):\n",
    "        label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "        test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataProcessor._read_tsv('MRPC/dev.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1355540',\n",
       " '1355592',\n",
       " \"He said the foodservice pie business doesn't fit the company's long-term growth strategy.\",\n",
       " '\"The foodservice pie business does not fit our long-term growth strategy.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = line[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'MRPC'\n",
    "bert_model = 'bert-base-uncased'\n",
    "task_name = 'mrpc'\n",
    "output_dir = 'MRPC_Bert_Predictions'\n",
    "max_seq_length = 128\n",
    "do_train = True\n",
    "do_eval = True\n",
    "do_lower_case = True\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 3.0\n",
    "warmup_proportion = 0.1\n",
    "no_cuda = True\n",
    "local_rank = -1\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 1\n",
    "fp16 = False\n",
    "loss_scale = 0\n",
    "\n",
    "processors = {\n",
    "        \"mrpc\": MrpcProcessor,\n",
    "    }\n",
    "\n",
    "num_labels_task = {\n",
    "    \"cola\": 2,\n",
    "    \"mnli\": 3,\n",
    "    \"mrpc\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:07 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "if local_rank == -1 or no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "    device, n_gpu, bool(local_rank != -1), fp16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), '\\t', 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device, '\\t', n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gradient_accumulation_steps < 1:\n",
    "    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                     gradient_accumulation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = int(train_batch_size / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if not do_train and not do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "task_name = task_name.lower()\n",
    "\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "processor = processors[task_name]()\n",
    "num_labels = num_labels_task[task_name]\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:14 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/varunn/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:15 - INFO - __main__ -   LOOKING AT MRPC/train.tsv\n"
     ]
    }
   ],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if do_train:\n",
    "    train_examples = processor.get_train_examples(data_dir)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668, '\\t', 343, '\\t', 32, '\\t', 3.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples), '\\t', num_train_steps, '\\t', train_batch_size, '\\t', num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(3668/32 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:20 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/varunn/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/02/2019 13:00:20 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/varunn/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_y/2dty3nzx05zdf0lpd_r9bj1jbr9qbr/T/tmp3xej9ris\n",
      "04/02/2019 13:00:25 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/02/2019 13:00:28 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2019 13:00:28 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model,\n",
    "          cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n",
    "          num_labels = num_labels)\n",
    "if fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "if local_rank != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    model = DDP(model)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Apex to use distributed and fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 113, done.\u001b[K\n",
      "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
      "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
      "remote: Total 2580 (delta 58), reused 68 (delta 33), pack-reused 2467\u001b[K\n",
      "Receiving objects: 100% (2580/2580), 7.94 MiB | 759.00 KiB/s, done.\n",
      "Resolving deltas: 100% (1585/1585), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/apex.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==0.4\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\",)': /packages/6c/41/5454da652b9cf75f4b5706ef71404352b6131484bb966f05c7899e6ac25d/torch-0.4.0-cp36-cp36m-macosx_10_7_x86_64.whl\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/41/5454da652b9cf75f4b5706ef71404352b6131484bb966f05c7899e6ac25d/torch-0.4.0-cp36-cp36m-macosx_10_7_x86_64.whl (8.2MB)\n",
      "\u001b[K    100% |ââââââââââââââââââââââââââââââââ| 8.2MB 1.9MB/s ta 0:00:011\n",
      "\u001b[31mpytorch-pretrained-bert 0.3.0 has requirement torch>=0.4.1, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 1.0.42 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Found existing installation: torch 1.0.0\n",
      "    Uninstalling torch-1.0.0:\n",
      "      Successfully uninstalled torch-1.0.0\n",
      "Successfully installed torch-0.4.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "('Warning: Torch did not find available GPUs on this system.\\n', 'If your intention is to cross-compile, this is not an error.')\n",
      "('torch.__version__  = ', '0.3.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"apex/setup.py\", line 16, in <module>\n",
      "    \"The latest stable release can be obtained from https://pytorch.org/\")\n",
      "RuntimeError: APEx requires Pytorch 0.4 or newer.\n",
      "The latest stable release can be obtained from https://pytorch.org/\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "if local_rank != -1:\n",
    "    t_total = t_total // torch.distributed.get_world_size()\n",
    "if fp16:\n",
    "    try:\n",
    "        from apex.optimizers import FP16_Optimizer\n",
    "        from apex.optimizers import FusedAdam\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                          lr=learning_rate,\n",
    "                          bias_correction=False,\n",
    "                          max_grad_norm=1.0)\n",
    "    if loss_scale == 0:\n",
    "        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "    else:\n",
    "        optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n",
    "\n",
    "else:\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=learning_rate,\n",
    "                         warmup=warmup_proportion,\n",
    "                         t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-1\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-2\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] yu ##ca ##ip ##a owned dominic ##k ' s before selling the chain to safe ##way in 1998 for $ 2 . 5 billion . [SEP] yu ##ca ##ip ##a bought dominic ##k ' s in 1995 for $ 69 ##3 million and sold it to safe ##way for $ 1 . 8 billion in 1998 . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-3\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] they had published an advertisement on the internet on june 10 , offering the cargo for sale , he added . [SEP] on june 10 , the ship ' s owners had published an advertisement on the internet , offering the explosives for sale . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-4\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] around 03 ##35 gm ##t , tab shares were up 19 cents , or 4 . 4 % , at a $ 4 . 56 , having earlier set a record high of a $ 4 . 57 . [SEP] tab shares jumped 20 cents , or 4 . 6 % , to set a record closing high at a $ 4 . 57 . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   guid: train-5\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   tokens: [CLS] the stock rose $ 2 . 11 , or about 11 percent , to close friday at $ 21 . 51 on the new york stock exchange . [SEP] pg & e corp . shares jumped $ 1 . 63 or 8 percent to $ 21 . 03 on the new york stock exchange on friday . [SEP]\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:00:43 - INFO - __main__ -   label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2572,\n",
       " 3217,\n",
       " 5831,\n",
       " 5496,\n",
       " 2010,\n",
       " 2567,\n",
       " 1010,\n",
       " 3183,\n",
       " 2002,\n",
       " 2170,\n",
       " 1000,\n",
       " 1996,\n",
       " 7409,\n",
       " 1000,\n",
       " 1010,\n",
       " 1997,\n",
       " 9969,\n",
       " 4487,\n",
       " 23809,\n",
       " 3436,\n",
       " 2010,\n",
       " 3350,\n",
       " 1012,\n",
       " 102,\n",
       " 7727,\n",
       " 2000,\n",
       " 2032,\n",
       " 2004,\n",
       " 2069,\n",
       " 1000,\n",
       " 1996,\n",
       " 7409,\n",
       " 1000,\n",
       " 1010,\n",
       " 2572,\n",
       " 3217,\n",
       " 5831,\n",
       " 5496,\n",
       " 2010,\n",
       " 2567,\n",
       " 1997,\n",
       " 9969,\n",
       " 4487,\n",
       " 23809,\n",
       " 3436,\n",
       " 2010,\n",
       " 3350,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
       "         2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
       "         3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
       "         1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
       "         2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2107, 1037,  ...,    0,    0,    0],\n",
      "        [ 101, 2021, 2485,  ...,    0,    0,    0],\n",
      "        [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1037, 3764,  ...,    0,    0,    0],\n",
      "        [ 101, 2002, 2056,  ...,    0,    0,    0],\n",
      "        [ 101, 9530, 8490,  ...,    0,    0,    0]])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "    print(input_ids)\n",
    "    print(input_ids.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-1\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-2\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] yu ##ca ##ip ##a owned dominic ##k ' s before selling the chain to safe ##way in 1998 for $ 2 . 5 billion . [SEP] yu ##ca ##ip ##a bought dominic ##k ' s in 1995 for $ 69 ##3 million and sold it to safe ##way for $ 1 . 8 billion in 1998 . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-3\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] they had published an advertisement on the internet on june 10 , offering the cargo for sale , he added . [SEP] on june 10 , the ship ' s owners had published an advertisement on the internet , offering the explosives for sale . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-4\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] around 03 ##35 gm ##t , tab shares were up 19 cents , or 4 . 4 % , at a $ 4 . 56 , having earlier set a record high of a $ 4 . 57 . [SEP] tab shares jumped 20 cents , or 4 . 6 % , to set a record closing high at a $ 4 . 57 . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   guid: train-5\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   tokens: [CLS] the stock rose $ 2 . 11 , or about 11 percent , to close friday at $ 21 . 51 on the new york stock exchange . [SEP] pg & e corp . shares jumped $ 1 . 63 or 8 percent to $ 21 . 03 on the new york stock exchange on friday . [SEP]\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 13:01:18 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 13:01:21 - INFO - __main__ -   ***** Running training *****\n",
      "04/02/2019 13:01:21 - INFO - __main__ -     Num examples = 3668\n",
      "04/02/2019 13:01:21 - INFO - __main__ -     Batch size = 32\n",
      "04/02/2019 13:01:21 - INFO - __main__ -     Num steps = 343\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/115 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 1/115 [00:37<1:10:23, 37.05s/it]\u001b[A\n",
      "Iteration:   2%|â         | 2/115 [01:10<1:07:50, 36.02s/it]\u001b[A\n",
      "Iteration:   3%|â         | 3/115 [01:46<1:07:12, 36.00s/it]\u001b[A\n",
      "Iteration:   3%|â         | 4/115 [02:23<1:07:01, 36.23s/it]\u001b[A\n",
      "Iteration:   4%|â         | 5/115 [03:00<1:07:06, 36.61s/it]\u001b[A\n",
      "Iteration:   5%|â         | 6/115 [03:30<1:02:34, 34.44s/it]\u001b[A\n",
      "Iteration:   6%|â         | 7/115 [03:59<59:18, 32.95s/it]  \u001b[A\n",
      "Iteration:   7%|â         | 8/115 [04:28<56:37, 31.75s/it]\u001b[A\n",
      "Iteration:   8%|â         | 9/115 [04:57<54:39, 30.94s/it]\u001b[A\n",
      "Iteration:   9%|â         | 10/115 [05:27<53:21, 30.49s/it]\u001b[A\n",
      "Iteration:  10%|â         | 11/115 [05:56<52:12, 30.12s/it]\u001b[A\n",
      "Iteration:  10%|â         | 12/115 [06:26<51:30, 30.01s/it]\u001b[A\n",
      "Iteration:  11%|ââ        | 13/115 [06:55<50:53, 29.94s/it]\u001b[A\n",
      "Iteration:  12%|ââ        | 14/115 [07:25<50:02, 29.73s/it]\u001b[A\n",
      "Iteration:  13%|ââ        | 15/115 [07:54<49:31, 29.72s/it]\u001b[A\n",
      "Iteration:  14%|ââ        | 16/115 [08:24<49:04, 29.74s/it]\u001b[A\n",
      "Iteration:  15%|ââ        | 17/115 [08:54<48:31, 29.71s/it]\u001b[A\n",
      "Iteration:  16%|ââ        | 18/115 [09:24<48:02, 29.72s/it]\u001b[A\n",
      "Iteration:  17%|ââ        | 19/115 [09:53<47:26, 29.66s/it]\u001b[A\n",
      "Iteration:  17%|ââ        | 20/115 [10:23<47:11, 29.81s/it]\u001b[A\n",
      "Iteration:  18%|ââ        | 21/115 [10:53<46:52, 29.92s/it]\u001b[A\n",
      "Iteration:  19%|ââ        | 22/115 [11:25<47:04, 30.37s/it]\u001b[A\n",
      "Iteration:  20%|ââ        | 23/115 [11:56<46:50, 30.55s/it]\u001b[A\n",
      "Iteration:  21%|ââ        | 24/115 [12:27<46:31, 30.68s/it]\u001b[A\n",
      "Iteration:  22%|âââ       | 25/115 [12:57<45:51, 30.57s/it]\u001b[A\n",
      "Iteration:  23%|âââ       | 26/115 [13:27<45:07, 30.42s/it]\u001b[A\n",
      "Iteration:  23%|âââ       | 27/115 [13:57<44:30, 30.34s/it]\u001b[A\n",
      "Iteration:  24%|âââ       | 28/115 [14:27<43:54, 30.28s/it]\u001b[A\n",
      "Iteration:  25%|âââ       | 29/115 [14:58<43:18, 30.21s/it]\u001b[A\n",
      "Iteration:  26%|âââ       | 30/115 [15:28<42:50, 30.25s/it]\u001b[A\n",
      "Iteration:  27%|âââ       | 31/115 [15:58<42:18, 30.22s/it]\u001b[A\n",
      "Iteration:  28%|âââ       | 32/115 [16:28<41:54, 30.30s/it]\u001b[A\n",
      "Iteration:  29%|âââ       | 33/115 [16:59<41:18, 30.23s/it]\u001b[A\n",
      "Iteration:  30%|âââ       | 34/115 [17:29<40:58, 30.35s/it]\u001b[A\n",
      "Iteration:  30%|âââ       | 35/115 [17:59<40:22, 30.28s/it]\u001b[A\n",
      "Iteration:  31%|ââââ      | 36/115 [18:30<39:58, 30.37s/it]\u001b[A\n",
      "Iteration:  32%|ââââ      | 37/115 [19:00<39:24, 30.31s/it]\u001b[A\n",
      "Iteration:  33%|ââââ      | 38/115 [19:30<38:53, 30.31s/it]\u001b[A\n",
      "Iteration:  34%|ââââ      | 39/115 [20:01<38:33, 30.44s/it]\u001b[A\n",
      "Iteration:  35%|ââââ      | 40/115 [20:31<37:40, 30.15s/it]\u001b[A\n",
      "Iteration:  36%|ââââ      | 41/115 [21:00<36:52, 29.90s/it]\u001b[A\n",
      "Iteration:  37%|ââââ      | 42/115 [21:29<36:12, 29.77s/it]\u001b[A\n",
      "Iteration:  37%|ââââ      | 43/115 [21:59<35:34, 29.65s/it]\u001b[A\n",
      "Iteration:  38%|ââââ      | 44/115 [22:29<35:16, 29.80s/it]\u001b[A\n",
      "Iteration:  39%|ââââ      | 45/115 [23:00<35:10, 30.15s/it]\u001b[A\n",
      "Iteration:  40%|ââââ      | 46/115 [23:30<34:34, 30.07s/it]\u001b[A\n",
      "Iteration:  41%|ââââ      | 47/115 [23:59<33:50, 29.86s/it]\u001b[A\n",
      "Iteration:  42%|âââââ     | 48/115 [24:29<33:14, 29.76s/it]\u001b[A\n",
      "Iteration:  43%|âââââ     | 49/115 [24:58<32:29, 29.54s/it]\u001b[A\n",
      "Iteration:  43%|âââââ     | 50/115 [25:27<31:57, 29.50s/it]\u001b[A\n",
      "Iteration:  44%|âââââ     | 51/115 [25:56<31:22, 29.42s/it]\u001b[A\n",
      "Iteration:  45%|âââââ     | 52/115 [26:26<30:53, 29.42s/it]\u001b[A\n",
      "Iteration:  46%|âââââ     | 53/115 [26:56<30:35, 29.61s/it]\u001b[A\n",
      "Iteration:  47%|âââââ     | 54/115 [27:26<30:18, 29.82s/it]\u001b[A\n",
      "Iteration:  48%|âââââ     | 55/115 [27:56<29:54, 29.90s/it]\u001b[A\n",
      "Iteration:  49%|âââââ     | 56/115 [28:27<29:41, 30.19s/it]\u001b[A\n",
      "Iteration:  50%|âââââ     | 57/115 [28:57<29:03, 30.06s/it]\u001b[A\n",
      "Iteration:  50%|âââââ     | 58/115 [29:27<28:36, 30.11s/it]\u001b[A\n",
      "Iteration:  51%|ââââââ    | 59/115 [29:57<28:08, 30.14s/it]\u001b[A\n",
      "Iteration:  52%|ââââââ    | 60/115 [30:27<27:28, 29.98s/it]\u001b[A\n",
      "Iteration:  53%|ââââââ    | 61/115 [30:56<26:47, 29.77s/it]\u001b[A\n",
      "Iteration:  54%|ââââââ    | 62/115 [31:26<26:13, 29.69s/it]\u001b[A\n",
      "Iteration:  55%|ââââââ    | 63/115 [31:55<25:37, 29.57s/it]\u001b[A\n",
      "Iteration:  56%|ââââââ    | 64/115 [32:24<25:06, 29.54s/it]\u001b[A\n",
      "Iteration:  57%|ââââââ    | 65/115 [32:54<24:39, 29.60s/it]\u001b[A\n",
      "Iteration:  57%|ââââââ    | 66/115 [33:24<24:17, 29.74s/it]\u001b[A\n",
      "Iteration:  58%|ââââââ    | 67/115 [33:54<23:53, 29.87s/it]\u001b[A\n",
      "Iteration:  59%|ââââââ    | 68/115 [34:24<23:28, 29.96s/it]\u001b[A\n",
      "Iteration:  60%|ââââââ    | 69/115 [34:55<23:02, 30.06s/it]\u001b[A\n",
      "Iteration:  61%|ââââââ    | 70/115 [35:24<22:22, 29.84s/it]\u001b[A\n",
      "Iteration:  62%|âââââââ   | 71/115 [35:53<21:44, 29.66s/it]\u001b[A\n",
      "Iteration:  63%|âââââââ   | 72/115 [36:23<21:14, 29.63s/it]\u001b[A\n",
      "Iteration:  63%|âââââââ   | 73/115 [36:52<20:41, 29.55s/it]\u001b[A\n",
      "Iteration:  64%|âââââââ   | 74/115 [37:22<20:08, 29.48s/it]\u001b[A\n",
      "Iteration:  65%|âââââââ   | 75/115 [37:51<19:36, 29.41s/it]\u001b[A\n",
      "Iteration:  66%|âââââââ   | 76/115 [38:21<19:15, 29.64s/it]\u001b[A\n",
      "Iteration:  67%|âââââââ   | 77/115 [38:51<18:46, 29.64s/it]\u001b[A\n",
      "Iteration:  68%|âââââââ   | 78/115 [39:21<18:21, 29.76s/it]\u001b[A\n",
      "Iteration:  69%|âââââââ   | 79/115 [39:51<17:55, 29.89s/it]\u001b[A\n",
      "Iteration:  70%|âââââââ   | 80/115 [40:20<17:21, 29.77s/it]\u001b[A\n",
      "Iteration:  70%|âââââââ   | 81/115 [40:50<16:47, 29.64s/it]\u001b[A\n",
      "Iteration:  71%|ââââââââ  | 82/115 [41:19<16:15, 29.57s/it]\u001b[A\n",
      "Iteration:  72%|ââââââââ  | 83/115 [41:48<15:42, 29.45s/it]\u001b[A\n",
      "Iteration:  73%|ââââââââ  | 84/115 [42:18<15:17, 29.61s/it]\u001b[A\n",
      "Iteration:  74%|ââââââââ  | 85/115 [42:48<14:50, 29.68s/it]\u001b[A\n",
      "Iteration:  75%|ââââââââ  | 86/115 [43:18<14:22, 29.75s/it]\u001b[A\n",
      "Iteration:  76%|ââââââââ  | 87/115 [43:48<13:54, 29.79s/it]\u001b[A\n",
      "Iteration:  77%|ââââââââ  | 88/115 [44:18<13:29, 29.98s/it]\u001b[A\n",
      "Iteration:  77%|ââââââââ  | 89/115 [44:48<13:00, 30.04s/it]\u001b[A\n",
      "Iteration:  78%|ââââââââ  | 90/115 [45:19<12:36, 30.25s/it]\u001b[A\n",
      "Iteration:  79%|ââââââââ  | 91/115 [45:49<12:04, 30.18s/it]\u001b[A\n",
      "Iteration:  80%|ââââââââ  | 92/115 [46:20<11:36, 30.27s/it]\u001b[A\n",
      "Iteration:  81%|ââââââââ  | 93/115 [46:51<11:13, 30.60s/it]\u001b[A\n",
      "Iteration:  82%|âââââââââ | 94/115 [47:22<10:42, 30.58s/it]\u001b[A\n",
      "Iteration:  83%|âââââââââ | 95/115 [47:51<10:03, 30.18s/it]\u001b[A\n",
      "Iteration:  83%|âââââââââ | 96/115 [48:26<10:00, 31.63s/it]\u001b[A\n",
      "Iteration:  84%|âââââââââ | 97/115 [48:57<09:25, 31.41s/it]\u001b[A\n",
      "Iteration:  85%|âââââââââ | 98/115 [49:28<08:50, 31.22s/it]\u001b[A\n",
      "Iteration:  86%|âââââââââ | 99/115 [49:57<08:12, 30.75s/it]\u001b[A\n",
      "Iteration:  87%|âââââââââ | 100/115 [50:27<07:38, 30.55s/it]\u001b[A\n",
      "Iteration:  88%|âââââââââ | 101/115 [50:57<07:05, 30.38s/it]\u001b[A\n",
      "Iteration:  89%|âââââââââ | 102/115 [51:27<06:34, 30.31s/it]\u001b[A\n",
      "Iteration:  90%|âââââââââ | 103/115 [51:58<06:04, 30.35s/it]\u001b[A\n",
      "Iteration:  90%|âââââââââ | 104/115 [52:29<05:35, 30.49s/it]\u001b[A\n",
      "Iteration:  91%|ââââââââââ| 105/115 [52:59<05:03, 30.31s/it]\u001b[A\n",
      "Iteration:  92%|ââââââââââ| 106/115 [53:28<04:30, 30.04s/it]\u001b[A\n",
      "Iteration:  93%|ââââââââââ| 107/115 [53:57<03:58, 29.82s/it]\u001b[A\n",
      "Iteration:  94%|ââââââââââ| 108/115 [54:27<03:28, 29.76s/it]\u001b[A\n",
      "Iteration:  95%|ââââââââââ| 109/115 [54:56<02:57, 29.62s/it]\u001b[A\n",
      "Iteration:  96%|ââââââââââ| 110/115 [55:26<02:28, 29.61s/it]\u001b[A\n",
      "Iteration:  97%|ââââââââââ| 111/115 [55:55<01:57, 29.48s/it]\u001b[A\n",
      "Iteration:  97%|ââââââââââ| 112/115 [56:25<01:28, 29.54s/it]\u001b[A\n",
      "Iteration:  98%|ââââââââââ| 113/115 [56:54<00:59, 29.55s/it]\u001b[A\n",
      "Iteration:  99%|ââââââââââ| 114/115 [57:24<00:29, 29.54s/it]\u001b[A\n",
      "Iteration: 100%|ââââââââââ| 115/115 [57:42<00:00, 26.30s/it]\u001b[A\n",
      "Epoch:  33%|ââââ      | 1/3 [57:42<1:55:25, 3462.99s/it]\n",
      "Iteration:   0%|          | 0/115 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/115 [00:29<56:19, 29.65s/it]\u001b[A\n",
      "Iteration:   2%|â         | 2/115 [00:59<56:02, 29.76s/it]\u001b[A\n",
      "Iteration:   3%|â         | 3/115 [01:29<55:30, 29.74s/it]\u001b[A\n",
      "Iteration:   3%|â         | 4/115 [01:58<54:41, 29.57s/it]\u001b[A\n",
      "Iteration:   4%|â         | 5/115 [02:27<54:01, 29.47s/it]\u001b[A\n",
      "Iteration:   5%|â         | 6/115 [02:57<53:24, 29.40s/it]\u001b[A\n",
      "Iteration:   6%|â         | 7/115 [03:26<52:50, 29.36s/it]\u001b[A\n",
      "Iteration:   7%|â         | 8/115 [03:55<52:20, 29.35s/it]\u001b[A\n",
      "Iteration:   8%|â         | 9/115 [04:24<51:49, 29.34s/it]\u001b[A\n",
      "Iteration:   9%|â         | 10/115 [04:54<51:17, 29.31s/it]\u001b[A\n",
      "Iteration:  10%|â         | 11/115 [05:23<50:45, 29.28s/it]\u001b[A\n",
      "Iteration:  10%|â         | 12/115 [05:52<50:21, 29.33s/it]\u001b[A\n",
      "Iteration:  11%|ââ        | 13/115 [06:23<50:24, 29.65s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  12%|ââ        | 14/115 [06:54<50:29, 30.00s/it]\u001b[A\n",
      "Iteration:  13%|ââ        | 15/115 [07:23<49:49, 29.89s/it]\u001b[A\n",
      "Iteration:  14%|ââ        | 16/115 [07:53<49:12, 29.82s/it]\u001b[A\n",
      "Iteration:  15%|ââ        | 17/115 [08:23<48:40, 29.80s/it]\u001b[A\n",
      "Iteration:  16%|ââ        | 18/115 [08:53<48:20, 29.90s/it]\u001b[A\n",
      "Iteration:  17%|ââ        | 19/115 [09:22<47:32, 29.71s/it]\u001b[A\n",
      "Iteration:  17%|ââ        | 20/115 [09:52<46:59, 29.68s/it]\u001b[A\n",
      "Iteration:  18%|ââ        | 21/115 [10:21<46:35, 29.74s/it]\u001b[A\n",
      "Iteration:  19%|ââ        | 22/115 [10:51<46:11, 29.80s/it]\u001b[A\n",
      "Iteration:  20%|ââ        | 23/115 [11:21<45:48, 29.87s/it]\u001b[A\n",
      "Iteration:  21%|ââ        | 24/115 [11:52<45:26, 29.97s/it]\u001b[A\n",
      "Iteration:  22%|âââ       | 25/115 [12:22<44:59, 29.99s/it]\u001b[A\n",
      "Iteration:  23%|âââ       | 26/115 [12:52<44:38, 30.10s/it]\u001b[A\n",
      "Iteration:  23%|âââ       | 27/115 [13:22<43:54, 29.94s/it]\u001b[A\n",
      "Iteration:  24%|âââ       | 28/115 [13:51<43:08, 29.75s/it]\u001b[A\n",
      "Iteration:  25%|âââ       | 29/115 [14:20<42:31, 29.67s/it]\u001b[A\n",
      "Iteration:  26%|âââ       | 30/115 [14:50<41:49, 29.52s/it]\u001b[A\n",
      "Iteration:  27%|âââ       | 31/115 [15:19<41:28, 29.63s/it]\u001b[A\n",
      "Iteration:  28%|âââ       | 32/115 [15:49<40:59, 29.64s/it]\u001b[A\n",
      "Iteration:  29%|âââ       | 33/115 [16:19<40:30, 29.65s/it]\u001b[A\n",
      "Iteration:  30%|âââ       | 34/115 [16:49<40:05, 29.70s/it]\u001b[A\n",
      "Iteration:  30%|âââ       | 35/115 [17:18<39:37, 29.72s/it]\u001b[A\n",
      "Iteration:  31%|ââââ      | 36/115 [17:48<39:13, 29.79s/it]\u001b[A\n",
      "Iteration:  32%|ââââ      | 37/115 [18:18<38:42, 29.78s/it]\u001b[A\n",
      "Iteration:  33%|ââââ      | 38/115 [18:48<38:20, 29.87s/it]\u001b[A\n",
      "Iteration:  34%|ââââ      | 39/115 [19:18<37:51, 29.89s/it]\u001b[A\n",
      "Iteration:  35%|ââââ      | 40/115 [19:48<37:27, 29.97s/it]\u001b[A\n",
      "Iteration:  36%|ââââ      | 41/115 [20:18<37:02, 30.04s/it]\u001b[A\n",
      "Iteration:  37%|ââââ      | 42/115 [20:56<39:17, 32.30s/it]\u001b[A\n",
      "Iteration:  37%|ââââ      | 43/115 [21:28<38:47, 32.33s/it]\u001b[A\n",
      "Iteration:  38%|ââââ      | 44/115 [22:04<39:23, 33.29s/it]\u001b[A\n",
      "Iteration:  39%|ââââ      | 45/115 [22:37<38:41, 33.16s/it]\u001b[A\n",
      "Iteration:  40%|ââââ      | 46/115 [23:10<38:06, 33.14s/it]\u001b[A\n",
      "Iteration:  41%|ââââ      | 47/115 [23:49<39:36, 34.95s/it]\u001b[A\n",
      "Iteration:  42%|âââââ     | 48/115 [24:20<37:31, 33.60s/it]\u001b[A\n",
      "Iteration:  43%|âââââ     | 49/115 [24:51<36:23, 33.08s/it]\u001b[A\n",
      "Iteration:  43%|âââââ     | 50/115 [25:22<35:09, 32.45s/it]\u001b[A\n",
      "Iteration:  44%|âââââ     | 51/115 [25:59<36:02, 33.78s/it]\u001b[A\n",
      "Iteration:  45%|âââââ     | 52/115 [26:33<35:27, 33.77s/it]\u001b[A\n",
      "Iteration:  46%|âââââ     | 53/115 [27:03<33:37, 32.55s/it]\u001b[A\n",
      "Iteration:  47%|âââââ     | 54/115 [27:32<31:59, 31.47s/it]\u001b[A\n",
      "Iteration:  48%|âââââ     | 55/115 [28:01<30:47, 30.80s/it]\u001b[A\n",
      "Iteration:  49%|âââââ     | 56/115 [28:30<29:55, 30.42s/it]\u001b[A\n",
      "Iteration:  50%|âââââ     | 57/115 [29:03<30:01, 31.06s/it]\u001b[A\n",
      "Iteration:  50%|âââââ     | 58/115 [29:32<28:51, 30.38s/it]\u001b[A\n",
      "Iteration:  51%|ââââââ    | 59/115 [30:00<27:53, 29.89s/it]\u001b[A\n",
      "Iteration:  52%|ââââââ    | 60/115 [30:32<27:48, 30.34s/it]\u001b[A\n",
      "Iteration:  53%|ââââââ    | 61/115 [31:03<27:26, 30.50s/it]\u001b[A\n",
      "Iteration:  54%|ââââââ    | 62/115 [31:34<27:03, 30.62s/it]\u001b[A\n",
      "Iteration:  55%|ââââââ    | 63/115 [32:12<28:27, 32.83s/it]\u001b[A\n",
      "Iteration:  56%|ââââââ    | 64/115 [32:42<27:23, 32.22s/it]\u001b[A\n",
      "Iteration:  57%|ââââââ    | 65/115 [33:17<27:19, 32.79s/it]\u001b[A\n",
      "Iteration:  57%|ââââââ    | 66/115 [33:49<26:45, 32.76s/it]\u001b[A\n",
      "Iteration:  58%|ââââââ    | 67/115 [34:30<28:13, 35.28s/it]\u001b[A\n",
      "Iteration:  59%|ââââââ    | 68/115 [35:01<26:35, 33.95s/it]\u001b[A\n",
      "Iteration:  60%|ââââââ    | 69/115 [35:34<25:50, 33.71s/it]\u001b[A\n",
      "Iteration:  61%|ââââââ    | 70/115 [36:08<25:17, 33.73s/it]\u001b[A\n",
      "Iteration:  62%|âââââââ   | 71/115 [36:40<24:16, 33.10s/it]\u001b[A\n",
      "Iteration:  63%|âââââââ   | 72/115 [37:10<23:02, 32.15s/it]\u001b[A\n",
      "Iteration:  63%|âââââââ   | 73/115 [37:39<21:56, 31.33s/it]\u001b[A\n",
      "Iteration:  64%|âââââââ   | 74/115 [38:08<20:52, 30.55s/it]\u001b[A\n",
      "Iteration:  65%|âââââââ   | 75/115 [38:37<20:02, 30.06s/it]\u001b[A\n",
      "Iteration:  66%|âââââââ   | 76/115 [39:10<20:12, 31.09s/it]\u001b[A\n",
      "Iteration:  67%|âââââââ   | 77/115 [39:41<19:40, 31.07s/it]\u001b[A\n",
      "Iteration:  68%|âââââââ   | 78/115 [40:15<19:33, 31.71s/it]\u001b[A\n",
      "Iteration:  69%|âââââââ   | 79/115 [40:52<20:02, 33.40s/it]\u001b[A\n",
      "Iteration:  70%|âââââââ   | 80/115 [41:27<19:50, 34.02s/it]\u001b[A\n",
      "Iteration:  70%|âââââââ   | 81/115 [41:58<18:39, 32.92s/it]\u001b[A\n",
      "Iteration:  71%|ââââââââ  | 82/115 [42:28<17:36, 32.01s/it]\u001b[A\n",
      "Iteration:  72%|ââââââââ  | 83/115 [42:59<17:01, 31.92s/it]\u001b[A\n",
      "Iteration:  73%|ââââââââ  | 84/115 [43:32<16:32, 32.03s/it]\u001b[A\n",
      "Iteration:  74%|ââââââââ  | 85/115 [44:05<16:15, 32.53s/it]\u001b[A\n",
      "Iteration:  75%|ââââââââ  | 86/115 [44:39<15:56, 32.99s/it]\u001b[A\n",
      "Iteration:  76%|ââââââââ  | 87/115 [45:09<14:51, 31.85s/it]\u001b[A\n",
      "Iteration:  77%|ââââââââ  | 88/115 [45:39<14:04, 31.29s/it]\u001b[A\n",
      "Iteration:  77%|ââââââââ  | 89/115 [46:10<13:37, 31.43s/it]\u001b[A\n",
      "Iteration:  78%|ââââââââ  | 90/115 [46:42<13:09, 31.58s/it]\u001b[A\n",
      "Iteration:  79%|ââââââââ  | 91/115 [47:14<12:37, 31.55s/it]\u001b[A\n",
      "Iteration:  80%|ââââââââ  | 92/115 [47:46<12:11, 31.79s/it]\u001b[A\n",
      "Iteration:  81%|ââââââââ  | 93/115 [48:20<11:53, 32.45s/it]\u001b[A\n",
      "Iteration:  82%|âââââââââ | 94/115 [48:51<11:14, 32.11s/it]\u001b[A\n",
      "Iteration:  83%|âââââââââ | 95/115 [49:31<11:26, 34.34s/it]\u001b[A\n",
      "Iteration:  83%|âââââââââ | 96/115 [50:06<10:58, 34.66s/it]\u001b[A\n",
      "Iteration:  84%|âââââââââ | 97/115 [50:39<10:13, 34.11s/it]\u001b[A\n",
      "Iteration:  85%|âââââââââ | 98/115 [51:11<09:30, 33.55s/it]\u001b[A\n",
      "Iteration:  86%|âââââââââ | 99/115 [51:46<09:00, 33.81s/it]\u001b[A\n",
      "Iteration:  87%|âââââââââ | 100/115 [52:16<08:10, 32.71s/it]\u001b[A\n",
      "Iteration:  88%|âââââââââ | 101/115 [52:49<07:39, 32.83s/it]\u001b[A\n",
      "Iteration:  89%|âââââââââ | 102/115 [53:20<07:00, 32.34s/it]\u001b[A\n",
      "Iteration:  90%|âââââââââ | 103/115 [53:50<06:19, 31.62s/it]\u001b[A\n",
      "Iteration:  90%|âââââââââ | 104/115 [54:23<05:52, 32.09s/it]\u001b[A\n",
      "Iteration:  91%|ââââââââââ| 105/115 [54:56<05:22, 32.20s/it]\u001b[A\n",
      "Iteration:  92%|ââââââââââ| 106/115 [55:27<04:46, 31.79s/it]\u001b[A\n",
      "Iteration:  93%|ââââââââââ| 107/115 [55:57<04:09, 31.22s/it]\u001b[A\n",
      "Iteration:  94%|ââââââââââ| 108/115 [56:26<03:35, 30.82s/it]\u001b[A\n",
      "Iteration:  95%|ââââââââââ| 109/115 [56:59<03:07, 31.31s/it]\u001b[A\n",
      "Iteration:  96%|ââââââââââ| 110/115 [57:33<02:41, 32.21s/it]\u001b[A\n",
      "Iteration:  97%|ââââââââââ| 111/115 [58:08<02:11, 32.99s/it]\u001b[A\n",
      "Iteration:  97%|ââââââââââ| 112/115 [58:43<01:40, 33.63s/it]\u001b[A\n",
      "Iteration:  98%|ââââââââââ| 113/115 [59:19<01:08, 34.38s/it]\u001b[A\n",
      "Iteration:  99%|ââââââââââ| 114/115 [59:51<00:33, 33.56s/it]\u001b[A\n",
      "Iteration: 100%|ââââââââââ| 115/115 [1:00:11<00:00, 29.62s/it]\u001b[A\n",
      "Epoch:  67%|âââââââ   | 2/3 [1:57:54<58:27, 3507.65s/it]\n",
      "Iteration:   0%|          | 0/115 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/115 [00:31<1:00:23, 31.79s/it]\u001b[A\n",
      "Iteration:   2%|â         | 2/115 [01:02<59:27, 31.57s/it]  \u001b[A\n",
      "Iteration:   3%|â         | 3/115 [01:35<59:26, 31.84s/it]\u001b[A\n",
      "Iteration:   3%|â         | 4/115 [02:07<58:52, 31.82s/it]\u001b[A\n",
      "Iteration:   4%|â         | 5/115 [02:39<58:29, 31.91s/it]\u001b[A\n",
      "Iteration:   5%|â         | 6/115 [03:08<56:44, 31.23s/it]\u001b[A\n",
      "Iteration:   6%|â         | 7/115 [03:38<55:10, 30.65s/it]\u001b[A\n",
      "Iteration:   7%|â         | 8/115 [04:09<54:58, 30.83s/it]\u001b[A\n",
      "Iteration:   8%|â         | 9/115 [04:38<53:48, 30.46s/it]\u001b[A\n",
      "Iteration:   9%|â         | 10/115 [05:08<53:02, 30.31s/it]\u001b[A\n",
      "Iteration:  10%|â         | 11/115 [05:40<53:04, 30.62s/it]\u001b[A\n",
      "Iteration:  10%|â         | 12/115 [06:10<52:19, 30.48s/it]\u001b[A\n",
      "Iteration:  11%|ââ        | 13/115 [06:42<52:28, 30.86s/it]\u001b[A\n",
      "Iteration:  12%|ââ        | 14/115 [07:17<54:21, 32.30s/it]\u001b[A\n",
      "Iteration:  13%|ââ        | 15/115 [07:50<54:03, 32.44s/it]\u001b[A\n",
      "Iteration:  14%|ââ        | 16/115 [08:22<53:09, 32.22s/it]\u001b[A\n",
      "Iteration:  15%|ââ        | 17/115 [08:57<54:06, 33.12s/it]\u001b[A\n",
      "Iteration:  16%|ââ        | 18/115 [09:33<55:05, 34.07s/it]\u001b[A\n",
      "Iteration:  17%|ââ        | 19/115 [10:06<53:39, 33.54s/it]\u001b[A\n",
      "Iteration:  17%|ââ        | 20/115 [10:38<52:30, 33.17s/it]\u001b[A\n",
      "Iteration:  18%|ââ        | 21/115 [11:15<53:35, 34.21s/it]\u001b[A\n",
      "Iteration:  19%|ââ        | 22/115 [11:47<52:01, 33.57s/it]\u001b[A\n",
      "Iteration:  20%|ââ        | 23/115 [12:19<51:07, 33.34s/it]\u001b[A\n",
      "Iteration:  21%|ââ        | 24/115 [12:51<49:31, 32.65s/it]\u001b[A\n",
      "Iteration:  22%|âââ       | 25/115 [13:23<48:43, 32.48s/it]\u001b[A\n",
      "Iteration:  23%|âââ       | 26/115 [13:53<47:20, 31.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  23%|âââ       | 27/115 [14:26<47:02, 32.08s/it]\u001b[A\n",
      "Iteration:  24%|âââ       | 28/115 [14:56<45:55, 31.67s/it]\u001b[A\n",
      "Iteration:  25%|âââ       | 29/115 [15:27<44:47, 31.25s/it]\u001b[A\n",
      "Iteration:  26%|âââ       | 30/115 [15:59<44:50, 31.66s/it]\u001b[A\n",
      "Iteration:  27%|âââ       | 31/115 [16:30<44:04, 31.49s/it]\u001b[A\n",
      "Iteration:  28%|âââ       | 32/115 [17:01<43:11, 31.23s/it]\u001b[A\n",
      "Iteration:  29%|âââ       | 33/115 [17:35<43:42, 31.98s/it]\u001b[A\n",
      "Iteration:  30%|âââ       | 34/115 [18:11<44:52, 33.24s/it]\u001b[A\n",
      "Iteration:  30%|âââ       | 35/115 [18:44<44:24, 33.31s/it]\u001b[A\n",
      "Iteration:  31%|ââââ      | 36/115 [19:15<42:51, 32.56s/it]\u001b[A\n",
      "Iteration:  32%|ââââ      | 37/115 [19:46<41:32, 31.95s/it]\u001b[A\n",
      "Iteration:  33%|ââââ      | 38/115 [20:17<40:51, 31.84s/it]\u001b[A\n",
      "Iteration:  34%|ââââ      | 39/115 [20:50<40:42, 32.13s/it]\u001b[A\n",
      "Iteration:  35%|ââââ      | 40/115 [21:23<40:34, 32.46s/it]\u001b[A\n",
      "Iteration:  36%|ââââ      | 41/115 [21:57<40:18, 32.69s/it]\u001b[A\n",
      "Iteration:  37%|ââââ      | 42/115 [22:30<40:07, 32.97s/it]\u001b[A\n",
      "Iteration:  37%|ââââ      | 43/115 [23:10<42:05, 35.08s/it]\u001b[A\n",
      "Iteration:  38%|ââââ      | 44/115 [23:49<43:00, 36.34s/it]\u001b[A\n",
      "Iteration:  39%|ââââ      | 45/115 [24:21<40:42, 34.89s/it]\u001b[A\n",
      "Iteration:  40%|ââââ      | 46/115 [25:00<41:32, 36.13s/it]\u001b[A\n",
      "Iteration:  41%|ââââ      | 47/115 [25:32<39:37, 34.97s/it]\u001b[A\n",
      "Iteration:  42%|âââââ     | 48/115 [26:04<37:56, 33.97s/it]\u001b[A\n",
      "Iteration:  43%|âââââ     | 49/115 [26:34<36:08, 32.85s/it]\u001b[A\n",
      "Iteration:  43%|âââââ     | 50/115 [27:07<35:30, 32.77s/it]\u001b[A\n",
      "Iteration:  44%|âââââ     | 51/115 [27:48<37:41, 35.34s/it]\u001b[A\n",
      "Iteration:  45%|âââââ     | 52/115 [28:20<36:03, 34.33s/it]\u001b[A\n",
      "Iteration:  46%|âââââ     | 53/115 [28:52<34:37, 33.51s/it]\u001b[A\n",
      "Iteration:  47%|âââââ     | 54/115 [29:23<33:26, 32.89s/it]\u001b[A\n",
      "Iteration:  48%|âââââ     | 55/115 [29:55<32:35, 32.59s/it]\u001b[A\n",
      "Iteration:  49%|âââââ     | 56/115 [30:28<32:11, 32.73s/it]\u001b[A\n",
      "Iteration:  50%|âââââ     | 57/115 [30:59<31:00, 32.08s/it]\u001b[A\n",
      "Iteration:  50%|âââââ     | 58/115 [31:31<30:30, 32.11s/it]\u001b[A\n",
      "Iteration:  51%|ââââââ    | 59/115 [32:04<30:23, 32.56s/it]\u001b[A\n",
      "Iteration:  52%|ââââââ    | 60/115 [32:36<29:42, 32.41s/it]\u001b[A\n",
      "Iteration:  53%|ââââââ    | 61/115 [33:13<30:19, 33.70s/it]\u001b[A\n",
      "Iteration:  54%|ââââââ    | 62/115 [33:43<28:46, 32.58s/it]\u001b[A\n",
      "Iteration:  55%|ââââââ    | 63/115 [34:12<27:20, 31.55s/it]\u001b[A\n",
      "Iteration:  56%|ââââââ    | 64/115 [34:44<26:47, 31.53s/it]\u001b[A\n",
      "Iteration:  57%|ââââââ    | 65/115 [35:20<27:31, 33.03s/it]\u001b[A\n",
      "Iteration:  57%|ââââââ    | 66/115 [35:56<27:32, 33.73s/it]\u001b[A\n",
      "Iteration:  58%|ââââââ    | 67/115 [36:33<27:52, 34.84s/it]\u001b[A\n",
      "Iteration:  59%|ââââââ    | 68/115 [37:02<25:52, 33.02s/it]\u001b[A\n",
      "Iteration:  60%|ââââââ    | 69/115 [37:31<24:21, 31.77s/it]\u001b[A\n",
      "Iteration:  61%|ââââââ    | 70/115 [37:59<23:04, 30.76s/it]\u001b[A\n",
      "Iteration:  62%|âââââââ   | 71/115 [38:28<22:06, 30.14s/it]\u001b[A\n",
      "Iteration:  63%|âââââââ   | 72/115 [38:56<21:16, 29.68s/it]\u001b[A\n",
      "Iteration:  63%|âââââââ   | 73/115 [39:25<20:34, 29.38s/it]\u001b[A\n",
      "Iteration:  64%|âââââââ   | 74/115 [39:54<19:55, 29.15s/it]\u001b[A\n",
      "Iteration:  65%|âââââââ   | 75/115 [40:23<19:26, 29.17s/it]\u001b[A\n",
      "Iteration:  66%|âââââââ   | 76/115 [40:59<20:23, 31.38s/it]\u001b[A\n",
      "Iteration:  67%|âââââââ   | 77/115 [41:32<20:04, 31.70s/it]\u001b[A\n",
      "Iteration:  68%|âââââââ   | 78/115 [42:02<19:10, 31.11s/it]\u001b[A\n",
      "Iteration:  69%|âââââââ   | 79/115 [42:31<18:19, 30.54s/it]\u001b[A\n",
      "Iteration:  70%|âââââââ   | 80/115 [43:00<17:33, 30.10s/it]\u001b[A\n",
      "Iteration:  70%|âââââââ   | 81/115 [43:32<17:22, 30.65s/it]\u001b[A\n",
      "Iteration:  71%|ââââââââ  | 82/115 [44:02<16:42, 30.38s/it]\u001b[A\n",
      "Iteration:  72%|ââââââââ  | 83/115 [44:32<16:10, 30.32s/it]\u001b[A\n",
      "Iteration:  73%|ââââââââ  | 84/115 [45:01<15:27, 29.93s/it]\u001b[A\n",
      "Iteration:  74%|ââââââââ  | 85/115 [45:30<14:52, 29.75s/it]\u001b[A\n",
      "Iteration:  75%|ââââââââ  | 86/115 [46:01<14:28, 29.96s/it]\u001b[A\n",
      "Iteration:  76%|ââââââââ  | 87/115 [46:30<13:55, 29.85s/it]\u001b[A\n",
      "Iteration:  77%|ââââââââ  | 88/115 [46:59<13:20, 29.63s/it]\u001b[A\n",
      "Iteration:  77%|ââââââââ  | 89/115 [47:28<12:45, 29.44s/it]\u001b[A\n",
      "Iteration:  78%|ââââââââ  | 90/115 [47:58<12:16, 29.45s/it]\u001b[A\n",
      "Iteration:  79%|ââââââââ  | 91/115 [48:28<11:49, 29.56s/it]\u001b[A\n",
      "Iteration:  80%|ââââââââ  | 92/115 [48:59<11:29, 29.99s/it]\u001b[A\n",
      "Iteration:  81%|ââââââââ  | 93/115 [49:30<11:07, 30.33s/it]\u001b[A\n",
      "Iteration:  82%|âââââââââ | 94/115 [50:00<10:39, 30.46s/it]\u001b[A\n",
      "Iteration:  83%|âââââââââ | 95/115 [50:31<10:07, 30.39s/it]\u001b[A\n",
      "Iteration:  83%|âââââââââ | 96/115 [51:00<09:30, 30.02s/it]\u001b[A\n",
      "Iteration:  84%|âââââââââ | 97/115 [51:29<08:55, 29.74s/it]\u001b[A\n",
      "Iteration:  85%|âââââââââ | 98/115 [51:58<08:21, 29.50s/it]\u001b[A\n",
      "Iteration:  86%|âââââââââ | 99/115 [52:28<07:53, 29.58s/it]\u001b[A\n",
      "Iteration:  87%|âââââââââ | 100/115 [52:57<07:23, 29.55s/it]\u001b[A\n",
      "Iteration:  88%|âââââââââ | 101/115 [53:26<06:52, 29.47s/it]\u001b[A\n",
      "Iteration:  89%|âââââââââ | 102/115 [53:55<06:21, 29.34s/it]\u001b[A\n",
      "Iteration:  90%|âââââââââ | 103/115 [54:25<05:51, 29.29s/it]\u001b[A\n",
      "Iteration:  90%|âââââââââ | 104/115 [54:55<05:26, 29.65s/it]\u001b[A\n",
      "Iteration:  91%|ââââââââââ| 105/115 [55:25<04:56, 29.69s/it]\u001b[A\n",
      "Iteration:  92%|ââââââââââ| 106/115 [55:54<04:26, 29.58s/it]\u001b[A\n",
      "Iteration:  93%|ââââââââââ| 107/115 [56:23<03:55, 29.46s/it]\u001b[A\n",
      "Iteration:  94%|ââââââââââ| 108/115 [56:52<03:25, 29.35s/it]\u001b[A\n",
      "Iteration:  95%|ââââââââââ| 109/115 [57:22<02:55, 29.29s/it]\u001b[A\n",
      "Iteration:  96%|ââââââââââ| 110/115 [57:51<02:26, 29.20s/it]\u001b[A\n",
      "Iteration:  97%|ââââââââââ| 111/115 [58:20<01:56, 29.24s/it]\u001b[A\n",
      "Iteration:  97%|ââââââââââ| 112/115 [58:49<01:27, 29.24s/it]\u001b[A\n",
      "Iteration:  98%|ââââââââââ| 113/115 [59:18<00:58, 29.24s/it]\u001b[A\n",
      "Iteration:  99%|ââââââââââ| 114/115 [59:48<00:29, 29.21s/it]\u001b[A\n",
      "Iteration: 100%|ââââââââââ| 115/115 [1:00:06<00:00, 26.03s/it]\u001b[A\n",
      "Epoch: 100%|ââââââââââ| 3/3 [2:58:01<00:00, 3537.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# training begins\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "if do_train:\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if local_rank == -1:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = DistributedSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            if fp16:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "if do_train:\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 16:18:42 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/varunn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/02/2019 16:18:42 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/varunn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_y/2dty3nzx05zdf0lpd_r9bj1jbr9qbr/T/tmppphemfdg\n",
      "04/02/2019 16:18:46 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/02/2019 16:18:49 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2019 16:18:49 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file, map_location=device)\n",
    "model1 = BertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
    "model1.load_state_dict(model_state_dict)\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "print(model1.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-1\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] \" the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-2\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] magna ##relli said ra ##cic ##ot hated the iraqi regime and looked forward to using his long years of training in the war . [SEP] his wife said he was \" 100 percent behind george bush \" and looked forward to using his years of training in the war . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-3\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] the dollar was at 116 . 92 yen against the yen , flat on the session , and at 1 . 289 ##1 against the swiss fran ##c , also flat . [SEP] the dollar was at 116 . 78 yen jp ##y = , virtually flat on the session , and at 1 . 287 ##1 against the swiss fran ##c ch ##f = , down 0 . 1 percent . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-4\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] the afl - ci ##o is waiting until october to decide if it will end ##ors ##e a candidate . [SEP] the afl - ci ##o announced wednesday that it will decide in october whether to end ##ors ##e a candidate before the primaries . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 1 (id = 1)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   *** Example ***\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   guid: dev-5\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   tokens: [CLS] no dates have been set for the civil or the criminal trial . [SEP] no dates have been set for the criminal or civil cases , but shan ##ley has pleaded not guilty . [SEP]\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   label: 0 (id = 0)\n",
      "04/02/2019 16:19:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2019 16:19:42 - INFO - __main__ -     Num examples = 408\n",
      "04/02/2019 16:19:42 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating:   2%|â         | 1/51 [00:04<04:03,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1818,  2.7929],\n",
      "        [ 1.3366, -1.0892],\n",
      "        [ 0.9410, -0.9200],\n",
      "        [-2.9937,  2.3675],\n",
      "        [ 1.3348, -1.0914],\n",
      "        [-2.2992,  1.5050],\n",
      "        [-1.2822,  0.4767],\n",
      "        [-3.1211,  2.5602]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   4%|â         | 2/51 [00:09<03:59,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5136,  1.7666],\n",
      "        [-2.7615,  2.0549],\n",
      "        [-3.1807,  2.7941],\n",
      "        [ 1.1995, -1.0859],\n",
      "        [ 1.2775, -1.0279],\n",
      "        [-3.1848,  2.6668],\n",
      "        [-3.1569,  2.7890],\n",
      "        [-2.3192,  1.5652]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   6%|â         | 3/51 [00:14<03:46,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2211,  2.7893],\n",
      "        [ 1.2416, -1.0223],\n",
      "        [-3.1625,  2.7746],\n",
      "        [ 1.3238, -1.0293],\n",
      "        [ 1.3748, -1.0547],\n",
      "        [-1.9296,  1.0845],\n",
      "        [ 1.3315, -1.0549],\n",
      "        [-3.2023,  2.7855]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   8%|â         | 4/51 [00:18<03:31,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1205,  1.3320],\n",
      "        [-1.7488,  0.9182],\n",
      "        [-1.0321,  0.2851],\n",
      "        [-3.1517,  2.7674],\n",
      "        [-2.0852,  1.2828],\n",
      "        [-3.0831,  2.4896],\n",
      "        [-0.1843, -0.2877],\n",
      "        [-3.1298,  2.7600]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  10%|â         | 5/51 [00:22<03:26,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0837,  1.2951],\n",
      "        [-1.9656,  1.1345],\n",
      "        [-3.2019,  2.7478],\n",
      "        [-2.3388,  1.5938],\n",
      "        [ 1.1030, -1.0202],\n",
      "        [ 1.3979, -1.0844],\n",
      "        [-3.1980,  2.7318],\n",
      "        [-3.1456,  2.7712]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  12%|ââ        | 6/51 [00:27<03:29,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4076, -0.1481],\n",
      "        [-2.8235,  2.1746],\n",
      "        [ 0.1338, -0.4460],\n",
      "        [ 1.3643, -1.0839],\n",
      "        [ 1.3143, -1.0564],\n",
      "        [-3.2090,  2.7757],\n",
      "        [-3.2254,  2.7678],\n",
      "        [ 1.3299, -1.0902]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  14%|ââ        | 7/51 [00:32<03:25,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2129,  2.7667],\n",
      "        [-3.1985,  2.7089],\n",
      "        [-2.3067,  1.5358],\n",
      "        [-1.5383,  0.7170],\n",
      "        [-3.0297,  2.3977],\n",
      "        [-3.1982,  2.7742],\n",
      "        [-2.9573,  2.3057],\n",
      "        [-3.2050,  2.7270]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  16%|ââ        | 8/51 [00:36<03:18,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1743, -1.0269],\n",
      "        [-3.2017,  2.7285],\n",
      "        [-3.1944,  2.7932],\n",
      "        [-3.0926,  2.4764],\n",
      "        [-1.0581,  0.2911],\n",
      "        [ 1.2766, -1.0675],\n",
      "        [-3.1875,  2.6808],\n",
      "        [-2.7346,  2.0415]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  18%|ââ        | 9/51 [00:41<03:09,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0669,  2.5105],\n",
      "        [ 1.3140, -1.0426],\n",
      "        [-3.1558,  2.7663],\n",
      "        [-3.1909,  2.7767],\n",
      "        [-1.9124,  1.0681],\n",
      "        [-3.2033,  2.7411],\n",
      "        [-3.2060,  2.7892],\n",
      "        [ 0.8407, -0.8689]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  20%|ââ        | 10/51 [00:45<02:59,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2109,  2.8005],\n",
      "        [-2.1449,  1.3296],\n",
      "        [-2.8536,  2.1929],\n",
      "        [-1.5405,  0.6902],\n",
      "        [-3.0333,  2.3964],\n",
      "        [-3.1767,  2.7695],\n",
      "        [-3.2149,  2.7159],\n",
      "        [-3.0994,  2.5121]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  22%|âââ       | 11/51 [00:49<02:56,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5099,  0.6397],\n",
      "        [-1.9535,  1.1056],\n",
      "        [-3.1567,  2.7229],\n",
      "        [-0.7058,  0.0464],\n",
      "        [-3.1954,  2.7818],\n",
      "        [-0.9743,  0.1974],\n",
      "        [-3.0007,  2.3635],\n",
      "        [-1.2765,  0.4788]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  24%|âââ       | 12/51 [00:54<02:53,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1968,  2.7983],\n",
      "        [-3.1400,  2.7710],\n",
      "        [-0.1323, -0.3073],\n",
      "        [-3.2106,  2.7798],\n",
      "        [-3.0609,  2.4845],\n",
      "        [-2.7962,  2.0339],\n",
      "        [-3.0390,  2.4558],\n",
      "        [-3.1879,  2.7819]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  25%|âââ       | 13/51 [00:59<02:54,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0179, -0.9779],\n",
      "        [-3.0278,  2.3962],\n",
      "        [-3.0475,  2.4421],\n",
      "        [-2.5764,  1.8436],\n",
      "        [-3.1079,  2.5155],\n",
      "        [-0.8198,  0.1153],\n",
      "        [-2.2475,  1.4614],\n",
      "        [-3.2229,  2.7572]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  27%|âââ       | 14/51 [01:03<02:45,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3043, -0.5447],\n",
      "        [-3.1994,  2.7313],\n",
      "        [-1.8163,  0.9421],\n",
      "        [ 1.3918, -1.0574],\n",
      "        [ 1.1817, -1.0261],\n",
      "        [-3.1716,  2.6331],\n",
      "        [ 0.3933, -0.6101],\n",
      "        [-3.1454,  2.5975]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  29%|âââ       | 15/51 [01:07<02:41,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4248,  1.7043],\n",
      "        [-3.1404,  2.7663],\n",
      "        [ 0.1137, -0.4335],\n",
      "        [ 1.2449, -1.0316],\n",
      "        [-2.8445,  2.1942],\n",
      "        [-3.1880,  2.7827],\n",
      "        [-3.1856,  2.7871],\n",
      "        [-3.1935,  2.7730]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  31%|ââââ      | 16/51 [01:12<02:40,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2100,  2.6788],\n",
      "        [-1.2290,  0.4393],\n",
      "        [ 1.3714, -1.0560],\n",
      "        [-3.1639,  2.6606],\n",
      "        [-3.1450,  2.6107],\n",
      "        [-3.2163,  2.7279],\n",
      "        [-3.0142,  2.3958],\n",
      "        [ 1.3420, -1.0953]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  33%|ââââ      | 17/51 [01:17<02:36,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1783,  2.7930],\n",
      "        [-2.5962,  1.8597],\n",
      "        [-1.9497,  1.1282],\n",
      "        [ 1.1526, -1.0123],\n",
      "        [-3.0561,  2.4247],\n",
      "        [-0.0360, -0.3656],\n",
      "        [-1.7644,  0.9708],\n",
      "        [-1.4805,  0.6539]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  35%|ââââ      | 18/51 [01:22<02:37,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5021,  1.7792],\n",
      "        [ 1.2467, -1.0385],\n",
      "        [-3.1753,  2.7889],\n",
      "        [-2.9068,  2.2818],\n",
      "        [-3.1693,  2.6497],\n",
      "        [ 0.1489, -0.4795],\n",
      "        [ 1.3980, -1.0752],\n",
      "        [-3.1631,  2.6243]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  37%|ââââ      | 19/51 [01:27<02:35,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3493, -1.0718],\n",
      "        [-0.0705, -0.3544],\n",
      "        [-3.2117,  2.7914],\n",
      "        [-0.8289,  0.1991],\n",
      "        [ 0.4038, -0.6140],\n",
      "        [-1.6331,  0.7842],\n",
      "        [ 1.3563, -1.0830],\n",
      "        [-2.4526,  1.7108]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  39%|ââââ      | 20/51 [01:31<02:25,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9536,  1.1252],\n",
      "        [-3.2088,  2.7216],\n",
      "        [-1.3331,  0.5007],\n",
      "        [-3.0982,  2.5204],\n",
      "        [-3.1132,  2.5583],\n",
      "        [-2.5822,  1.8298],\n",
      "        [ 0.9970, -0.9455],\n",
      "        [-2.0234,  1.2269]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  41%|ââââ      | 21/51 [01:36<02:18,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7242,  2.0196],\n",
      "        [-2.9499,  2.2998],\n",
      "        [-3.1997,  2.7970],\n",
      "        [-3.1317,  2.7522],\n",
      "        [-3.2071,  2.7130],\n",
      "        [-3.1903,  2.7440],\n",
      "        [-1.5830,  0.7405],\n",
      "        [ 0.2505, -0.5091]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  43%|âââââ     | 22/51 [01:40<02:11,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4642,  0.6133],\n",
      "        [ 1.1757, -1.0473],\n",
      "        [ 0.3019, -0.5543],\n",
      "        [-1.6345,  0.7953],\n",
      "        [-0.9621,  0.2114],\n",
      "        [-2.9112,  2.2258],\n",
      "        [-1.0607,  0.2863],\n",
      "        [-1.0961,  0.3208]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  45%|âââââ     | 23/51 [01:44<02:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8694,  1.0229],\n",
      "        [ 0.7884, -0.8344],\n",
      "        [-3.1739,  2.7778],\n",
      "        [-2.8865,  2.2046],\n",
      "        [-1.6830,  0.8193],\n",
      "        [-2.3050,  1.5067],\n",
      "        [-2.5492,  1.7894],\n",
      "        [-3.1890,  2.7614]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  47%|âââââ     | 24/51 [01:49<01:58,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4889,  0.6661],\n",
      "        [-2.4364,  1.6658],\n",
      "        [ 1.3435, -1.0801],\n",
      "        [-0.9731,  0.2438],\n",
      "        [ 1.3693, -1.0814],\n",
      "        [-2.3653,  1.6406],\n",
      "        [-3.2158,  2.7622],\n",
      "        [ 1.3600, -1.0969]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  49%|âââââ     | 25/51 [01:53<01:54,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5511, -0.0622],\n",
      "        [-3.1615,  2.7676],\n",
      "        [-1.4607,  0.6183],\n",
      "        [-2.1695,  1.3731],\n",
      "        [-3.1919,  2.7752],\n",
      "        [ 0.4148, -0.6010],\n",
      "        [-2.5478,  1.8654],\n",
      "        [-2.0784,  1.2436]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  51%|âââââ     | 26/51 [01:58<01:57,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1308,  1.3970],\n",
      "        [-1.5608,  0.7371],\n",
      "        [ 1.3607, -1.0632],\n",
      "        [-2.5188,  1.7784],\n",
      "        [-1.6605,  0.8169],\n",
      "        [ 1.3465, -1.0766],\n",
      "        [-3.1250,  2.6092],\n",
      "        [-2.1401,  1.3606]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  53%|ââââââ    | 27/51 [02:03<01:52,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7743,  0.0793],\n",
      "        [ 0.8036, -0.8407],\n",
      "        [ 0.8722, -0.8987],\n",
      "        [-3.1756,  2.6570],\n",
      "        [-0.7237,  0.0412],\n",
      "        [ 1.3261, -1.0507],\n",
      "        [-3.0141,  2.4124],\n",
      "        [-3.1564,  2.7652]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  55%|ââââââ    | 28/51 [02:08<01:47,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7211,  0.8675],\n",
      "        [-2.9397,  2.2926],\n",
      "        [ 1.2882, -1.0855],\n",
      "        [-3.0499,  2.4557],\n",
      "        [-2.2374,  1.4764],\n",
      "        [-2.9414,  2.3583],\n",
      "        [-3.1988,  2.6888],\n",
      "        [ 1.3338, -1.0734]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  57%|ââââââ    | 29/51 [02:12<01:42,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1733,  2.7753],\n",
      "        [-3.1840,  2.7741],\n",
      "        [-3.1533,  2.7741],\n",
      "        [-1.5058,  0.5978],\n",
      "        [-1.9600,  1.1219],\n",
      "        [-1.9383,  1.1369],\n",
      "        [-2.8081,  2.1412],\n",
      "        [-2.3521,  1.6039]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  59%|ââââââ    | 30/51 [02:17<01:34,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2722e-01, -1.3810e-01],\n",
      "        [-4.5669e-01, -1.2393e-01],\n",
      "        [-8.2717e-01,  1.2975e-01],\n",
      "        [-6.6382e-01,  1.1546e-03],\n",
      "        [-1.3805e+00,  5.2609e-01],\n",
      "        [ 1.0202e+00, -9.7470e-01],\n",
      "        [-2.0344e+00,  1.2020e+00],\n",
      "        [-1.5958e-01, -2.9451e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  61%|ââââââ    | 31/51 [02:21<01:28,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0668,  2.5262],\n",
      "        [ 1.3566, -1.0945],\n",
      "        [ 0.3718, -0.5925],\n",
      "        [-2.5327,  1.8166],\n",
      "        [-2.3661,  1.6811],\n",
      "        [-1.9523,  1.1035],\n",
      "        [-2.7422,  2.0217],\n",
      "        [-2.0635,  1.2453]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  63%|âââââââ   | 32/51 [02:25<01:23,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8425,  2.1759],\n",
      "        [-2.9399,  2.3148],\n",
      "        [-2.6125,  1.8908],\n",
      "        [ 0.6652, -0.7683],\n",
      "        [ 0.0450, -0.4019],\n",
      "        [-1.5507,  0.7113],\n",
      "        [ 1.1428, -1.0135],\n",
      "        [ 1.4285, -1.0767]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  65%|âââââââ   | 33/51 [02:29<01:18,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2875,  1.5203],\n",
      "        [-2.5832,  1.8941],\n",
      "        [-2.2116,  1.4114],\n",
      "        [-0.4674, -0.0959],\n",
      "        [-1.9173,  1.1143],\n",
      "        [-3.0804,  2.4971],\n",
      "        [-2.7803,  2.0696],\n",
      "        [-1.3149,  0.5095]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  67%|âââââââ   | 34/51 [02:34<01:13,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5956,  1.7718],\n",
      "        [-0.7749,  0.0955],\n",
      "        [ 0.8500, -0.8538],\n",
      "        [ 0.2823, -0.5431],\n",
      "        [ 1.3724, -1.0500],\n",
      "        [-2.3651,  1.5904],\n",
      "        [ 0.9246, -0.9213],\n",
      "        [-2.6482,  1.9284]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  69%|âââââââ   | 35/51 [02:38<01:11,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1331,  2.5524],\n",
      "        [-3.1872,  2.7934],\n",
      "        [-3.1479,  2.7084],\n",
      "        [-3.1832,  2.7086],\n",
      "        [-3.1866,  2.7022],\n",
      "        [-0.6897,  0.0250],\n",
      "        [-2.6012,  1.8670],\n",
      "        [ 1.3289, -1.0675]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  71%|âââââââ   | 36/51 [02:43<01:06,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1308,  1.3318],\n",
      "        [-0.8877,  0.1763],\n",
      "        [-2.1823,  1.4274],\n",
      "        [ 1.3378, -1.0816],\n",
      "        [ 0.2745, -0.5069],\n",
      "        [-3.1980,  2.7238],\n",
      "        [-2.7059,  1.9726],\n",
      "        [-2.1275,  1.3332]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  73%|ââââââââ  | 37/51 [02:48<01:03,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2078,  2.7776],\n",
      "        [ 1.2104, -1.0308],\n",
      "        [ 0.4212, -0.6100],\n",
      "        [ 1.2925, -1.0791],\n",
      "        [-3.1951,  2.7925],\n",
      "        [-2.0279,  1.2188],\n",
      "        [-2.7700,  2.0845],\n",
      "        [ 0.9598, -0.9308]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  75%|ââââââââ  | 38/51 [02:53<01:00,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3588, -1.0907],\n",
      "        [-1.1043,  0.3275],\n",
      "        [-3.1665,  2.7765],\n",
      "        [-1.3034,  0.5314],\n",
      "        [-3.1491,  2.7707],\n",
      "        [-3.1922,  2.6970],\n",
      "        [-3.0859,  2.5194],\n",
      "        [ 0.9119, -0.8955]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  76%|ââââââââ  | 39/51 [02:57<00:53,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1945,  2.7872],\n",
      "        [-3.1947,  2.7113],\n",
      "        [ 1.4139, -1.0936],\n",
      "        [-3.2007,  2.7759],\n",
      "        [ 1.3869, -1.0983],\n",
      "        [-0.9958,  0.2351],\n",
      "        [-3.0112,  2.3651],\n",
      "        [-3.2038,  2.7998]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  78%|ââââââââ  | 40/51 [03:01<00:48,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2421,  0.4883],\n",
      "        [ 1.2034, -1.0511],\n",
      "        [-3.1963,  2.7828],\n",
      "        [ 0.9999, -0.9657],\n",
      "        [-1.2790,  0.4466],\n",
      "        [-3.1735,  2.6451],\n",
      "        [ 1.2365, -1.0549],\n",
      "        [ 0.5568, -0.7051]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  80%|ââââââââ  | 41/51 [03:05<00:43,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3423, -1.0910],\n",
      "        [ 1.2792, -1.0619],\n",
      "        [ 1.2771, -1.0361],\n",
      "        [-2.0934,  1.3338],\n",
      "        [ 0.5900, -0.7255],\n",
      "        [-3.1457,  2.7758],\n",
      "        [-2.3611,  1.6411],\n",
      "        [-3.2081,  2.7919]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  82%|âââââââââ | 42/51 [03:09<00:38,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.6679,  1.9419],\n",
      "        [-1.9656,  1.1518],\n",
      "        [-3.1301,  2.5764],\n",
      "        [-3.1795,  2.6576],\n",
      "        [-1.4228,  0.5777],\n",
      "        [-2.1648,  1.3741],\n",
      "        [-3.1820,  2.7828],\n",
      "        [-3.0221,  2.4067]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  84%|âââââââââ | 43/51 [03:13<00:34,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2143,  2.7632],\n",
      "        [-2.9056,  2.2608],\n",
      "        [ 1.3182, -1.0793],\n",
      "        [-2.1765,  1.4223],\n",
      "        [-3.1154,  2.5835],\n",
      "        [-3.1728,  2.7677],\n",
      "        [ 1.3403, -1.0925],\n",
      "        [-0.5491, -0.0611]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  86%|âââââââââ | 44/51 [03:18<00:30,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2153,  2.7373],\n",
      "        [-3.2096,  2.7976],\n",
      "        [-3.2036,  2.7591],\n",
      "        [-3.2162,  2.7959],\n",
      "        [-0.9563,  0.2273],\n",
      "        [-3.2012,  2.7447],\n",
      "        [ 1.1923, -1.0526],\n",
      "        [-3.1991,  2.7564]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  88%|âââââââââ | 45/51 [03:23<00:26,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2668,  1.4232],\n",
      "        [-2.3256,  1.5842],\n",
      "        [-2.7625,  2.0707],\n",
      "        [ 1.3068, -1.0516],\n",
      "        [-2.4183,  1.6585],\n",
      "        [-2.5501,  1.8183],\n",
      "        [-2.9358,  2.3314],\n",
      "        [-2.3695,  1.6170]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  90%|âââââââââ | 46/51 [03:27<00:22,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9610, -0.9278],\n",
      "        [-2.9314,  2.2895],\n",
      "        [-3.1449,  2.7668],\n",
      "        [-1.8638,  1.0022],\n",
      "        [-2.7574,  2.0107],\n",
      "        [-3.2009,  2.7993],\n",
      "        [-3.1954,  2.7033],\n",
      "        [-1.6754,  0.8472]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  92%|ââââââââââ| 47/51 [03:32<00:18,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1476, -1.0601],\n",
      "        [ 1.3759, -1.0547],\n",
      "        [-0.9250,  0.1878],\n",
      "        [-3.1579,  2.6812],\n",
      "        [-2.6467,  1.9231],\n",
      "        [ 0.1745, -0.5052],\n",
      "        [ 1.3546, -1.0861],\n",
      "        [ 1.0821, -1.0020]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  94%|ââââââââââ| 48/51 [03:36<00:13,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3477, -0.5600],\n",
      "        [ 1.4137, -1.0883],\n",
      "        [-2.0451,  1.2514],\n",
      "        [-3.2081,  2.7173],\n",
      "        [-1.8683,  1.0134],\n",
      "        [-3.1964,  2.8001],\n",
      "        [-0.3271, -0.2212],\n",
      "        [-3.1510,  2.7615]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  96%|ââââââââââ| 49/51 [03:41<00:09,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9890,  2.3864],\n",
      "        [-0.3441, -0.1820],\n",
      "        [-2.5237,  1.7909],\n",
      "        [ 1.1096, -1.0121],\n",
      "        [-3.1888,  2.7937],\n",
      "        [-0.7072,  0.0301],\n",
      "        [-3.1906,  2.6910],\n",
      "        [-0.7787,  0.1050]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:  98%|ââââââââââ| 50/51 [03:46<00:04,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2151,  2.7859],\n",
      "        [-3.1834,  2.6645],\n",
      "        [-2.0960,  1.2165],\n",
      "        [-0.1442, -0.2922],\n",
      "        [-3.2159,  2.7947],\n",
      "        [-0.9473,  0.2199],\n",
      "        [-2.5020,  1.7557],\n",
      "        [-3.1770,  2.6475]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|ââââââââââ| 51/51 [03:51<00:00,  4.68s/it]\n",
      "04/02/2019 16:23:33 - INFO - __main__ -   ***** Eval results *****\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     eval_accuracy = 0.8602941176470589\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     eval_loss = 0.415484676977583\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     global_step = 345\n",
      "04/02/2019 16:23:33 - INFO - __main__ -     loss = 0.18990675794041675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3128, -1.0338],\n",
      "        [-3.2089,  2.7509],\n",
      "        [-1.9044,  1.0529],\n",
      "        [ 1.4479, -1.0434],\n",
      "        [ 0.1515, -0.4591],\n",
      "        [-3.1568,  2.7844],\n",
      "        [ 1.2631, -1.0399],\n",
      "        [-2.4895,  1.8019]])\n"
     ]
    }
   ],
   "source": [
    "if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    eval_examples = processor.get_dev_examples(data_dir)\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "        print(logits)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss/nb_tr_steps if do_train else None\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "              'global_step': global_step,\n",
    "              'loss': loss}\n",
    "\n",
    "    output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.415484676977583,\n",
       " 'eval_accuracy': 0.8602941176470589,\n",
       " 'global_step': 345,\n",
       " 'loss': 0.18990675794041675}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
