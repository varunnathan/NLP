{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF Entity Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import typing\n",
    "from typing import Any, Dict, List, Optional, Text, Tuple\n",
    "\n",
    "from rasa_nlu.config import InvalidConfigError, RasaNLUModelConfig\n",
    "from rasa_nlu.extractors import EntityExtractor\n",
    "from rasa_nlu.model import Metadata\n",
    "from rasa_nlu.training_data import Message, TrainingData\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except ImportError:\n",
    "    spacy = None\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if typing.TYPE_CHECKING:\n",
    "    import sklearn_crfsuite\n",
    "\n",
    "\n",
    "class CRFEntityExtractor(EntityExtractor):\n",
    "\n",
    "    provides = [\"entities\"]\n",
    "\n",
    "    requires = [\"tokens\"]\n",
    "\n",
    "    defaults = {\n",
    "        # BILOU_flag determines whether to use BILOU tagging or not.\n",
    "        # More rigorous however requires more examples per entity\n",
    "        # rule of thumb: use only if more than 100 egs. per entity\n",
    "        \"BILOU_flag\": True,\n",
    "\n",
    "        # crf_features is [before, word, after] array with before, word,\n",
    "        # after holding keys about which\n",
    "        # features to use for each word, for example, 'title' in\n",
    "        # array before will have the feature\n",
    "        # \"is the preceding word in title case?\"\n",
    "        # POS features require spaCy to be installed\n",
    "        \"features\": [\n",
    "            [\"low\", \"title\", \"upper\"],\n",
    "            [\"bias\", \"low\", \"prefix5\", \"prefix2\", \"suffix5\", \"suffix3\",\n",
    "             \"suffix2\", \"upper\", \"title\", \"digit\", \"pattern\"],\n",
    "            [\"low\", \"title\", \"upper\"]],\n",
    "\n",
    "        # The maximum number of iterations for optimization algorithms.\n",
    "        \"max_iterations\": 50,\n",
    "\n",
    "        # weight of theL1 regularization\n",
    "        \"L1_c\": 0.1,\n",
    "\n",
    "        # weight of the L2 regularization\n",
    "        \"L2_c\": 0.1\n",
    "    }\n",
    "\n",
    "    function_dict = {\n",
    "        'low': lambda doc: doc[0].lower(),\n",
    "        'title': lambda doc: doc[0].istitle(),\n",
    "        'prefix5': lambda doc: doc[0][:5],\n",
    "        'prefix2': lambda doc: doc[0][:2],\n",
    "        'suffix5': lambda doc: doc[0][-5:],\n",
    "        'suffix3': lambda doc: doc[0][-3:],\n",
    "        'suffix2': lambda doc: doc[0][-2:],\n",
    "        'suffix1': lambda doc: doc[0][-1:],\n",
    "        'pos': lambda doc: doc[1],\n",
    "        'pos2': lambda doc: doc[1][:2],\n",
    "        'bias': lambda doc: 'bias',\n",
    "        'upper': lambda doc: doc[0].isupper(),\n",
    "        'digit': lambda doc: doc[0].isdigit(),\n",
    "        'pattern': lambda doc: doc[3],\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 component_config: Optional[Dict[Text, Any]] = None,\n",
    "                 ent_tagger: Optional[Dict[Text, Any]] = None) -> None:\n",
    "\n",
    "        super(CRFEntityExtractor, self).__init__(component_config)\n",
    "\n",
    "        self.ent_tagger = ent_tagger\n",
    "\n",
    "        self._validate_configuration()\n",
    "\n",
    "        self._check_pos_features_and_spacy()\n",
    "\n",
    "    def _check_pos_features_and_spacy(self):\n",
    "        import itertools\n",
    "        features = self.component_config.get(\"features\", [])\n",
    "        fts = set(itertools.chain.from_iterable(features))\n",
    "        self.pos_features = ('pos' in fts or 'pos2' in fts)\n",
    "        if self.pos_features:\n",
    "            self._check_spacy()\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_spacy():\n",
    "        if spacy is None:\n",
    "            raise ImportError(\n",
    "                'Failed to import `spaCy`. '\n",
    "                '`spaCy` is required for POS features '\n",
    "                'See https://spacy.io/usage/ for installation'\n",
    "                'instructions.')\n",
    "\n",
    "    def _validate_configuration(self):\n",
    "        if len(self.component_config.get(\"features\", [])) % 2 != 1:\n",
    "            raise ValueError(\"Need an odd number of crf feature \"\n",
    "                             \"lists to have a center word.\")\n",
    "\n",
    "    @classmethod\n",
    "    def required_packages(cls):\n",
    "        return [\"sklearn_crfsuite\", \"sklearn\"]\n",
    "\n",
    "    def train(self,\n",
    "              training_data: TrainingData,\n",
    "              config: RasaNLUModelConfig,\n",
    "              **kwargs: Any) -> None:\n",
    "\n",
    "        # checks whether there is at least one\n",
    "        # example with an entity annotation\n",
    "        if training_data.entity_examples:\n",
    "            self._check_spacy_doc(training_data.training_examples[0])\n",
    "\n",
    "            # filter out pre-trained entity examples\n",
    "            filtered_entity_examples = self.filter_trainable_entities(\n",
    "                training_data.training_examples)\n",
    "\n",
    "            # convert the dataset into features\n",
    "            # this will train on ALL examples, even the ones\n",
    "            # without annotations\n",
    "            dataset = self._create_dataset(filtered_entity_examples)\n",
    "\n",
    "            self._train_model(dataset)\n",
    "\n",
    "    def _create_dataset(self,\n",
    "                        examples: List[Message]\n",
    "                        ) -> List[List[Tuple[Text, Text, Text, Text]]]:\n",
    "        dataset = []\n",
    "        for example in examples:\n",
    "            entity_offsets = self._convert_example(example)\n",
    "            dataset.append(self._from_json_to_crf(example, entity_offsets))\n",
    "        return dataset\n",
    "\n",
    "    def _check_spacy_doc(self, message):\n",
    "        if self.pos_features and message.get(\"spacy_doc\") is None:\n",
    "            raise InvalidConfigError(\n",
    "                'Could not find `spacy_doc` attribute for '\n",
    "                'message {}\\n'\n",
    "                'POS features require a pipeline component '\n",
    "                'that provides `spacy_doc` attributes, i.e. `SpacyNLP`. '\n",
    "                'See https://nlu.rasa.com/pipeline.html#nlp-spacy '\n",
    "                'for details'.format(message.text))\n",
    "\n",
    "    def process(self, message: Message, **kwargs: Any) -> None:\n",
    "\n",
    "        self._check_spacy_doc(message)\n",
    "\n",
    "        extracted = self.add_extractor_name(self.extract_entities(message))\n",
    "        message.set(\"entities\", message.get(\"entities\", []) + extracted,\n",
    "                    add_to_output=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_example(example: Message) -> List[Tuple[int, int, Text]]:\n",
    "\n",
    "        def convert_entity(entity):\n",
    "            return entity[\"start\"], entity[\"end\"], entity[\"entity\"]\n",
    "\n",
    "        return [convert_entity(ent) for ent in example.get(\"entities\", [])]\n",
    "\n",
    "    def extract_entities(self, message: Message) -> List[Dict[Text, Any]]:\n",
    "        \"\"\"Take a sentence and return entities in json format\"\"\"\n",
    "\n",
    "        if self.ent_tagger is not None:\n",
    "            text_data = self._from_text_to_crf(message)\n",
    "            features = self._sentence_to_features(text_data)\n",
    "            ents = self.ent_tagger.predict_marginals_single(features)\n",
    "            return self._from_crf_to_json(message, ents)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def most_likely_entity(self, idx, entities):\n",
    "        if len(entities) > idx:\n",
    "            entity_probs = entities[idx]\n",
    "        else:\n",
    "            entity_probs = None\n",
    "        if entity_probs:\n",
    "            label = max(entity_probs,\n",
    "                        key=lambda key: entity_probs[key])\n",
    "            if self.component_config[\"BILOU_flag\"]:\n",
    "                # if we are using bilou flags, we will combine the prob\n",
    "                # of the B, I, L and U tags for an entity (so if we have a\n",
    "                # score of 60% for `B-address` and 40% and 30%\n",
    "                # for `I-address`, we will return 70%)\n",
    "                return label, sum([v\n",
    "                                   for k, v in entity_probs.items()\n",
    "                                   if k[2:] == label[2:]])\n",
    "            else:\n",
    "                return label, entity_probs[label]\n",
    "        else:\n",
    "            return \"\", 0.0\n",
    "\n",
    "    def _create_entity_dict(self, tokens, start, end, entity, confidence):\n",
    "        if self.pos_features:\n",
    "            _start = tokens[start].idx\n",
    "            _end = tokens[start:end + 1].end_char\n",
    "            value = tokens[start:end + 1].text\n",
    "        else:\n",
    "            _start = tokens[start].offset\n",
    "            _end = tokens[end].end\n",
    "            value = ' '.join(t.text for t in tokens[start:end + 1])\n",
    "\n",
    "        return {\n",
    "            'start': _start,\n",
    "            'end': _end,\n",
    "            'value': value,\n",
    "            'entity': entity,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _entity_from_label(label):\n",
    "        return label[2:]\n",
    "\n",
    "    @staticmethod\n",
    "    def _bilou_from_label(label):\n",
    "        if len(label) >= 2 and label[1] == \"-\":\n",
    "            return label[0].upper()\n",
    "        return None\n",
    "\n",
    "    def _find_bilou_end(self, word_idx, entities):\n",
    "        ent_word_idx = word_idx + 1\n",
    "        finished = False\n",
    "\n",
    "        # get information about the first word, tagged with `B-...`\n",
    "        label, confidence = self.most_likely_entity(word_idx, entities)\n",
    "        entity_label = self._entity_from_label(label)\n",
    "\n",
    "        while not finished:\n",
    "            label, label_confidence = self.most_likely_entity(\n",
    "                ent_word_idx, entities)\n",
    "\n",
    "            confidence = min(confidence, label_confidence)\n",
    "\n",
    "            if label[2:] != entity_label:\n",
    "                # words are not tagged the same entity class\n",
    "                logger.debug(\"Inconsistent BILOU tagging found, B- tag, L- \"\n",
    "                             \"tag pair encloses multiple entity classes.i.e. \"\n",
    "                             \"[B-a, I-b, L-a] instead of [B-a, I-a, L-a].\\n\"\n",
    "                             \"Assuming B- class is correct.\")\n",
    "\n",
    "            if label.startswith('L-'):\n",
    "                # end of the entity\n",
    "                finished = True\n",
    "            elif label.startswith('I-'):\n",
    "                # middle part of the entity\n",
    "                ent_word_idx += 1\n",
    "            else:\n",
    "                # entity not closed by an L- tag\n",
    "                finished = True\n",
    "                ent_word_idx -= 1\n",
    "                logger.debug(\"Inconsistent BILOU tagging found, B- tag not \"\n",
    "                             \"closed by L- tag, i.e [B-a, I-a, O] instead of \"\n",
    "                             \"[B-a, L-a, O].\\nAssuming last tag is L-\")\n",
    "        return ent_word_idx, confidence\n",
    "\n",
    "    def _handle_bilou_label(self, word_idx, entities):\n",
    "        label, confidence = self.most_likely_entity(word_idx, entities)\n",
    "        entity_label = self._entity_from_label(label)\n",
    "\n",
    "        if self._bilou_from_label(label) == \"U\":\n",
    "            return word_idx, confidence, entity_label\n",
    "\n",
    "        elif self._bilou_from_label(label) == \"B\":\n",
    "            # start of multi word-entity need to represent whole extent\n",
    "            ent_word_idx, confidence = self._find_bilou_end(\n",
    "                word_idx, entities)\n",
    "            return ent_word_idx, confidence, entity_label\n",
    "\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    def _from_crf_to_json(self,\n",
    "                          message: Message,\n",
    "                          entities: List[Any]) -> List[Dict[Text, Any]]:\n",
    "\n",
    "        if self.pos_features:\n",
    "            tokens = message.get(\"spacy_doc\")\n",
    "        else:\n",
    "            tokens = message.get(\"tokens\")\n",
    "\n",
    "        if len(tokens) != len(entities):\n",
    "            raise Exception('Inconsistency in amount of tokens '\n",
    "                            'between crfsuite and message')\n",
    "\n",
    "        if self.component_config[\"BILOU_flag\"]:\n",
    "            return self._convert_bilou_tagging_to_entity_result(\n",
    "                tokens, entities)\n",
    "        else:\n",
    "            # not using BILOU tagging scheme, multi-word entities are split.\n",
    "            return self._convert_simple_tagging_to_entity_result(\n",
    "                tokens, entities)\n",
    "\n",
    "    def _convert_bilou_tagging_to_entity_result(self, tokens, entities):\n",
    "        # using the BILOU tagging scheme\n",
    "        json_ents = []\n",
    "        word_idx = 0\n",
    "        while word_idx < len(tokens):\n",
    "            end_idx, confidence, entity_label = self._handle_bilou_label(\n",
    "                word_idx, entities)\n",
    "\n",
    "            if end_idx is not None:\n",
    "                ent = self._create_entity_dict(tokens,\n",
    "                                               word_idx,\n",
    "                                               end_idx,\n",
    "                                               entity_label,\n",
    "                                               confidence)\n",
    "                json_ents.append(ent)\n",
    "                word_idx = end_idx + 1\n",
    "            else:\n",
    "                word_idx += 1\n",
    "        return json_ents\n",
    "\n",
    "    def _convert_simple_tagging_to_entity_result(self, tokens, entities):\n",
    "        json_ents = []\n",
    "\n",
    "        for word_idx in range(len(tokens)):\n",
    "            entity_label, confidence = self.most_likely_entity(\n",
    "                word_idx, entities)\n",
    "            word = tokens[word_idx]\n",
    "            if entity_label != 'O':\n",
    "                if self.pos_features:\n",
    "                    start = word.idx\n",
    "                    end = word.idx + len(word)\n",
    "                else:\n",
    "                    start = word.offset\n",
    "                    end = word.end\n",
    "                ent = {'start': start,\n",
    "                       'end': end,\n",
    "                       'value': word.text,\n",
    "                       'entity': entity_label,\n",
    "                       'confidence': confidence}\n",
    "                json_ents.append(ent)\n",
    "\n",
    "        return json_ents\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls,\n",
    "             meta: Dict[Text, Any],\n",
    "             model_dir: Text = None,\n",
    "             model_metadata: Metadata = None,\n",
    "             cached_component: Optional['CRFEntityExtractor'] = None,\n",
    "             **kwargs: Any\n",
    "             ) -> 'CRFEntityExtractor':\n",
    "        from sklearn.externals import joblib\n",
    "\n",
    "        file_name = meta.get(\"file\")\n",
    "        model_file = os.path.join(model_dir, file_name)\n",
    "\n",
    "        if os.path.exists(model_file):\n",
    "            ent_tagger = joblib.load(model_file)\n",
    "            return cls(meta, ent_tagger)\n",
    "        else:\n",
    "            return cls(meta)\n",
    "\n",
    "    def persist(self,\n",
    "                file_name: Text,\n",
    "                model_dir: Text) -> Optional[Dict[Text, Any]]:\n",
    "        \"\"\"Persist this model into the passed directory.\n",
    "        Returns the metadata necessary to load the model again.\"\"\"\n",
    "\n",
    "        from sklearn.externals import joblib\n",
    "        file_name = file_name + \".pkl\"\n",
    "        if self.ent_tagger:\n",
    "            model_file_name = os.path.join(model_dir, file_name)\n",
    "            joblib.dump(self.ent_tagger, model_file_name)\n",
    "\n",
    "        return {\"file\": file_name}\n",
    "\n",
    "    def _sentence_to_features(self,\n",
    "                              sentence: List[Tuple[Text, Text, Text, Text]]\n",
    "                              ) -> List[Dict[Text, Any]]:\n",
    "        \"\"\"Convert a word into discrete features in self.crf_features,\n",
    "        including word before and word after.\"\"\"\n",
    "\n",
    "        configured_features = self.component_config[\"features\"]\n",
    "        sentence_features = []\n",
    "\n",
    "        for word_idx in range(len(sentence)):\n",
    "            # word before(-1), current word(0), next word(+1)\n",
    "            feature_span = len(configured_features)\n",
    "            half_span = feature_span // 2\n",
    "            feature_range = range(- half_span, half_span + 1)\n",
    "            prefixes = [str(i) for i in feature_range]\n",
    "            word_features = {}\n",
    "            for f_i in feature_range:\n",
    "                if word_idx + f_i >= len(sentence):\n",
    "                    word_features['EOS'] = True\n",
    "                    # End Of Sentence\n",
    "                elif word_idx + f_i < 0:\n",
    "                    word_features['BOS'] = True\n",
    "                    # Beginning Of Sentence\n",
    "                else:\n",
    "                    word = sentence[word_idx + f_i]\n",
    "                    f_i_from_zero = f_i + half_span\n",
    "                    prefix = prefixes[f_i_from_zero]\n",
    "                    features = configured_features[f_i_from_zero]\n",
    "                    for feature in features:\n",
    "                        if feature == \"pattern\":\n",
    "                            # add all regexes as a feature\n",
    "                            regex_patterns = self.function_dict[feature](word)\n",
    "                            for p_name, matched in regex_patterns.items():\n",
    "                                feature_name = (prefix + \":\" +\n",
    "                                                feature +\n",
    "                                                \":\" + p_name)\n",
    "                                word_features[feature_name] = matched\n",
    "                        else:\n",
    "                            # append each feature to a feature vector\n",
    "                            value = self.function_dict[feature](word)\n",
    "                            word_features[prefix + \":\" + feature] = value\n",
    "            sentence_features.append(word_features)\n",
    "        return sentence_features\n",
    "\n",
    "    @staticmethod\n",
    "    def _sentence_to_labels(sentence: List[Tuple[Text, Text, Text, Text]]\n",
    "                            ) -> List[Text]:\n",
    "\n",
    "        return [label for _, _, label, _ in sentence]\n",
    "\n",
    "    def _from_json_to_crf(self,\n",
    "                          message: Message,\n",
    "                          entity_offsets: List[Tuple[int, int, Text]]\n",
    "                          ) -> List[Tuple[Text, Text, Text, Text]]:\n",
    "        \"\"\"Convert json examples to format of underlying crfsuite.\"\"\"\n",
    "\n",
    "        if self.pos_features:\n",
    "            from spacy.gold import GoldParse\n",
    "\n",
    "            doc = message.get(\"spacy_doc\")\n",
    "            gold = GoldParse(doc, entities=entity_offsets)\n",
    "            ents = [l[5] for l in gold.orig_annot]\n",
    "        else:\n",
    "            tokens = message.get(\"tokens\")\n",
    "            ents = self._bilou_tags_from_offsets(tokens, entity_offsets)\n",
    "\n",
    "        if '-' in ents:\n",
    "            logger.warning(\"Misaligned entity annotation in sentence '{}'. \"\n",
    "                           \"Make sure the start and end values of the \"\n",
    "                           \"annotated training examples end at token \"\n",
    "                           \"boundaries (e.g. don't include trailing \"\n",
    "                           \"whitespaces or punctuation).\"\n",
    "                           \"\".format(message.text))\n",
    "        if not self.component_config[\"BILOU_flag\"]:\n",
    "            for i, label in enumerate(ents):\n",
    "                if self._bilou_from_label(label) in {\"B\", \"I\", \"U\", \"L\"}:\n",
    "                    # removes BILOU prefix from label\n",
    "                    ents[i] = self._entity_from_label(label)\n",
    "\n",
    "        return self._from_text_to_crf(message, ents)\n",
    "\n",
    "    @staticmethod\n",
    "    def _bilou_tags_from_offsets(tokens, entities, missing='O'):\n",
    "        # From spacy.spacy.GoldParse, under MIT License\n",
    "        starts = {token.offset: i for i, token in enumerate(tokens)}\n",
    "        ends = {token.end: i for i, token in enumerate(tokens)}\n",
    "        bilou = ['-' for _ in tokens]\n",
    "        # Handle entity cases\n",
    "        for start_char, end_char, label in entities:\n",
    "            start_token = starts.get(start_char)\n",
    "            end_token = ends.get(end_char)\n",
    "            # Only interested if the tokenization is correct\n",
    "            if start_token is not None and end_token is not None:\n",
    "                if start_token == end_token:\n",
    "                    bilou[start_token] = 'U-%s' % label\n",
    "                else:\n",
    "                    bilou[start_token] = 'B-%s' % label\n",
    "                    for i in range(start_token + 1, end_token):\n",
    "                        bilou[i] = 'I-%s' % label\n",
    "                    bilou[end_token] = 'L-%s' % label\n",
    "        # Now distinguish the O cases from ones where we miss the tokenization\n",
    "        entity_chars = set()\n",
    "        for start_char, end_char, label in entities:\n",
    "            for i in range(start_char, end_char):\n",
    "                entity_chars.add(i)\n",
    "        for n, token in enumerate(tokens):\n",
    "            for i in range(token.offset, token.end):\n",
    "                if i in entity_chars:\n",
    "                    break\n",
    "            else:\n",
    "                bilou[n] = missing\n",
    "\n",
    "        return bilou\n",
    "\n",
    "    @staticmethod\n",
    "    def __pattern_of_token(message, i):\n",
    "        if message.get(\"tokens\") is not None:\n",
    "            return message.get(\"tokens\")[i].get(\"pattern\", {})\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def __tag_of_token(token):\n",
    "        if spacy.about.__version__ > \"2\" and token._.has(\"tag\"):\n",
    "            return token._.get(\"tag\")\n",
    "        else:\n",
    "            return token.tag_\n",
    "\n",
    "    def _from_text_to_crf(self,\n",
    "                          message: Message,\n",
    "                          entities: List[Text] = None\n",
    "                          ) -> List[Tuple[Text, Text, Text, Text]]:\n",
    "        \"\"\"Takes a sentence and switches it to crfsuite format.\"\"\"\n",
    "\n",
    "        crf_format = []\n",
    "        if self.pos_features:\n",
    "            tokens = message.get(\"spacy_doc\")\n",
    "        else:\n",
    "            tokens = message.get(\"tokens\")\n",
    "        for i, token in enumerate(tokens):\n",
    "            pattern = self.__pattern_of_token(message, i)\n",
    "            entity = entities[i] if entities else \"N/A\"\n",
    "            tag = self.__tag_of_token(token) if self.pos_features else None\n",
    "            crf_format.append((token.text, tag, entity, pattern))\n",
    "        return crf_format\n",
    "\n",
    "    def _train_model(self,\n",
    "                     df_train: List[List[Tuple[Text, Text, Text, Text]]]\n",
    "                     ) -> None:\n",
    "        \"\"\"Train the crf tagger based on the training data.\"\"\"\n",
    "        import sklearn_crfsuite\n",
    "\n",
    "        X_train = [self._sentence_to_features(sent) for sent in df_train]\n",
    "        y_train = [self._sentence_to_labels(sent) for sent in df_train]\n",
    "        self.ent_tagger = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            # coefficient for L1 penalty\n",
    "            c1=self.component_config[\"L1_c\"],\n",
    "            # coefficient for L2 penalty\n",
    "            c2=self.component_config[\"L2_c\"],\n",
    "            # stop earlier\n",
    "            max_iterations=self.component_config[\"max_iterations\"],\n",
    "            # include transitions that are possible, but not observed\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "        self.ent_tagger.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.config import RasaNLUModelConfig\n",
    "from rasa_nlu.extractors.spacy_entity_extractor import SpacyEntityExtractor\n",
    "from rasa_nlu.training_data import TrainingData, Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import sklearn_crfsuite\n",
    "except:\n",
    "    !pip install sklearn_crfsuite\n",
    "    import sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = {\n",
    "  \"intent\":{\n",
    "   \"restaurant_search\": [\"anywhere in the west\", \"central indian restaurant\", \"looking for french hotels\",\n",
    "\t\t\t\t\t\t\"show me some south indian places to eat in the north\", \"looking for exotic eating spots\",\n",
    "\t\t\t\t\t\t\"any north indian hotels near me\", \"show me chinese restaurants\", \"want to go to an italian restaurant\",\n",
    "\t\t\t\t\t\t\"i'm looking for a place in the north of town\", \"show me a mexican place in the centre\",\n",
    "\t\t\t\t\t\t\"hi, show me some authentic tamilian hotels\", \"hello, i need some fish food\", \"good morning, i'm looking for spanish or mexican hotels, thanks\"],\n",
    "   \"greet\": [\"hi\", \"hello\", \"good day\", \"hi!\", \"hey\", \"good morning\", \"good evening\", \"hey there!\", \"hey, how are you?\"],\n",
    "   \"thanks\": [\"thanks\", \"thank you\", \"thanx\", \"this is helpful, thanks\", \"regards\", \"good bye\", \"bye\", \"thanks & regards\", \"thanks and regards\",\n",
    "             \"with regards\"]\n",
    "  },\n",
    " \"entities\": {\n",
    "  \"restaurant_search\": [[{\"value\": \"west\", \"entity\": \"location\"}],\n",
    "                        [{\"value\": \"central\", \"entity\": \"location\"},\n",
    "                         {\"value\": \"indian\", \"entity\": \"cuisine\"}],\n",
    "                        [{\"value\": \"french\", \"entity\": \"cuisine\"}],\n",
    "                        [{\"value\": \"south indian\", \"entity\": \"cuisine\"},\n",
    "                         {\"value\": \"north\", \"entity\": \"location\"}], [],\n",
    "                        [{\"value\": \"north indian\", \"entity\": \"cuisine\"}],\n",
    "                        [{\"value\": \"chinese\", \"entity\": \"cuisine\"}],\n",
    "                        [{\"value\": \"italian\", \"entity\": \"cuisine\"}],\n",
    "                        [{\"value\": \"north\", \"entity\": \"location\"}],\n",
    "                        [{\"value\": \"mexican\", \"entity\": \"cuisine\"},\n",
    "                         {\"value\": \"centre\", \"entity\": \"location\"}],\n",
    "                        [{\"value\": \"tamilian\", \"entity\": \"cuisine\"}],\n",
    "                        [{\"value\": \"fish\", \"entity\": \"cuisine\"}],\n",
    "                        [{\"value\": \"spanish\", \"entity\": \"cuisine\"},\n",
    "                         {\"value\": \"mexican\", \"entity\": \"cuisine\"}]],\n",
    "  \"greet\": [[], [], [], [], [], [], [], [], []],\n",
    "  \"thanks\": [[], [], [], [], [], [], [], [], [], []]\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"i had / i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i shall / i will\",\n",
    "\"i'll've\": \"i shall have / i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent\n",
      "restaurant_search\n",
      "greet\n",
      "thanks\n",
      "entities\n",
      "restaurant_search\n",
      "greet\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "# preprocessing input data\n",
    "# lowering, contractions, regex to remove special characters\n",
    "import re\n",
    "def regex_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\x01', ' ')\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    text = re.sub(r\"http[s]?\\S+\", \" \", text)\n",
    "    text = re.sub(r\"xx+\", \" \", text)\n",
    "    text = re.sub(r\"x{2,}\", \"\", text)\n",
    "    text = re.sub(r\"www\\S+\\.com\", \"\", text)\n",
    "    text = re.sub(r\"\\S+fresh[a-zA-Z]+.com\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[-()._\\#/@&%;*:<>{}+=~!|?,]\", \" \", text)\n",
    "    text = re.sub(r\"[\\[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"`\", \"\", text)\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    # lower\n",
    "    text = str(text).lower()\n",
    "    # contractions\n",
    "    text = \" \".join(CONTRACTIONS[y.lower()].split('/')[0]\n",
    "                    if y.lower() in CONTRACTIONS else y\n",
    "                    for y in str(text).split())\n",
    "    # regex cleaning\n",
    "    text = regex_cleaning(text)\n",
    "    return text\n",
    "    \n",
    "inp_data_pre = {}\n",
    "for key, value in train_examples.items():\n",
    "    print(key)\n",
    "    inp_data_pre[key] = {}\n",
    "    for key1, value1 in value.items():\n",
    "        print(key1)\n",
    "        if key == \"intent\":\n",
    "            value1_new = [preprocess(x) for x in value1]\n",
    "        elif key == \"entities\":\n",
    "            value1_new = []\n",
    "            for item in value1:\n",
    "                new_item = []\n",
    "                for item1 in item:\n",
    "                    if type(item1['value']) == str:\n",
    "                        item1['value'] = preprocess(item1['value'])\n",
    "                    new_item.append(item1)\n",
    "                value1_new.append(new_item)\n",
    "        inp_data_pre[key][key1] = value1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_pos_entity(utterance, entity):\n",
    "    start_pos = utterance.index(entity)\n",
    "    end_pos = start_pos + len(entity)\n",
    "    return start_pos, end_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en')\n",
    "intents = inp_data_pre['intent']\n",
    "entities = inp_data_pre['entities']\n",
    "train_examples_crfformat = []\n",
    "for intent, intent_values in intents.items():\n",
    "    for i, intent_value in enumerate(intent_values):\n",
    "        out = {}\n",
    "        entity_value = entities[intent][i]\n",
    "        if entity_value:\n",
    "            entity_value_new = []\n",
    "            for ent in entity_value:\n",
    "                start_pos, end_pos = get_start_end_pos_entity(intent_value,\n",
    "                                                              ent['value'])\n",
    "                ent['start'] = start_pos\n",
    "                ent['end'] = end_pos\n",
    "                entity_value_new.append(ent)\n",
    "        else:\n",
    "            entity_value_new = entity_value\n",
    "        # update out dict\n",
    "        out['intent'] = intent\n",
    "        out['entities'] = entity_value_new\n",
    "        out['spacy_doc']= spacy_nlp(intent_value)\n",
    "        train_examples_crfformat.append(Message(intent_value, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TrainingData(train_examples_crfformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_crf_pos_feature_config = {'BILOU_flag': True,\n",
    " 'features': [['low', 'title', 'upper', 'pos', 'pos2'],\n",
    "  ['bias',\n",
    "   'low',\n",
    "   'upper',\n",
    "   'title',\n",
    "   'digit',\n",
    "   'pos',\n",
    "   'pos2',\n",
    "   'pattern'],\n",
    "  ['low', 'title', 'upper', 'pos', 'pos2']],\n",
    " 'max_iterations': 50,\n",
    " 'L1_c': 0.1,\n",
    " 'L2_c': 0.1,\n",
    " 'name': 'ner_crf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model = CRFEntityExtractor(component_config=ner_crf_pos_feature_config)\n",
    "model.train(training_data, RasaNLUModelConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'crf_model.pkl'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "model.persist(file_name='crf_model', model_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "meta = {'file': 'crf_model.pkl'}\n",
    "model1 = CRFEntityExtractor.load(meta=meta, model_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = 'hi, looking for hotels, thanks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input to crf format\n",
    "test_example = preprocess(test_example)\n",
    "doc = {\"spacy_doc\": spacy_nlp(test_example)}\n",
    "test_example_entity = Message(test_example, doc)\n",
    "crf_format = model1._from_text_to_crf(test_example_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spacy_doc': hi  looking for hotels  thanks,\n",
       " 'text': 'hi  looking for hotels  thanks'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example_entity.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 'UH', 'N/A', {}),\n",
       " (' ', '', 'N/A', {}),\n",
       " ('looking', 'VBG', 'N/A', {}),\n",
       " ('for', 'IN', 'N/A', {}),\n",
       " ('hotels', 'NNS', 'N/A', {}),\n",
       " (' ', '', 'N/A', {}),\n",
       " ('thanks', 'NNS', 'N/A', {})]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "feats = model1._sentence_to_features(crf_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 16,\n",
       "  'end': 23,\n",
       "  'value': 'chinese',\n",
       "  'entity': 'cuisine',\n",
       "  'confidence': 0.9040291061614395}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entity extraction\n",
    "model1.extract_entities(Message(test_example, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'O': 0.9935840842746052,\n",
       "  'U-location': 0.0025731677625073003,\n",
       "  'U-cuisine': 0.0017296234288615114,\n",
       "  'B-cuisine': 0.0007274616553097736,\n",
       "  'L-cuisine': 0.001385662878716232},\n",
       " {'O': 0.9898204053676593,\n",
       "  'U-location': 0.0020218403525690105,\n",
       "  'U-cuisine': 0.005896475653237937,\n",
       "  'B-cuisine': 0.0011422690125467414,\n",
       "  'L-cuisine': 0.0011190096139869765},\n",
       " {'O': 0.9924463733352759,\n",
       "  'U-location': 0.0015172228416319435,\n",
       "  'U-cuisine': 0.004342663170587768,\n",
       "  'B-cuisine': 0.0008372696213589309,\n",
       "  'L-cuisine': 0.0008564710311452163},\n",
       " {'O': 0.9935702155957924,\n",
       "  'U-location': 0.0027424378654503333,\n",
       "  'U-cuisine': 0.0011966517269220759,\n",
       "  'B-cuisine': 0.0019181793111712714,\n",
       "  'L-cuisine': 0.0005725155006636245},\n",
       " {'O': 0.09037864030526141,\n",
       "  'U-location': 0.0055922535332987265,\n",
       "  'U-cuisine': 0.8896389573535589,\n",
       "  'B-cuisine': 0.003087143981098501,\n",
       "  'L-cuisine': 0.01130300482678207},\n",
       " {'O': 0.9953961755882361,\n",
       "  'U-location': 0.001174507786454426,\n",
       "  'U-cuisine': 0.0013689711793611113,\n",
       "  'B-cuisine': 0.0008078289015946273,\n",
       "  'L-cuisine': 0.0012525165443536225},\n",
       " {'O': 0.9639165296386746,\n",
       "  'U-location': 0.0030050891008910354,\n",
       "  'U-cuisine': 0.02617624564880357,\n",
       "  'B-cuisine': 0.001771130402400911,\n",
       "  'L-cuisine': 0.00513100520922974},\n",
       " {'O': 0.9876509639351566,\n",
       "  'U-location': 0.00504102754037118,\n",
       "  'U-cuisine': 0.0031303883542120677,\n",
       "  'B-cuisine': 0.0025754421218437767,\n",
       "  'L-cuisine': 0.0016021780484161987}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.ent_tagger.predict_marginals_single(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = model.filter_trainable_entities(train_examples_crfformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'value': 'west', 'entity': 'location', 'start': 16, 'end': 20}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered[0].get('entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy NER Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 852.3MB 707kB/s ta 0:00:0112    20% |██████▊                         | 178.9MB 7.9MB/s eta 0:01:25    33% |██████████▋                     | 282.5MB 1.3MB/s eta 0:07:32    40% |█████████████                   | 346.8MB 7.2MB/s eta 0:01:11    48% |███████████████▍                | 410.8MB 710kB/s eta 0:10:22    55% |█████████████████▉              | 475.3MB 3.0MB/s eta 0:02:07    92% |█████████████████████████████▊  | 791.1MB 607kB/s eta 0:01:41\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "  Running setup.py install for en-core-web-lg ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-lg-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.6/site-packages/en_core_web_lg -->\n",
      "    /anaconda3/lib/python3.6/site-packages/spacy/data/en_core_web_lg\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spacy_nlp = spacy.load(\"en_core_web_lg\")\n",
    "except:\n",
    "    !python -m spacy download en_core_web_lg\n",
    "    spacy_nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_nlp(\"looking for indian restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian\n",
      "12\n",
      "18\n",
      "NORP\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc.ents:\n",
    "    print(token.text)\n",
    "    print(token.start_char)\n",
    "    print(token.end_char)\n",
    "    print(token.label_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
