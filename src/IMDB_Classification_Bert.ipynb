{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/varunn/.pytorch_pretrained_bert')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_csv(cls, input_file, size=2000, header=None):\n",
    "        \"\"\"Reads a csv file.\"\"\"\n",
    "        df = pd.read_csv(input_file, header=header)\n",
    "        labels = df[0].tolist()\n",
    "        text = df[1].tolist()\n",
    "        out = []\n",
    "        for idx, line in enumerate(labels):\n",
    "            out_line = []\n",
    "            out_line.append(line)\n",
    "            out_line.append(text[idx])\n",
    "            out.append(out_line)\n",
    "        return out[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = DataProcessor._read_csv('data/imdb_clas/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "labels = [int(x[0]) for x in train_lines]\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor(DataProcessor):\n",
    "    \"\"\"Processor for the custom data set\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.csv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for train and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1]\n",
    "            label = line[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 09:27:29 - INFO - __main__ -   LOOKING AT data/imdb_clas/train.csv\n"
     ]
    }
   ],
   "source": [
    "data = Processor().get_train_examples('data/imdb_clas/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-0\n",
      "Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n",
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data[0].guid)\n",
    "print(data[0].text_a)\n",
    "print(data[0].text_b)\n",
    "print(data[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[str(example.label)]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/imdb_clas/'\n",
    "task_name = 'mrpc'\n",
    "bert_model = 'bert-base-uncased'\n",
    "output_dir = 'data/imdb_Bert_Predictions'\n",
    "max_seq_length = 128\n",
    "do_train = True\n",
    "do_eval = True\n",
    "do_lower_case = True\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 3.0\n",
    "warmup_proportion = 0.1\n",
    "no_cuda = True\n",
    "local_rank = -1\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 1\n",
    "fp16 = False\n",
    "loss_scale = 0\n",
    "\n",
    "processors = {\n",
    "        \"mrpc\": Processor,\n",
    "    }\n",
    "\n",
    "num_labels_task = {\n",
    "    \"cola\": 2,\n",
    "    \"mnli\": 3,\n",
    "    \"mrpc\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 09:56:54 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "if local_rank == -1 or no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "    device, n_gpu, bool(local_rank != -1), fp16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gradient_accumulation_steps < 1:\n",
    "    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                     gradient_accumulation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = int(train_batch_size / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if not do_train and not do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "task_name = task_name.lower()\n",
    "\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "processor = processors[task_name]()\n",
    "num_labels = num_labels_task[task_name]\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, ['0', '1'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 09:58:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/varunn/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 09:59:19 - INFO - __main__ -   LOOKING AT data/imdb_clas/train.csv\n"
     ]
    }
   ],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if do_train:\n",
    "    train_examples = processor.get_train_examples(data_dir)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 09:59:50 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/varunn/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/03/2019 09:59:50 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/varunn/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_y/2dty3nzx05zdf0lpd_r9bj1jbr9qbr/T/tmptiw2hoty\n",
      "02/03/2019 09:59:54 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/03/2019 09:59:57 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "02/03/2019 09:59:57 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model,\n",
    "          cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n",
    "          num_labels = num_labels)\n",
    "if fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "if local_rank != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    model = DDP(model)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "if local_rank != -1:\n",
    "    t_total = t_total // torch.distributed.get_world_size()\n",
    "if fp16:\n",
    "    try:\n",
    "        from apex.optimizers import FP16_Optimizer\n",
    "        from apex.optimizers import FusedAdam\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                          lr=learning_rate,\n",
    "                          bias_correction=False,\n",
    "                          max_grad_norm=1.0)\n",
    "    if loss_scale == 0:\n",
    "        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "    else:\n",
    "        optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n",
    "\n",
    "else:\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=learning_rate,\n",
    "                         warmup=warmup_proportion,\n",
    "                         t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertAdam (\n",
       "Parameter Group 0\n",
       "    b1: 0.9\n",
       "    b2: 0.999\n",
       "    e: 1e-06\n",
       "    lr: 5e-05\n",
       "    max_grad_norm: 1.0\n",
       "    schedule: warmup_linear\n",
       "    t_total: 187\n",
       "    warmup: 0.1\n",
       "    weight_decay: 0.01\n",
       "    weight_decay_rate: 0.01\n",
       "\n",
       "Parameter Group 1\n",
       "    b1: 0.9\n",
       "    b2: 0.999\n",
       "    e: 1e-06\n",
       "    lr: 5e-05\n",
       "    max_grad_norm: 1.0\n",
       "    schedule: warmup_linear\n",
       "    t_total: 187\n",
       "    warmup: 0.1\n",
       "    weight_decay: 0.0\n",
       "    weight_decay_rate: 0.01\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_examples[0]\n",
    "label_map = {label : i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'0': 0, '1': 1}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(a.label, label_map)\n",
    "print(label_map[str(a.label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 10:09:49 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   guid: train-0\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   tokens: [CLS] un - b ##lee ##ping - bel ##ie ##vable ! meg ryan doesn ' t even look her usual per ##t lo ##vable self in this , which normally makes me forgive her shallow tick ##y acting sc ##ht ##ick . hard to believe she was the producer on this dog . plus kevin k ##line : what kind of suicide trip has his career been on ? who ##osh . . . ban ##zai ! ! ! finally this was directed by the guy who did big chill ? must be a replay of jones ##town - hollywood style . woo ##off ##f ! [SEP]\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_ids: 101 4895 1011 1038 10559 4691 1011 19337 2666 12423 999 12669 4575 2987 1005 1056 2130 2298 2014 5156 2566 2102 8840 12423 2969 1999 2023 1010 2029 5373 3084 2033 9641 2014 8467 16356 2100 3772 8040 11039 6799 1012 2524 2000 2903 2016 2001 1996 3135 2006 2023 3899 1012 4606 4901 1047 4179 1024 2054 2785 1997 5920 4440 2038 2010 2476 2042 2006 1029 2040 17369 1012 1012 1012 7221 25290 999 999 999 2633 2023 2001 2856 2011 1996 3124 2040 2106 2502 10720 1029 2442 2022 1037 15712 1997 3557 4665 1011 5365 2806 1012 15854 7245 2546 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   label: 0 (id = 0)\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   guid: train-1\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   tokens: [CLS] this is a extremely well - made film . the acting , script and camera - work are all first - rate . the music is good , too , though it is mostly early in the film , when things are still relatively cheer ##y . there are no really superstar ##s in the cast , though several faces will be familiar . the entire cast does an excellent job with the script . < br / > < br / > but it is hard to watch , because there is no good end to a situation like the one presented . it is now fashionable to blame the british for setting hindus and muslims against each other , and then cruel ##ly separating [SEP]\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_ids: 101 2023 2003 1037 5186 2092 1011 2081 2143 1012 1996 3772 1010 5896 1998 4950 1011 2147 2024 2035 2034 1011 3446 1012 1996 2189 2003 2204 1010 2205 1010 2295 2009 2003 3262 2220 1999 1996 2143 1010 2043 2477 2024 2145 4659 15138 2100 1012 2045 2024 2053 2428 18795 2015 1999 1996 3459 1010 2295 2195 5344 2097 2022 5220 1012 1996 2972 3459 2515 2019 6581 3105 2007 1996 5896 1012 1026 7987 1013 1028 1026 7987 1013 1028 2021 2009 2003 2524 2000 3422 1010 2138 2045 2003 2053 2204 2203 2000 1037 3663 2066 1996 2028 3591 1012 2009 2003 2085 19964 2000 7499 1996 2329 2005 4292 18221 1998 7486 2114 2169 2060 1010 1998 2059 10311 2135 14443 102\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   label: 1 (id = 1)\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   guid: train-2\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   tokens: [CLS] every once in a long while a movie will come along that will be so awful that i feel compelled to warn people . if i labor all my days and i can save but one soul from watching this movie , how great will be my joy . < br / > < br / > where to begin my discussion of pain . for starters , there was a musical mont ##age every five minutes . there was no character development . every character was a stereo ##type . we had swearing guy , fat guy who eats don ##uts , goofy foreign guy , etc . the script felt as if it were being written as the movie was being shot . the [SEP]\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_ids: 101 2296 2320 1999 1037 2146 2096 1037 3185 2097 2272 2247 2008 2097 2022 2061 9643 2008 1045 2514 15055 2000 11582 2111 1012 2065 1045 4450 2035 2026 2420 1998 1045 2064 3828 2021 2028 3969 2013 3666 2023 3185 1010 2129 2307 2097 2022 2026 6569 1012 1026 7987 1013 1028 1026 7987 1013 1028 2073 2000 4088 2026 6594 1997 3255 1012 2005 29400 1010 2045 2001 1037 3315 18318 4270 2296 2274 2781 1012 2045 2001 2053 2839 2458 1012 2296 2839 2001 1037 12991 13874 1012 2057 2018 25082 3124 1010 6638 3124 2040 20323 2123 16446 1010 27243 3097 3124 1010 4385 1012 1996 5896 2371 2004 2065 2009 2020 2108 2517 2004 1996 3185 2001 2108 2915 1012 1996 102\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   label: 0 (id = 0)\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   guid: train-3\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   tokens: [CLS] name just says it all . i watched this movie with my dad when it came out and having served in korea he had great admiration for the man . the disappointing thing about this film is that it only concentrate on a short period of the man ' s life - interesting ##ly enough the man ' s entire life would have made such an epic bio - pic that it is staggering to imagine the cost for production . < br / > < br / > some posters el ##ude to the flawed characteristics about the man , which are cheap shots . the theme of the movie \" duty , honor , country \" are not just mere words b ##lat ##hered [SEP]\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_ids: 101 2171 2074 2758 2009 2035 1012 1045 3427 2023 3185 2007 2026 3611 2043 2009 2234 2041 1998 2383 2366 1999 4420 2002 2018 2307 17005 2005 1996 2158 1012 1996 15640 2518 2055 2023 2143 2003 2008 2009 2069 10152 2006 1037 2460 2558 1997 1996 2158 1005 1055 2166 1011 5875 2135 2438 1996 2158 1005 1055 2972 2166 2052 2031 2081 2107 2019 8680 16012 1011 27263 2008 2009 2003 26233 2000 5674 1996 3465 2005 2537 1012 1026 7987 1013 1028 1026 7987 1013 1028 2070 14921 3449 12672 2000 1996 25077 6459 2055 1996 2158 1010 2029 2024 10036 7171 1012 1996 4323 1997 1996 3185 1000 4611 1010 3932 1010 2406 1000 2024 2025 2074 8210 2616 1038 20051 27190 102\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 10:09:49 - INFO - __main__ -   label: 1 (id = 1)\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   guid: train-4\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   tokens: [CLS] this movie succeeds at being one of the most unique movies you ' ve seen . however this comes from the fact that you can ' t make heads or tails of this mess . it almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid . if you don ' t want to feel slight ##ed you ' ll sit through this horrible film and develop a real sense of pity for the actors involved , they ' ve all seen better days , but then you realize they actually got paid quite a bit of money to do this and you ' ll [SEP]\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_ids: 101 2023 3185 21645 2012 2108 2028 1997 1996 2087 4310 5691 2017 1005 2310 2464 1012 2174 2023 3310 2013 1996 2755 2008 2017 2064 1005 1056 2191 4641 2030 17448 1997 2023 6752 1012 2009 2471 3849 2004 1037 2186 1997 7860 2275 2039 2000 5646 3251 2030 2025 2017 2024 5627 2000 3328 2041 1997 1996 3185 1998 2507 2039 1996 2769 2017 2074 3825 1012 2065 2017 2123 1005 1056 2215 2000 2514 7263 2098 2017 1005 2222 4133 2083 2023 9202 2143 1998 4503 1037 2613 3168 1997 12063 2005 1996 5889 2920 1010 2027 1005 2310 2035 2464 2488 2420 1010 2021 2059 2017 5382 2027 2941 2288 3825 3243 1037 2978 1997 2769 2000 2079 2023 1998 2017 1005 2222 102\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 10:09:49 - INFO - __main__ -   label: 0 (id = 0)\n",
      "02/03/2019 10:09:59 - INFO - __main__ -   ***** Running training *****\n",
      "02/03/2019 10:09:59 - INFO - __main__ -     Num examples = 2000\n",
      "02/03/2019 10:09:59 - INFO - __main__ -     Batch size = 32\n",
      "02/03/2019 10:09:59 - INFO - __main__ -     Num steps = 187\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 1/63 [00:32<33:12, 32.13s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 2/63 [01:10<34:29, 33.93s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 3/63 [01:40<32:40, 32.68s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 4/63 [02:08<30:53, 31.42s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 5/63 [02:36<29:17, 30.30s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 6/63 [03:03<28:00, 29.49s/it]\u001b[A\n",
      "Iteration:  11%|█         | 7/63 [03:31<27:00, 28.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 8/63 [03:59<26:11, 28.57s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 9/63 [04:26<25:27, 28.30s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 10/63 [04:54<24:52, 28.17s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 11/63 [05:22<24:20, 28.08s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 12/63 [05:50<23:47, 27.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 13/63 [06:25<25:14, 30.28s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 14/63 [06:57<25:02, 30.67s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 15/63 [07:27<24:15, 30.32s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 16/63 [07:56<23:34, 30.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 17/63 [08:28<23:34, 30.76s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 18/63 [09:00<23:09, 30.87s/it]\u001b[A\n",
      "Iteration:  30%|███       | 19/63 [09:32<23:00, 31.38s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 20/63 [10:04<22:33, 31.48s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 21/63 [10:35<22:03, 31.52s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 22/63 [11:05<21:08, 30.95s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 23/63 [11:34<20:13, 30.33s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 24/63 [12:02<19:21, 29.78s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 25/63 [12:31<18:43, 29.56s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 26/63 [13:03<18:32, 30.07s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 27/63 [13:34<18:11, 30.33s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 28/63 [14:04<17:39, 30.26s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 29/63 [14:34<17:08, 30.24s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 30/63 [15:06<16:58, 30.86s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 31/63 [15:38<16:37, 31.18s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 32/63 [16:10<16:10, 31.30s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 33/63 [16:42<15:43, 31.46s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 34/63 [17:13<15:14, 31.53s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 35/63 [17:46<14:51, 31.84s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 36/63 [18:18<14:20, 31.86s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 37/63 [18:49<13:43, 31.67s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 38/63 [19:19<12:57, 31.09s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 39/63 [19:50<12:25, 31.08s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 40/63 [20:21<11:52, 30.98s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 41/63 [20:52<11:23, 31.06s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 42/63 [21:23<10:52, 31.08s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 43/63 [21:54<10:23, 31.18s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 44/63 [22:25<09:52, 31.16s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 45/63 [22:57<09:23, 31.31s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 46/63 [23:29<08:56, 31.55s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 47/63 [24:00<08:23, 31.45s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 48/63 [24:32<07:51, 31.42s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 49/63 [25:07<07:34, 32.43s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 50/63 [25:40<07:06, 32.78s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 51/63 [26:09<06:17, 31.50s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 52/63 [26:40<05:45, 31.38s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 53/63 [27:11<05:12, 31.25s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 54/63 [27:46<04:52, 32.51s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 55/63 [28:15<04:12, 31.53s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 56/63 [28:44<03:34, 30.63s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 57/63 [40:13<22:49, 228.30s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 58/63 [40:42<14:01, 168.27s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 59/63 [41:10<08:25, 126.34s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 60/63 [41:38<04:50, 96.83s/it] \u001b[A\n",
      "Iteration:  97%|█████████▋| 61/63 [42:06<02:32, 76.20s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 62/63 [42:35<01:01, 61.95s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 63/63 [42:49<00:00, 47.70s/it]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [42:49<1:25:39, 2569.90s/it]\n",
      "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 1/63 [00:28<29:21, 28.41s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 2/63 [00:56<28:47, 28.32s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 3/63 [01:24<28:13, 28.23s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 4/63 [01:53<27:50, 28.32s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 5/63 [02:21<27:21, 28.30s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 6/63 [02:49<26:59, 28.41s/it]\u001b[A\n",
      "Iteration:  11%|█         | 7/63 [03:18<26:35, 28.49s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 8/63 [03:47<26:08, 28.52s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 9/63 [04:15<25:41, 28.54s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 10/63 [04:44<25:12, 28.55s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 11/63 [05:12<24:44, 28.55s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 12/63 [05:41<24:19, 28.62s/it]\u001b[A\n",
      "Iteration:  21%|██        | 13/63 [06:10<23:51, 28.63s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 14/63 [06:38<23:18, 28.54s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 15/63 [07:07<22:49, 28.54s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 16/63 [07:35<22:20, 28.53s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 17/63 [08:04<21:50, 28.49s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 18/63 [08:32<21:14, 28.33s/it]\u001b[A\n",
      "Iteration:  30%|███       | 19/63 [09:00<20:42, 28.23s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 20/63 [09:28<20:19, 28.35s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 21/63 [09:57<19:54, 28.44s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 22/63 [10:25<19:20, 28.30s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 23/63 [10:53<18:48, 28.21s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 24/63 [11:21<18:19, 28.18s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 25/63 [11:49<17:50, 28.16s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 26/63 [12:17<17:23, 28.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  43%|████▎     | 27/63 [12:46<16:59, 28.32s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 28/63 [13:14<16:32, 28.36s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 29/63 [13:43<16:04, 28.38s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 30/63 [14:14<16:00, 29.10s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 31/63 [14:45<15:48, 29.64s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 32/63 [15:13<15:05, 29.20s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 33/63 [15:41<14:25, 28.85s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 34/63 [16:09<13:49, 28.59s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 35/63 [16:37<13:15, 28.40s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 36/63 [17:05<12:43, 28.29s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 37/63 [17:33<12:13, 28.22s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 38/63 [18:01<11:44, 28.18s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 39/63 [18:29<11:15, 28.14s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 40/63 [18:57<10:49, 28.26s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 41/63 [19:26<10:24, 28.37s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 42/63 [19:55<09:58, 28.50s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 43/63 [20:23<09:30, 28.52s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 44/63 [20:52<09:01, 28.49s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 45/63 [21:21<08:34, 28.59s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 46/63 [21:49<08:02, 28.41s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 47/63 [22:17<07:33, 28.33s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 48/63 [22:45<07:05, 28.35s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 49/63 [23:14<06:37, 28.41s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 50/63 [23:42<06:09, 28.42s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 51/63 [24:10<05:40, 28.35s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 52/63 [24:38<05:10, 28.25s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 53/63 [33:13<29:02, 174.22s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 54/63 [33:55<20:11, 134.61s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 55/63 [34:26<13:48, 103.52s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 56/63 [34:55<09:28, 81.20s/it] \u001b[A\n",
      "Iteration:  90%|█████████ | 57/63 [35:26<06:36, 66.00s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 58/63 [36:03<04:45, 57.17s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 59/63 [36:34<03:17, 49.35s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 60/63 [37:03<02:09, 43.19s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 61/63 [37:32<01:18, 39.11s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 62/63 [38:02<00:36, 36.36s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 63/63 [38:21<00:00, 31.28s/it]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [1:21:11<41:29, 2489.52s/it]\n",
      "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 1/63 [00:32<33:43, 32.63s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 2/63 [01:02<32:19, 31.79s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 3/63 [01:31<31:02, 31.04s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 4/63 [02:01<30:10, 30.68s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 5/63 [02:30<29:13, 30.24s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 6/63 [03:00<28:27, 29.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 7/63 [03:29<27:45, 29.75s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 8/63 [03:58<27:13, 29.70s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 9/63 [04:28<26:35, 29.54s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 10/63 [04:57<26:08, 29.60s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 11/63 [05:27<25:44, 29.70s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 12/63 [05:56<24:58, 29.39s/it]\u001b[A\n",
      "Iteration:  21%|██        | 13/63 [06:24<24:11, 29.03s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 14/63 [06:53<23:38, 28.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 15/63 [07:22<23:06, 28.88s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 16/63 [07:50<22:27, 28.68s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 17/63 [08:19<21:59, 28.69s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 18/63 [08:47<21:24, 28.54s/it]\u001b[A\n",
      "Iteration:  30%|███       | 19/63 [09:15<20:51, 28.43s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 20/63 [09:43<20:19, 28.36s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 21/63 [10:12<19:56, 28.48s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 22/63 [10:41<19:32, 28.60s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 23/63 [17:37<1:36:39, 145.00s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 24/63 [18:10<1:12:16, 111.19s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 25/63 [18:39<54:52, 86.64s/it]   \u001b[A\n",
      "Iteration:  41%|████▏     | 26/63 [19:09<42:58, 69.68s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 27/63 [19:40<34:45, 57.93s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 28/63 [20:12<29:21, 50.33s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 29/63 [20:49<26:13, 46.27s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 30/63 [21:23<23:26, 42.61s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 31/63 [21:57<21:22, 40.06s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 32/63 [22:30<19:38, 38.01s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 33/63 [23:05<18:29, 36.99s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 34/63 [23:35<16:55, 35.02s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 35/63 [24:07<15:49, 33.92s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 36/63 [24:40<15:07, 33.61s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 37/63 [25:09<13:57, 32.23s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 38/63 [25:39<13:09, 31.56s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 39/63 [26:08<12:21, 30.92s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 40/63 [26:37<11:37, 30.31s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 41/63 [27:06<10:59, 29.99s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 42/63 [27:36<10:30, 30.04s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 43/63 [28:06<09:57, 29.86s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 44/63 [28:35<09:21, 29.56s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 45/63 [29:04<08:49, 29.39s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 46/63 [29:33<08:17, 29.27s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 47/63 [30:02<07:48, 29.31s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 48/63 [30:31<07:18, 29.25s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 49/63 [30:59<06:44, 28.91s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 50/63 [31:27<06:12, 28.68s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 51/63 [31:59<05:55, 29.59s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 52/63 [32:32<05:35, 30.53s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 53/63 [33:01<05:00, 30.03s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 54/63 [33:30<04:26, 29.65s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 55/63 [33:58<03:54, 29.27s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 56/63 [34:27<03:24, 29.15s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 57/63 [34:59<03:01, 30.20s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 58/63 [35:29<02:30, 30.01s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 59/63 [35:58<01:58, 29.68s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 60/63 [36:27<01:28, 29.42s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 61/63 [36:56<00:58, 29.35s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 62/63 [37:30<00:30, 30.63s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 63/63 [37:45<00:00, 26.12s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [1:58:57<00:00, 2422.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# training begins\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "if do_train:\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if local_rank == -1:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = DistributedSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            if fp16:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.010441914200783, 189)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_loss, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "if do_train:\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 12:09:56 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/varunn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/03/2019 12:09:56 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/varunn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_y/2dty3nzx05zdf0lpd_r9bj1jbr9qbr/T/tmp0s8ohwrd\n",
      "02/03/2019 12:10:00 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/03/2019 12:10:03 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "02/03/2019 12:10:03 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file, map_location=device)\n",
    "model1 = BertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
    "model1.load_state_dict(model_state_dict)\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 12:12:36 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   guid: test-0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   tokens: [CLS] bud abbott and lou costello always had a good following among children , but in their careers i think you could say that they only made one film that could be designated for kids . jack and the beans ##talk was that one film . < br / > < br / > it was part of a two picture independent deal from warner brothers , the second film being abbott and costello meet captain kidd . these were the only two films the boys made in color . < br / > < br / > the two of them , out of work as usual , take a job for a very pre ##co ##cious and ob ##no ##xious young david st ##oll ##ery [SEP]\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_ids: 101 13007 14455 1998 10223 21015 2467 2018 1037 2204 2206 2426 2336 1010 2021 1999 2037 10922 1045 2228 2017 2071 2360 2008 2027 2069 2081 2028 2143 2008 2071 2022 4351 2005 4268 1012 2990 1998 1996 13435 28014 2001 2008 2028 2143 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 2001 2112 1997 1037 2048 3861 2981 3066 2013 6654 3428 1010 1996 2117 2143 2108 14455 1998 21015 3113 2952 25358 1012 2122 2020 1996 2069 2048 3152 1996 3337 2081 1999 3609 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2048 1997 2068 1010 2041 1997 2147 2004 5156 1010 2202 1037 3105 2005 1037 2200 3653 3597 18436 1998 27885 3630 25171 2402 2585 2358 14511 7301 102\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   label: 1 (id = 1)\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   guid: test-1\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   tokens: [CLS] this film is not your typical hollywood fare , though the picking ##s are so bad i often tend to stay away from movies rather than be disappointed . however , this little low - budget gem is thoroughly love ##able and enjoyable and definitely a keeper . the actors are as varied as the characters they portray , the buffalo setting is charming ( what a pretty city ) , and the story sparkle ##s . the lack of gr ##at ##uit ##ous violence , sex and the \" f \" word doesn ' t det ##rac ##t in the least ! take the kids , take grandma , take a break from hollywood ! i give it an 11 out of 10 ! [SEP]\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_ids: 101 2023 2143 2003 2025 2115 5171 5365 13258 1010 2295 1996 8130 2015 2024 2061 2919 1045 2411 7166 2000 2994 2185 2013 5691 2738 2084 2022 9364 1012 2174 1010 2023 2210 2659 1011 5166 17070 2003 12246 2293 3085 1998 22249 1998 5791 1037 10684 1012 1996 5889 2024 2004 9426 2004 1996 3494 2027 17279 1010 1996 6901 4292 2003 11951 1006 2054 1037 3492 2103 1007 1010 1998 1996 2466 26831 2015 1012 1996 3768 1997 24665 4017 14663 3560 4808 1010 3348 1998 1996 1000 1042 1000 2773 2987 1005 1056 20010 22648 2102 1999 1996 2560 999 2202 1996 4268 1010 2202 13055 1010 2202 1037 3338 2013 5365 999 1045 2507 2009 2019 2340 2041 1997 2184 999 102 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   label: 1 (id = 1)\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   guid: test-2\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   tokens: [CLS] henry thomas , and robin tun ##ny , are a couple of the most under ##rated performers in the business . it ' s beyond me as to why they haven ' t received more recognition than they have . this movie is a perfect example of how bound ##less their abilities are . < br / > < br / > acting out the lives of folks who could be referred to as a bit odd seems to be their special ##ity , and if these characters ain ' t odd , i ' m at a loss to find anyone who is . < br / > < br / > the story is funny , romantic , dramatic , complicated , and tragic [SEP]\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_ids: 101 2888 2726 1010 1998 5863 27112 4890 1010 2024 1037 3232 1997 1996 2087 2104 9250 9567 1999 1996 2449 1012 2009 1005 1055 3458 2033 2004 2000 2339 2027 4033 1005 1056 2363 2062 5038 2084 2027 2031 1012 2023 3185 2003 1037 3819 2742 1997 2129 5391 3238 2037 7590 2024 1012 1026 7987 1013 1028 1026 7987 1013 1028 3772 2041 1996 3268 1997 12455 2040 2071 2022 3615 2000 2004 1037 2978 5976 3849 2000 2022 2037 2569 3012 1010 1998 2065 2122 3494 7110 1005 1056 5976 1010 1045 1005 1049 2012 1037 3279 2000 2424 3087 2040 2003 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2466 2003 6057 1010 6298 1010 6918 1010 8552 1010 1998 13800 102\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   label: 1 (id = 1)\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   guid: test-3\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   tokens: [CLS] this digital horror film brings us into the micro - budget film genre where the more blood and chicks in distress the better . the story is weak , the acting is respectable and the special effects , well , they ' re special alright . a quality horror film for the fans that already know what to expect . [SEP]\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_ids: 101 2023 3617 5469 2143 7545 2149 2046 1996 12702 1011 5166 2143 6907 2073 1996 2062 2668 1998 20649 1999 12893 1996 2488 1012 1996 2466 2003 5410 1010 1996 3772 2003 19416 1998 1996 2569 3896 1010 2092 1010 2027 1005 2128 2569 10303 1012 1037 3737 5469 2143 2005 1996 4599 2008 2525 2113 2054 2000 5987 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   label: 0 (id = 0)\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   *** Example ***\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   guid: test-4\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   tokens: [CLS] i just saw this on a local independent station in the new york city area . the cast showed promise but when i saw the director , george co ##smo ##tos , i became suspicious . and sure enough , it was every bit as bad , every bit as pointless and stupid as every george co ##smo ##tos movie i ever saw . he ' s like a stupid man ' s michael bey - - with all the awful ##ness that acc ##ola ##de promises . < br / > < br / > there ' s no point to the conspiracy , no burning issues that urge the con ##sp ##ira ##tors on . we are left to ourselves to connect the dots [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 12:12:36 - INFO - __main__ -   input_ids: 101 1045 2074 2387 2023 2006 1037 2334 2981 2276 1999 1996 2047 2259 2103 2181 1012 1996 3459 3662 4872 2021 2043 1045 2387 1996 2472 1010 2577 2522 25855 13122 1010 1045 2150 10027 1012 1998 2469 2438 1010 2009 2001 2296 2978 2004 2919 1010 2296 2978 2004 23100 1998 5236 2004 2296 2577 2522 25855 13122 3185 1045 2412 2387 1012 2002 1005 1055 2066 1037 5236 2158 1005 1055 2745 20289 1011 1011 2007 2035 1996 9643 2791 2008 16222 6030 3207 10659 1012 1026 7987 1013 1028 1026 7987 1013 1028 2045 1005 1055 2053 2391 2000 1996 9714 1010 2053 5255 3314 2008 9075 1996 9530 13102 7895 6591 2006 1012 2057 2024 2187 2000 9731 2000 7532 1996 14981 102\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/03/2019 12:12:36 - INFO - __main__ -   label: 0 (id = 0)\n",
      "02/03/2019 12:12:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "02/03/2019 12:12:45 - INFO - __main__ -     Num examples = 2000\n",
      "02/03/2019 12:12:45 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100%|██████████| 250/250 [16:42<00:00,  3.98s/it]\n",
      "02/03/2019 12:29:27 - INFO - __main__ -   ***** Eval results *****\n",
      "02/03/2019 12:29:27 - INFO - __main__ -     eval_accuracy = 0.8245\n",
      "02/03/2019 12:29:27 - INFO - __main__ -     eval_loss = 0.4050906317234039\n",
      "02/03/2019 12:29:27 - INFO - __main__ -     global_step = 189\n",
      "02/03/2019 12:29:27 - INFO - __main__ -     loss = 0.25413399863810765\n"
     ]
    }
   ],
   "source": [
    "if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    eval_examples = processor.get_dev_examples(data_dir)\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss/nb_tr_steps if do_train else None\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "              'global_step': global_step,\n",
    "              'loss': loss}\n",
    "\n",
    "    output_eval_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4050906317234039,\n",
       " 'eval_accuracy': 0.8245,\n",
       " 'global_step': 189,\n",
       " 'loss': 0.25413399863810765}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
