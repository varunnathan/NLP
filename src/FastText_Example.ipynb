{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of Fasttext and Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloads\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = \"C:/Users/lenovo/Documents/\"\n",
    "data_path = local_data_root + 'data/'\n",
    "code_path = local_data_root + 'NLP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not os.path.isfile(data_path+'brown_corp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate brown corpus text file\n",
    "import os\n",
    "if not os.path.isfile(data_path+'brown_corp.txt'):\n",
    "    with open(data_path+'brown_corp.txt', 'w+') as f:\n",
    "        for word in nltk.corpus.brown.words():\n",
    "            f.write('{word}'.format(word=word))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4856\n",
      "drwxr-xr-x 1 lenovo 197121       0 May 28 12:06 .\n",
      "drwxr-xr-x 1 lenovo 197121       0 May 28 12:06 ..\n",
      "-rw-r--r-- 1 lenovo 197121 4965882 May 28 12:05 brown_corp.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(data_path)\n",
    "os.chdir(data_path)\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile(data_path+'text8.zip')) & (not os.path.isfile(data_path+'text8.txt')):\n",
    "    !curl http://mattmahoney.net/dc/text8.zip\n",
    "    !unzip text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot run this\n",
    "if (not os.path.isfile(data_path+'text9.zip')) & (not os.path.isfile(data_path+'text9.txt')):\n",
    "    !curl http://mattmahoney.net/dc/enwik9.zip\n",
    "    !unzip enwik9.zip\n",
    "    !perl {FT_HOME}wikifil.pl enwik9 > text9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = data_path+'models/'\n",
    "if not os.path.isdir(MODELS_DIR):\n",
    "    os.chdir(data_path)\n",
    "    !mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "C:/Users/lenovo/Documents/data/models/\n"
     ]
    }
   ],
   "source": [
    "print(os.path.isdir(MODELS_DIR))\n",
    "print(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec on C:/Users/lenovo/Documents/data/brown_corp.txt corpus..\n",
      "Wall time: 33.4 s\n",
      "\n",
      "Saved gensim model as brown_gs.vec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "dim = 100\n",
    "ws = 5\n",
    "epoch = 5\n",
    "minCount = 1\n",
    "neg = 5\n",
    "loss = 'ns'\n",
    "t = 1e-4\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "# Same values as used for fastText training above\n",
    "params = {\n",
    "    'alpha': lr,\n",
    "    'size': dim,\n",
    "    'window': ws,\n",
    "    'iter': epoch,\n",
    "    'min_count': minCount,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 0,\n",
    "    'negative': neg\n",
    "}\n",
    "\n",
    "def train_models(corpus_file, output_name):\n",
    "        \n",
    "    output_file = '{:s}_gs'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n",
    "        print('\\nTraining word2vec on {:s} corpus..'.format(corpus_file))\n",
    "        \n",
    "        # Text8Corpus class for reading space-separated words file\n",
    "        %time gs_model = Word2Vec(Text8Corpus(corpus_file), **params); gs_model\n",
    "        # Direct local variable lookup doesn't work properly with magic statements (%time)\n",
    "        locals()['gs_model'].wv.save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n",
    "        print('\\nSaved gensim model as {:s}.vec'.format(output_file))\n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n",
    "\n",
    "evaluation_data = {}\n",
    "train_models(data_path+'brown_corp.txt', 'brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WV_DIR = local_data_root + 'word2vec/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/lenovo/Documents/word2vec/\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(WV_DIR)\n",
    "print(os.path.isdir(WV_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 13:34:16,066 : INFO : loading projection weights from C:/Users/lenovo/Documents/data/models/brown_gs.vec\n",
      "2018-05-28 13:34:16,159 : INFO : loaded (1, 100) matrix from C:/Users/lenovo/Documents/data/models/brown_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Gensim embeddings\n",
      "Accuracy for Word2Vec:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-1ca0a5ca86df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mbrown_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODELS_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'brown_gs.vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for Word2Vec:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprint_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrown_gs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_analogies_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-1ca0a5ca86df>\u001b[0m in \u001b[0;36mprint_accuracy\u001b[1;34m(model, questions_file)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msem_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'correct'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msem_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'correct'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'incorrect'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0msem_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msem_correct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msem_total\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msem_correct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msem_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msem_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import gensim.models as gsm\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Training times in seconds\n",
    "evaluation_data['brown'] = [(33.4)]\n",
    "\n",
    "def print_accuracy(model, questions_file):\n",
    "    print('Evaluating...\\n')\n",
    "    acc = model.accuracy(questions_file)\n",
    "\n",
    "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
    "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
    "    sem_acc = 100*float(sem_correct)/sem_total\n",
    "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
    "    \n",
    "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
    "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
    "    syn_acc = 100*float(syn_correct)/syn_total\n",
    "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
    "    return (sem_acc, syn_acc)\n",
    "\n",
    "word_analogies_file = WV_DIR+'questions-words.txt'\n",
    "accuracies = []\n",
    "print('\\nLoading Gensim embeddings')\n",
    "brown_gs = gsm.KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_gs.vec')\n",
    "print('Accuracy for Word2Vec:')\n",
    "accuracies.append(print_accuracy(brown_gs, word_analogies_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as gsm\n",
    "model = gsm.KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_gs.vec')\n",
    "questions_file = WV_DIR+'questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method accuracy in module gensim.models.keyedvectors:\n",
      "\n",
      "accuracy(questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x0000000BAADD7400>, case_insensitive=True) method of gensim.models.keyedvectors.Word2VecKeyedVectors instance\n",
      "    Compute accuracy of the model. `questions` is a filename where lines are\n",
      "    4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      "    See questions-words.txt in\n",
      "    https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip\n",
      "    for an example.\n",
      "    \n",
      "    The accuracy is reported (=printed to log and returned as a list) for each\n",
      "    section separately, plus there's one aggregate summary at the end.\n",
      "    \n",
      "    Use `restrict_vocab` to ignore all questions containing a word not in the first `restrict_vocab`\n",
      "    words (default 30,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      "    In case `case_insensitive` is True, the first `restrict_vocab` words are taken first, and then\n",
      "    case normalization is performed.\n",
      "    \n",
      "    Use `case_insensitive` to convert all words in questions and vocab to their uppercase form before\n",
      "    evaluating the accuracy (default True). Useful in case of case-mismatch between training tokens\n",
      "    and question words. In case of multiple case variants of a single word, the vector for the first\n",
      "    occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      "    \n",
      "    This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.accuracy(questions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'capital-world'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'currency'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'city-in-state'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'family'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram3-comparative'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram4-superlative'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram5-present-participle'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram6-nationality-adjective'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram7-past-tense'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram8-plural'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'total'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
