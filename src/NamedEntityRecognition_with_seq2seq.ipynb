{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "LOCAL_DATA_ROOT = '/Users/varunn/Documents/kaggle/'\n",
    "INP_PATH = LOCAL_DATA_ROOT + 'entity-annotated-corpus/'\n",
    "INP_FN = INP_PATH + 'ner_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = pd.read_csv(INP_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 4)\n",
      "    Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "print ner_df.shape\n",
    "print ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O        887908\n",
      "B-geo     37644\n",
      "B-tim     20333\n",
      "B-org     20143\n",
      "I-per     17251\n",
      "B-per     16990\n",
      "I-org     16784\n",
      "B-gpe     15870\n",
      "I-geo      7414\n",
      "I-tim      6528\n",
      "B-art       402\n",
      "B-eve       308\n",
      "I-art       297\n",
      "I-eve       253\n",
      "B-nat       201\n",
      "I-gpe       198\n",
      "I-nat        51\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print ner_df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47959\n",
      "<type 'str'>\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print ner_df['Sentence #'].nunique()\n",
    "sent = ner_df.loc[0, 'Sentence #']\n",
    "print type(sent)\n",
    "print isinstance(sent, basestring)\n",
    "print 'Sentence:' in sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_words = []\n",
    "sentences_tags = []\n",
    "curr_sent_num = -1\n",
    "current_sentence_words = []\n",
    "current_sentence_tags = []\n",
    "for sent_num, word, tag in ner_df[['Sentence #', 'Word', 'Tag']].values:\n",
    "    if isinstance(sent_num, basestring) and 'Sentence: ' in sent_num:\n",
    "        curr_sent_num = int(sent_num.split(':')[1].strip())\n",
    "        \n",
    "        if current_sentence_words and current_sentence_tags:\n",
    "            sentences_words.append(current_sentence_words)\n",
    "            sentences_tags.append(current_sentence_tags)\n",
    "            \n",
    "        current_sentence_words = []\n",
    "        current_sentence_tags = []\n",
    "        \n",
    "    current_sentence_words.append(word.decode(errors='replace'))\n",
    "    current_sentence_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47958\n",
      "47958\n"
     ]
    }
   ],
   "source": [
    "print len(sentences_tags)\n",
    "print len(sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Thousands',\n",
       " u'of',\n",
       " u'demonstrators',\n",
       " u'have',\n",
       " u'marched',\n",
       " u'through',\n",
       " u'London',\n",
       " u'to',\n",
       " u'protest',\n",
       " u'the',\n",
       " u'war',\n",
       " u'in',\n",
       " u'Iraq',\n",
       " u'and',\n",
       " u'demand',\n",
       " u'the',\n",
       " u'withdrawal',\n",
       " u'of',\n",
       " u'British',\n",
       " u'troops',\n",
       " u'from',\n",
       " u'that',\n",
       " u'country',\n",
       " u'.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 38366 38366\n",
      "Test: 9592 9592\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(sentences_words) * 0.8)\n",
    "\n",
    "train_sentences_words = sentences_words[:train_size]\n",
    "train_sentences_tags = sentences_tags[:train_size]\n",
    "test_sentences_words = sentences_words[train_size:]\n",
    "test_sentences_tags = sentences_tags[train_size:]\n",
    "\n",
    "print 'Train:', len(train_sentences_words), len(train_sentences_tags)\n",
    "print 'Test:', len(test_sentences_words), len(test_sentences_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer().fit(map(\n",
    "     lambda s: ' '.join(s), train_sentences_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows = count_vectorizer.transform(map(lambda s: ' '.join(s), train_sentences_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38366, 25446)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands\n",
      "(1, 25446)\n",
      "of\n",
      "(1, 25446)\n",
      "demonstrators\n",
      "(1, 25446)\n",
      "have\n",
      "(1, 25446)\n",
      "marched\n",
      "(1, 25446)\n",
      "through\n",
      "(1, 25446)\n",
      "London\n",
      "(1, 25446)\n",
      "to\n",
      "(1, 25446)\n",
      "protest\n",
      "(1, 25446)\n",
      "the\n",
      "(1, 25446)\n",
      "war\n",
      "(1, 25446)\n",
      "in\n",
      "(1, 25446)\n",
      "Iraq\n",
      "(1, 25446)\n",
      "and\n",
      "(1, 25446)\n",
      "demand\n",
      "(1, 25446)\n",
      "the\n",
      "(1, 25446)\n",
      "withdrawal\n",
      "(1, 25446)\n",
      "of\n",
      "(1, 25446)\n",
      "British\n",
      "(1, 25446)\n",
      "troops\n",
      "(1, 25446)\n",
      "from\n",
      "(1, 25446)\n",
      "that\n",
      "(1, 25446)\n",
      "country\n",
      "(1, 25446)\n",
      ".\n",
      "(1, 25446)\n"
     ]
    }
   ],
   "source": [
    "tmp_words = train_sentences_words[0]\n",
    "tmp_tags = train_sentences_tags[0]\n",
    "tmp_bows = bows[0]\n",
    "X = []\n",
    "y = []\n",
    "for w, t in zip(tmp_words, tmp_tags):\n",
    "    print w\n",
    "    v = count_vectorizer.transform([w])[0]\n",
    "    print v.shape\n",
    "    v = scipy.sparse.hstack([v, tmp_bows])\n",
    "    X.append(v)\n",
    "    y.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 50892)\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print scipy.sparse.vstack(X).shape\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_instances(words, tags, bow, count_vectorizer):\n",
    "    X = []\n",
    "    y = []\n",
    "    for w, t in zip(words, tags):\n",
    "        v = count_vectorizer.transform([w])[0]\n",
    "        v = scipy.sparse.hstack([v, bow])\n",
    "        X.append(v)\n",
    "        y.append(t)\n",
    "        \n",
    "    return scipy.sparse.vstack(X), y\n",
    "\n",
    "def sentences_to_instances(sentences_words, sentences_tags,\n",
    "                           count_vectorizer):\n",
    "    bows = count_vectorizer.transform(map(lambda s: ' '.join(s),\n",
    "                                          sentences_words))\n",
    "    X = []\n",
    "    y = []\n",
    "    for words, tags, bow in zip(sentences_words, sentences_tags, bows):\n",
    "        sent_X, sent_y = sentence_to_instances(words, tags, bow,\n",
    "                                               count_vectorizer)\n",
    "        X.append(sent_X)\n",
    "        y += sent_y\n",
    "        \n",
    "    return scipy.sparse.vstack(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = sentences_to_instances(train_sentences_words,\n",
    "                                          train_sentences_tags,\n",
    "                                          count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(839214, 50892)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(839214,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((209353, 50892), (209353,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X, test_y = sentences_to_instances(test_sentences_words, test_sentences_tags, count_vectorizer)\n",
    "test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_arr = train_X.toarray()\n",
    "#test_X_arr = test_X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier().fit(train_X.toarray(), train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Thousands',\n",
       " u'of',\n",
       " u'demonstrators',\n",
       " u'have',\n",
       " u'marched',\n",
       " u'through',\n",
       " u'London',\n",
       " u'to',\n",
       " u'protest',\n",
       " u'the',\n",
       " u'war',\n",
       " u'in',\n",
       " u'Iraq',\n",
       " u'and',\n",
       " u'demand',\n",
       " u'the',\n",
       " u'withdrawal',\n",
       " u'of',\n",
       " u'British',\n",
       " u'troops',\n",
       " u'from',\n",
       " u'that',\n",
       " u'country',\n",
       " u'.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(itertools.chain(*[[w for w in s] for s in train_sentences_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31809"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set(itertools.chain(*[[t for t in s] for s in train_sentences_tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_len = map(lambda s: len(s), train_sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38366"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.5510e+03, 1.5670e+04, 1.4854e+04, 3.8890e+03, 3.3800e+02,\n",
       "        5.2000e+01, 1.0000e+01, 1.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([  1. ,  11.3,  21.6,  31.9,  42.2,  52.5,  62.8,  73.1,  83.4,\n",
       "         93.7, 104. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentences_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sentences_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75#max(sentenecs_lens)\n",
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters=[],\n",
    "                            oov_token='__UNKNOWN__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenizer.fit_on_texts(map(lambda s: ' '.join(s),\n",
    "                                 train_sentences_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = words_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 28824\n"
     ]
    }
   ],
   "source": [
    "word_index['__PADDING__'] = 0\n",
    "index_word = {i: w for w, i in word_index.iteritems()}\n",
    "print 'Unique Tokens: %d' % (len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = words_tokenizer.texts_to_sequences(map(\n",
    "     lambda s: \" \".join(s), train_sentences_words))\n",
    "test_sequences = words_tokenizer.texts_to_sequences(map(\n",
    "     lambda s: \" \".join(s), test_sentences_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38366\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print len(train_sequences)\n",
    "print len(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_padded = pad_sequences(sequences=train_sequences,\n",
    "                                       maxlen=MAX_LEN)\n",
    "test_sequences_padded = pad_sequences(sequences=test_sequences,\n",
    "                                      maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[257, 5, 935, 16, 1718, 234, 487, 6, 519, 1, 129, 4, 61, 8, 610, 1, 926, 5, 193, 90, 22, 14, 54, 2]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  257    5  935   16 1718\n",
      "  234  487    6  519    1  129    4   61    8  610    1  926    5  193\n",
      "   90   22   14   54    2]\n"
     ]
    }
   ],
   "source": [
    "print train_sequences[0]\n",
    "print train_sequences_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags\n",
    "tags_tokenizer = Tokenizer(num_words=len(tags), filters='', lower=False,\n",
    "                           oov_token='__UNKNOWN__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_tokenizer.fit_on_texts(map(lambda s: \" \".join(s),\n",
    "                                train_sentences_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = tags_tokenizer.word_index\n",
    "tag_index['__PADDING__'] = 0\n",
    "index_tag = {i: w for w, i in tag_index.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B-art': 11,\n",
       " 'B-eve': 12,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 8,\n",
       " 'B-nat': 16,\n",
       " 'B-org': 4,\n",
       " 'B-per': 6,\n",
       " 'B-tim': 3,\n",
       " 'I-art': 13,\n",
       " 'I-eve': 14,\n",
       " 'I-geo': 9,\n",
       " 'I-gpe': 15,\n",
       " 'I-nat': 17,\n",
       " 'I-org': 7,\n",
       " 'I-per': 5,\n",
       " 'I-tim': 10,\n",
       " 'O': 1,\n",
       " '__PADDING__': 0,\n",
       " '__UNKNOWN__': 18}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tag_index))\n",
    "tag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tag_wo_padding = dict(index_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '__PADDING__',\n",
       " 1: 'O',\n",
       " 2: 'B-geo',\n",
       " 3: 'B-tim',\n",
       " 4: 'B-org',\n",
       " 5: 'I-per',\n",
       " 6: 'B-per',\n",
       " 7: 'I-org',\n",
       " 8: 'B-gpe',\n",
       " 9: 'I-geo',\n",
       " 10: 'I-tim',\n",
       " 11: 'B-art',\n",
       " 12: 'B-eve',\n",
       " 13: 'I-art',\n",
       " 14: 'I-eve',\n",
       " 15: 'I-gpe',\n",
       " 16: 'B-nat',\n",
       " 17: 'I-nat',\n",
       " 18: '__UNKNOWN__'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_tag_wo_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tag_wo_padding[tag_index['__PADDING__']] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0',\n",
       " 1: 'O',\n",
       " 2: 'B-geo',\n",
       " 3: 'B-tim',\n",
       " 4: 'B-org',\n",
       " 5: 'I-per',\n",
       " 6: 'B-per',\n",
       " 7: 'I-org',\n",
       " 8: 'B-gpe',\n",
       " 9: 'I-geo',\n",
       " 10: 'I-tim',\n",
       " 11: 'B-art',\n",
       " 12: 'B-eve',\n",
       " 13: 'I-art',\n",
       " 14: 'I-eve',\n",
       " 15: 'I-gpe',\n",
       " 16: 'B-nat',\n",
       " 17: 'I-nat',\n",
       " 18: '__UNKNOWN__'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_tag_wo_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = tags_tokenizer.texts_to_sequences(map(lambda s: \" \".join(s),\n",
    "                                                   train_sentences_tags))\n",
    "test_tags = tags_tokenizer.texts_to_sequences(map(lambda s: \" \".join(s),\n",
    "                                                  test_sentences_tags))\n",
    "train_tags_padded = pad_sequences(sequences=train_tags, maxlen=MAX_LEN)\n",
    "test_tags_padded = pad_sequences(sequences=test_tags, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 8, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38366, 75, 1) (9592, 75, 1)\n"
     ]
    }
   ],
   "source": [
    "train_tags_padded = np.expand_dims(train_tags_padded, -1)\n",
    "test_tags_padded = np.expand_dims(test_tags_padded, -1)\n",
    "print train_tags_padded.shape, test_tags_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags_padded[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "__PADDING__ __PADDING__\n",
      "not O\n",
      "counting O\n",
      "the O\n",
      "latest O\n",
      "death O\n",
      ", O\n",
      "the O\n",
      "world B-org\n",
      "health I-org\n",
      "organization I-org\n",
      "says O\n",
      "227 O\n",
      "people O\n",
      "around O\n",
      "the O\n",
      "world O\n",
      "have O\n",
      "died O\n",
      "from O\n",
      "bird O\n",
      "flu O\n",
      "since O\n",
      "2003 B-tim\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for w, t in zip(train_sequences_padded[123], train_tags_padded[123]):\n",
    "    print index_word[w], index_tag[t[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional, Input, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=300,\n",
    "                                   input_length=MAX_LEN)\n",
    "sequence_input = Input(shape=[MAX_LEN,], dtype='int32')\n",
    "embedded_sequences = random_embedding_layer(sequence_input)\n",
    "x = Bidirectional(LSTM(units=64, return_sequences=True))(embedded_sequences)\n",
    "x = Dropout(rate=0.3)(x)\n",
    "x = Dense(units=32, activation='relu')(x)\n",
    "preds = Dense(units=len(tag_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 75, 300)           9542700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 75, 128)           186880    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75, 32)            4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 75, 19)            627       \n",
      "=================================================================\n",
      "Total params: 9,734,335\n",
      "Trainable params: 9,734,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*19 + 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38366 samples, validate on 9592 samples\n",
      "Epoch 1/10\n",
      "38366/38366 [==============================] - 277s 7ms/step - loss: 0.1247 - sparse_categorical_accuracy: 0.9703 - val_loss: 0.0429 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 2/10\n",
      "38366/38366 [==============================] - 262s 7ms/step - loss: 0.0364 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0383 - val_sparse_categorical_accuracy: 0.9885\n",
      "Epoch 3/10\n",
      "38366/38366 [==============================] - 256s 7ms/step - loss: 0.0279 - sparse_categorical_accuracy: 0.9914 - val_loss: 0.0387 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 4/10\n",
      "38366/38366 [==============================] - 255s 7ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0405 - val_sparse_categorical_accuracy: 0.9887\n",
      "Epoch 5/10\n",
      "38366/38366 [==============================] - 275s 7ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0437 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 6/10\n",
      "38366/38366 [==============================] - 254s 7ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.0483 - val_sparse_categorical_accuracy: 0.9881\n",
      "Epoch 7/10\n",
      "38366/38366 [==============================] - 254s 7ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.9879\n",
      "Epoch 8/10\n",
      "38366/38366 [==============================] - 254s 7ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9967 - val_loss: 0.0590 - val_sparse_categorical_accuracy: 0.9876\n",
      "Epoch 9/10\n",
      "38366/38366 [==============================] - 1689s 44ms/step - loss: 0.0084 - sparse_categorical_accuracy: 0.9972 - val_loss: 0.0633 - val_sparse_categorical_accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "38366/38366 [==============================] - 304s 8ms/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0688 - val_sparse_categorical_accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1244eb5d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_sequences_padded, y=train_tags_padded, batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(test_sequences_padded, test_tags_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predicted = model.predict(x=test_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(9592, 75, 19)\n"
     ]
    }
   ],
   "source": [
    "print(type(lstm_predicted))\n",
    "print(lstm_predicted.shape)\n",
    "tmp_pred = lstm_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9999940e-01 5.1133986e-07 4.3792315e-18 ... 9.1154917e-08\n",
      "  1.8091484e-16 3.9395717e-19]\n",
      " [9.9999988e-01 9.4265651e-08 7.2841990e-20 ... 9.1868273e-09\n",
      "  1.2606010e-18 1.5080721e-21]\n",
      " [1.0000000e+00 4.0962661e-08 2.8506844e-20 ... 5.3263678e-09\n",
      "  2.7265429e-19 3.4326806e-22]\n",
      " ...\n",
      " [6.8709634e-09 9.9998462e-01 1.4413334e-10 ... 5.1311055e-10\n",
      "  9.4567778e-13 1.4875413e-16]\n",
      " [7.7630338e-09 9.9999654e-01 1.3339680e-09 ... 4.0580789e-11\n",
      "  2.2345340e-13 7.7700536e-17]\n",
      " [1.2818803e-07 9.9999583e-01 9.3933771e-08 ... 7.2392822e-08\n",
      "  3.0441330e-11 2.1512337e-13]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 5 1 1 1 3 1 1 3 1 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'B-per', 'I-per', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'B-per', 'I-per', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print tmp_pred\n",
    "tag_pred = np.argmax(tmp_pred, axis=1)\n",
    "print tag_pred\n",
    "tag_pred = map(index_tag_wo_padding.get, tag_pred)\n",
    "print tag_pred\n",
    "print tag_pred[-len(tmp_pred):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predicted_tags = []\n",
    "for s, s_pred in zip(test_sentences_words, lstm_predicted):\n",
    "    tags = np.argmax(s_pred, axis=1)\n",
    "    tags = map(index_tag_wo_padding.get, tags)[-len(s):]\n",
    "    lstm_predicted_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Mr.', u'Nour', u'was', u'arrested', u'in', u'January', u'and', u'spent', u'six', u'weeks', u'in', u'a', u'Cairo', u'jail', u',', u'before', u'his', u'release', u'on', u'bond', u'last', u'week', u'.']\n",
      "['B-per', 'I-per', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['B-per', 'I-per', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print test_sentences_words[0]\n",
    "print lstm_predicted_tags[0]\n",
    "print test_sentences_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "===============\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "      B-art       0.18      0.08      0.11        74\n",
      "      B-eve       0.46      0.42      0.44        43\n",
      "      B-geo       0.90      0.92      0.91      4870\n",
      "      B-gpe       0.95      0.96      0.96      2604\n",
      "      B-nat       0.65      0.62      0.63        39\n",
      "      B-org       0.76      0.73      0.75      3204\n",
      "      B-per       0.88      0.84      0.86      2672\n",
      "      B-tim       0.96      0.93      0.94      3460\n",
      "      I-art       0.13      0.10      0.11        30\n",
      "      I-eve       0.12      0.12      0.12        26\n",
      "      I-geo       0.75      0.80      0.77      1173\n",
      "      I-gpe       0.89      0.71      0.79        34\n",
      "      I-nat       0.00      0.00      0.00         9\n",
      "      I-org       0.71      0.71      0.71      1739\n",
      "      I-per       0.91      0.89      0.90      2111\n",
      "      I-tim       0.84      0.80      0.82       844\n",
      "          O       1.00      1.00      1.00      9590\n",
      "\n",
      "avg / total       0.90      0.90      0.90     32522\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/metrics/classification.py:960: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print 'LSTM'\n",
    "print '='*15\n",
    "print classification_report(test_sentences_tags, lstm_predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM: 0.8998731015733037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/utils/multiclass.py:198: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/varunn/.virtualenvs/xgb/lib/python2.7/site-packages/sklearn/metrics/classification.py:960: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print 'LSTM:', f1_score(test_sentences_tags, lstm_predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
