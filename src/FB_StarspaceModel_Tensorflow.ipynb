{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import TrainingData, Message\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Optional, Text, Tuple\n",
    "\n",
    "from rasa_nlu.classifiers import INTENT_RANKING_LENGTH\n",
    "from rasa_nlu.components import Component\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "LANGUAGE = 'en'\n",
    "spacy_nlp = spacy.load(LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [Message(\"anywhere in the west\", {\n",
    "                    \"intent\": \"restaurant_search\",\n",
    "                    \"entities\": [{\"start\": 16, \"end\": 20,\n",
    "                                  \"value\": \"west\", \"entity\": \"location\"}],\n",
    "                    \"spacy_doc\": spacy_nlp(\"anywhere in the west\")\n",
    "                    }),\n",
    "            Message(\"central indian restaurant\", {\n",
    "                    \"intent\": \"restaurant_search\",\n",
    "                    \"entities\": [\n",
    "                     {\"start\": 0, \"end\": 7, \"value\": \"central\",\n",
    "                      \"entity\": \"location\",\n",
    "                      \"extractor\": \"random_extractor\"},\n",
    "                     {\"start\": 8, \"end\": 14, \"value\": \"indian\",\n",
    "                      \"entity\": \"cuisine\",\n",
    "                      \"extractor\": \"CRFEntityExtractor\"}\n",
    "                                 ],\n",
    "                    \"spacy_doc\": spacy_nlp(\"central indian restaurant\")\n",
    "                    }),\n",
    "            Message(\"hi there!\", {\"intent\": \"greet\", \"entities\": [],\n",
    "                                  \"spacy_doc\": spacy_nlp(\"hi there!\")}),\n",
    "            Message(\"good morning\", {\"intent\": \"greet\", \"entities\": [],         \n",
    "                                     \"spacy_doc\": spacy_nlp(\"good morning\")}),\n",
    "            Message(\"thank you\", {\"intent\": \"thanks\", \"entities\": [],\n",
    "                                  \"spacy_doc\": spacy_nlp(\"thank you\")}),\n",
    "            Message(\"good bye\", {\"intent\": \"thanks\", \"entities\": [],\n",
    "                                 \"spacy_doc\": spacy_nlp(\"good bye\")})        \n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<rasa_nlu.training_data.message.Message at 0x10971c978>,\n",
       " <rasa_nlu.training_data.message.Message at 0x109fd02b0>,\n",
       " <rasa_nlu.training_data.message.Message at 0x11a1d27f0>,\n",
       " <rasa_nlu.training_data.message.Message at 0x11a1d2208>,\n",
       " <rasa_nlu.training_data.message.Message at 0x11b088a58>,\n",
       " <rasa_nlu.training_data.message.Message at 0x11b088a20>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/rasa_nlu/training_data/training_data.py:194: UserWarning: Entity 'cuisine' has only 1 training examples! minimum is 2, training may fail.\n",
      "  self.MIN_EXAMPLES_PER_ENTITY))\n"
     ]
    }
   ],
   "source": [
    "training_data = TrainingData(training_examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_intents = set([example.get(\"intent\") for example in training_data.intent_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greet', 'restaurant_search', 'thanks'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dict = {intent: idx for idx, intent in enumerate(sorted(distinct_intents))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greet': 0, 'restaurant_search': 1, 'thanks': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_intent_token_dict(intents,\n",
    "                              intent_split_symbol):\n",
    "     \"\"\"Create intent token dictionary\"\"\"\n",
    "\n",
    "     distinct_tokens = set([token\n",
    "                            for intent in intents\n",
    "                            for token in intent.split(intent_split_symbol)])\n",
    "     return {token: idx\n",
    "            for idx, token in enumerate(sorted(distinct_tokens))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_intent_dict = {v: k for k, v in intent_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'greet', 1: 'restaurant_search', 2: 'thanks'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_intent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = {'hidden_layers_sizes_a': [256, 128],\n",
    " 'hidden_layers_sizes_b': [],\n",
    " 'batch_size': [64, 256],\n",
    " 'epochs': 300,\n",
    " 'embed_dim': 20,\n",
    " 'mu_pos': 0.8,\n",
    " 'mu_neg': -0.4,\n",
    " 'similarity_type': 'cosine',\n",
    " 'num_neg': 20,\n",
    " 'use_max_sim_neg': True,\n",
    " 'random_seed': None,\n",
    " 'C2': 0.002,\n",
    " 'C_emb': 0.8,\n",
    " 'droprate': 0.2,\n",
    " 'intent_tokenization_flag': False,\n",
    " 'intent_split_symbol': '_',\n",
    " 'evaluate_every_num_epochs': 10,\n",
    " 'evaluate_on_num_examples': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def _create_encoded_intents(intent_dict):\n",
    "    \"\"\"Create matrix with intents encoded in rows as bag of words.\n",
    "       If intent_tokenization_flag is off, returns identity matrix.\n",
    "    \"\"\"\n",
    "    return np.eye(len(intent_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all_intents = _create_encoded_intents(intent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_all_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(training_data.training_examples)\n",
    "def _create_all_Y(size):\n",
    "    \"\"\"Stack encoded_all_intents on top of each other\n",
    "    to create candidates for training examples and\n",
    "    to calculate training accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    return np.stack([encoded_all_intents] * size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Y = _create_all_Y(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_data_for_training(training_data, intent_dict):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "\n",
    "    X = np.stack([e.get(\"text_features\")\n",
    "                  for e in training_data.intent_examples])\n",
    "\n",
    "    intents_for_X = np.array([intent_dict[e.get(\"intent\")]\n",
    "                              for e in training_data.intent_examples])\n",
    "\n",
    "    Y = np.stack([encoded_all_intents[intent_idx]\n",
    "                  for intent_idx in intents_for_X])\n",
    "\n",
    "    return X, Y, intents_for_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.featurizers.count_vectors_featurizer import CountVectorsFeaturizer\n",
    "ftr = CountVectorsFeaturizer({\"token_pattern\": r'(?u)\\b\\w+\\b'})\n",
    "ftr.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.intent_examples[0].get('text_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, intents_for_X = _prepare_data_for_training(training_data,\n",
    "                                                 intent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents_for_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg = min(defaults['num_neg'], encoded_all_intents.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_embed_nn(x_in, is_training, layer_sizes, name):\n",
    "    \"\"\"Create nn with hidden layers and name\"\"\"\n",
    "    reg = tf.contrib.layers.l2_regularizer(defaults['C2'])\n",
    "    x = x_in\n",
    "    for i, layer_size in enumerate(layer_sizes):\n",
    "        x = tf.layers.dense(inputs=x, units=layer_size,\n",
    "                            activation=tf.nn.relu, kernel_regularizer=reg,\n",
    "                            name='hidden_layer_{}_{}'.format(name, i))\n",
    "        x = tf.layers.dropout(x, rate=defaults['droprate'],\n",
    "                              training=is_training)\n",
    "    x = tf.layers.dense(inputs=x, units=defaults['embed_dim'],\n",
    "                        kernel_regularizer=reg,\n",
    "                        name='embed_layer_{}'.format(name))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_embed(a_in, b_in, is_training):\n",
    "    \"\"\"Create tf graph for training\"\"\"\n",
    "    emb_a = _create_tf_embed_nn(a_in, is_training,\n",
    "                                defaults['hidden_layers_sizes_a'],\n",
    "                                name='a')\n",
    "    emb_b = _create_tf_embed_nn(b_in, is_training,\n",
    "                                defaults['hidden_layers_sizes_b'],\n",
    "                                name='b')\n",
    "    return emb_a, emb_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_sim(a, b):\n",
    "    \"\"\"Define similarity\n",
    "    in two cases:\n",
    "    sim: between embedded words and embedded intent labels\n",
    "    sim_emb: between individual embedded intent labels only\n",
    "    \"\"\"\n",
    "    if defaults['similarity_type'] == 'cosine':\n",
    "        a = tf.nn.l2_normalize(a, -1)\n",
    "        b = tf.nn.l2_normalize(b, -1)\n",
    "    if defaults['similarity_type'] in {'cosine', 'inner'}:\n",
    "        sim = tf.reduce_sum(tf.expand_dims(a, 1) * b, -1)\n",
    "        sim_emb = tf.reduce_sum(b[:, 0:1, :] * b[:, 1:, :], -1)\n",
    "        return sim, sim_emb\n",
    "    else:\n",
    "        raise ValueError(\"Wrong similarity type {}, \"\n",
    "                         \"should be 'cosine' or 'inner'\"\n",
    "                         \"\".format(defaults['similarity_type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_loss(sim, sim_emb):\n",
    "    \"\"\"Define loss\"\"\"\n",
    "    loss = tf.maximum(0., defaults['mu_pos'] - sim[:, 0])\n",
    "    if defaults['use_max_sim_neg']:\n",
    "        max_sim_neg = tf.reduce_max(sim[:, 1:], -1)\n",
    "        loss += tf.maximum(0., defaults['mu_neg'] + max_sim_neg)\n",
    "    else:\n",
    "        max_margin = tf.maximum(0., defaults['mu_neg'] + sim[:, 1:])\n",
    "        loss += tf.reduce_sum(max_margin, -1)\n",
    "    max_sim_emb = tf.maximum(0., tf.reduce_max(sim_emb, -1))\n",
    "    loss += max_sim_emb * defaults['C_emb']\n",
    "    loss = (tf.reduce_mean(loss) + tf.losses.get_regularization_loss())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_batch_b(batch_pos_b, intent_ids):\n",
    "        \"\"\"Create batch of intents.\n",
    "        Where the first is correct intent\n",
    "        and the rest are wrong intents sampled randomly\n",
    "        \"\"\"\n",
    "\n",
    "        batch_pos_b = batch_pos_b[:, np.newaxis, :]\n",
    "\n",
    "        # sample negatives\n",
    "        batch_neg_b = np.zeros((batch_pos_b.shape[0], num_neg,\n",
    "                                batch_pos_b.shape[-1]))\n",
    "        for b in range(batch_pos_b.shape[0]):\n",
    "            # create negative indexes out of possible ones\n",
    "            # except for correct index of b\n",
    "            negative_indexes = [i for i in\n",
    "                                range(encoded_all_intents.shape[0])\n",
    "                                if i != intent_ids[b]]\n",
    "            negs = np.random.choice(negative_indexes, size=num_neg)\n",
    "\n",
    "            batch_neg_b[b] = encoded_all_intents[negs]\n",
    "\n",
    "        return np.concatenate([batch_pos_b, batch_neg_b], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linearly_increasing_batch_size(epoch):\n",
    "        \"\"\"Linearly increase batch size with every epoch.\n",
    "        The idea comes from https://arxiv.org/abs/1711.00489\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(defaults['batch_size'], list):\n",
    "            return int(defaults['batch_size'])\n",
    "\n",
    "        if defaults['epochs'] > 1:\n",
    "            return int(defaults['batch_size'][0] +\n",
    "                       epoch * (defaults['batch_size'][1] -\n",
    "                                defaults['batch_size'][0]) / (defaults['epochs'] - 1))\n",
    "        else:\n",
    "            return int(defaults['batch_size'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _output_training_stat(a_in, b_in, X, intents_for_X, is_training,\n",
    "                          session, sim_op):\n",
    "        \"\"\"Output training statistics\"\"\"\n",
    "\n",
    "        n = defaults['evaluate_on_num_examples']\n",
    "        ids = np.random.permutation(len(X))[:n]\n",
    "        all_Y = _create_all_Y(X[ids].shape[0])\n",
    "\n",
    "        train_sim = session.run(sim_op,\n",
    "                                feed_dict={a_in: X[ids],\n",
    "                                           b_in: all_Y,\n",
    "                                           is_training: False})\n",
    "\n",
    "        train_acc = np.mean(np.argmax(train_sim, -1) == intents_for_X[ids])\n",
    "        return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_tf(a_in, b_in, X, Y, intents_for_X, loss, is_training,\n",
    "              train_op, session, sim_op):\n",
    "        \"\"\"Train tf graph\"\"\"\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        if defaults['evaluate_on_num_examples']:\n",
    "            logger.info(\"Accuracy is updated every {} epochs\"\n",
    "                        \"\".format(defaults['evaluate_every_num_epochs']))\n",
    "\n",
    "        pbar = tqdm(range(defaults['epochs']), desc=\"Epochs\")\n",
    "        train_acc = 0\n",
    "        last_loss = 0\n",
    "        for ep in pbar:\n",
    "            indices = np.random.permutation(len(X))\n",
    "\n",
    "            batch_size = _linearly_increasing_batch_size(ep)\n",
    "            batches_per_epoch = (len(X) // batch_size +\n",
    "                                 int(len(X) % batch_size > 0))\n",
    "\n",
    "            ep_loss = 0\n",
    "            for i in range(batches_per_epoch):\n",
    "                end_idx = (i + 1) * batch_size\n",
    "                start_idx = i * batch_size\n",
    "                batch_a = X[indices[start_idx:end_idx]]\n",
    "                batch_pos_b = Y[indices[start_idx:end_idx]]\n",
    "                intents_for_b = intents_for_X[indices[start_idx:end_idx]]\n",
    "                # add negatives\n",
    "                batch_b = _create_batch_b(batch_pos_b, intents_for_b)\n",
    "\n",
    "                sess_out = session.run(\n",
    "                    {'loss': loss, 'train_op': train_op},\n",
    "                    feed_dict={a_in: batch_a,\n",
    "                               b_in: batch_b,\n",
    "                               is_training: True}\n",
    "                )\n",
    "                ep_loss += sess_out.get('loss') / batches_per_epoch\n",
    "\n",
    "            if defaults['evaluate_on_num_examples']:\n",
    "                if (ep == 0 or\n",
    "                        (ep + 1) % defaults['evaluate_every_num_epochs'] == 0 or\n",
    "                        (ep + 1) == defaults['epochs']):\n",
    "                    train_acc = _output_training_stat(a_in, b_in, X,\n",
    "                                                      intents_for_X,\n",
    "                                                      is_training,\n",
    "                                                      session, sim_op)\n",
    "                    last_loss = ep_loss\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": \"{:.3f}\".format(ep_loss),\n",
    "                    \"acc\": \"{:.3f}\".format(train_acc)\n",
    "                })\n",
    "            else:\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": \"{:.3f}\".format(ep_loss)\n",
    "                })\n",
    "\n",
    "        if defaults['evaluate_on_num_examples']:\n",
    "            logger.info(\"Finished training embedding classifier, \"\n",
    "                        \"loss={:.3f}, train accuracy={:.3f}\"\n",
    "                        \"\".format(last_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholders\n",
      "Embeddings for features and intent\n",
      "Similarity estimation\n",
      "loss estimation\n",
      "optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 300/300 [00:01<00:00, 221.09it/s, loss=0.089, acc=1.000]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    random_seed = None\n",
    "    np.random.seed(random_seed)\n",
    "    print('placeholders')\n",
    "    a_in = tf.placeholder(tf.float32, (None, X.shape[-1]), name='a')\n",
    "    b_in = tf.placeholder(tf.float32, (None, None, Y.shape[-1]), name='b')\n",
    "    is_training = tf.placeholder_with_default(False, shape=())\n",
    "    print('Embeddings for features and intent')\n",
    "    word_embed, intent_embed = _create_tf_embed(a_in, b_in, is_training)\n",
    "    print('Similarity estimation')\n",
    "    sim_op, sim_emb = _tf_sim(word_embed, intent_embed)\n",
    "    print('loss estimation')\n",
    "    loss = _tf_loss(sim_op, sim_emb)\n",
    "    print('optimizer')\n",
    "    train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "    session = tf.Session()\n",
    "    print('training begins')\n",
    "    _train_tf(a_in, b_in, X, Y, intents_for_X,\n",
    "              loss, is_training, train_op, session, sim_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_message_sim(X, all_Y):\n",
    "        \"\"\"Load tf graph and calculate message similarities\"\"\"\n",
    "\n",
    "        message_sim = session.run(sim_op, feed_dict={a_in: X,\n",
    "                                                     b_in: all_Y})\n",
    "        message_sim = message_sim.flatten()  # sim is a matrix\n",
    "\n",
    "        intent_ids = message_sim.argsort()[::-1]\n",
    "        message_sim[::-1].sort()\n",
    "\n",
    "        if defaults['similarity_type'] == 'cosine':\n",
    "            # clip negative values to zero\n",
    "            message_sim[message_sim < 0] = 0\n",
    "        elif defaults['similarity_type'] == 'inner':\n",
    "            # normalize result to [0, 1] with softmax\n",
    "            message_sim = np.exp(message_sim)\n",
    "            message_sim /= np.sum(message_sim)\n",
    "\n",
    "        # transform sim to python list for JSON serializing\n",
    "        return intent_ids, message_sim.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(message, **kwargs):\n",
    "        \"\"\"Return the most likely intent and its similarity to the input.\"\"\"\n",
    "\n",
    "        intent = {\"name\": None, \"confidence\": 0.0}\n",
    "        intent_ranking = []\n",
    "\n",
    "        if session is None:\n",
    "            logger.error(\"There is no trained tf.session: \"\n",
    "                         \"component is either not trained or \"\n",
    "                         \"didn't receive enough training data\")\n",
    "\n",
    "        else:\n",
    "            # get features (bag of words) for a message\n",
    "            # noinspection PyPep8Naming\n",
    "            X = message.get(\"text_features\").reshape(1, -1)\n",
    "\n",
    "            # stack encoded_all_intents on top of each other\n",
    "            # to create candidates for test examples\n",
    "            # noinspection PyPep8Naming\n",
    "            all_Y = _create_all_Y(X.shape[0])\n",
    "\n",
    "            # load tf graph and session\n",
    "            intent_ids, message_sim = _calculate_message_sim(X, all_Y)\n",
    "\n",
    "            # if X contains all zeros do not predict some label\n",
    "            if X.any() and intent_ids.size > 0:\n",
    "                intent = {\"name\": inv_intent_dict[intent_ids[0]],\n",
    "                          \"confidence\": message_sim[0]}\n",
    "\n",
    "                ranking = list(zip(list(intent_ids), message_sim))\n",
    "                ranking = ranking[:INTENT_RANKING_LENGTH]\n",
    "                intent_ranking = [{\"name\": inv_intent_dict[intent_idx],\n",
    "                                   \"confidence\": score}\n",
    "                                  for intent_idx, score in ranking]\n",
    "        return intent, intent_ranking\n",
    "\n",
    "        message.set(\"intent\", intent, add_to_output=True)\n",
    "        message.set(\"intent_ranking\", intent_ranking, add_to_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist(file_name, model_dir):\n",
    "    \"\"\"Persist this model into the passed directory.\n",
    "    Return the metadata necessary to load the model again.\n",
    "    \"\"\"\n",
    "\n",
    "    if session is None:\n",
    "        return {\"file\": None}\n",
    "\n",
    "    checkpoint = os.path.join(model_dir, file_name + \".ckpt\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(checkpoint))\n",
    "    except OSError as e:\n",
    "        # be happy if someone already created the path\n",
    "        import errno\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    with graph.as_default():\n",
    "        graph.clear_collection('message_placeholder')\n",
    "        graph.add_to_collection('message_placeholder',\n",
    "                                a_in)\n",
    "\n",
    "        graph.clear_collection('intent_placeholder')\n",
    "        graph.add_to_collection('intent_placeholder',\n",
    "                                b_in)\n",
    "\n",
    "        graph.clear_collection('similarity_op')\n",
    "        graph.add_to_collection('similarity_op',\n",
    "                                sim_op)\n",
    "\n",
    "        graph.clear_collection('word_embed')\n",
    "        graph.add_to_collection('word_embed',\n",
    "                                word_embed)\n",
    "        graph.clear_collection('intent_embed')\n",
    "        graph.add_to_collection('intent_embed',\n",
    "                                intent_embed)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(session, checkpoint)\n",
    "\n",
    "    with io.open(os.path.join(\n",
    "            model_dir,\n",
    "            file_name + \"_inv_intent_dict.pkl\"), 'wb') as f:\n",
    "        pickle.dump(inv_intent_dict, f)\n",
    "    with io.open(os.path.join(\n",
    "            model_dir,\n",
    "            file_name + \"_encoded_all_intents.pkl\"), 'wb') as f:\n",
    "        pickle.dump(encoded_all_intents, f)\n",
    "\n",
    "    return {\"file\": file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(meta, model_dir, model_metadata, cached_component, **kwargs):\n",
    "\n",
    "    if model_dir and meta.get(\"file\"):\n",
    "        file_name = meta.get(\"file\")\n",
    "        checkpoint = os.path.join(model_dir, file_name + \".ckpt\")\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            sess = tf.Session()\n",
    "            saver = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "            a_in = tf.get_collection('message_placeholder')[0]\n",
    "            b_in = tf.get_collection('intent_placeholder')[0]\n",
    "\n",
    "            sim_op = tf.get_collection('similarity_op')[0]\n",
    "\n",
    "            word_embed = tf.get_collection('word_embed')[0]\n",
    "            intent_embed = tf.get_collection('intent_embed')[0]\n",
    "\n",
    "        with io.open(os.path.join(\n",
    "                model_dir,\n",
    "                file_name + \"_inv_intent_dict.pkl\"), 'rb') as f:\n",
    "            inv_intent_dict = pickle.load(f)\n",
    "        with io.open(os.path.join(\n",
    "                model_dir,\n",
    "                file_name + \"_encoded_all_intents.pkl\"), 'rb') as f:\n",
    "            encoded_all_intents = pickle.load(f)\n",
    "\n",
    "        return cls(\n",
    "            component_config=meta,\n",
    "            inv_intent_dict=inv_intent_dict,\n",
    "            encoded_all_intents=encoded_all_intents,\n",
    "            session=sess,\n",
    "            graph=graph,\n",
    "            message_placeholder=a_in,\n",
    "            intent_placeholder=b_in,\n",
    "            similarity_op=sim_op,\n",
    "            word_embed=word_embed,\n",
    "            intent_embed=intent_embed\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        logger.warning(\"Failed to load nlu model. Maybe path {} \"\n",
    "                       \"doesn't exist\"\n",
    "                       \"\".format(os.path.abspath(model_dir)))\n",
    "        return cls(component_config=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'embedding_intent_classifier_exampledataset'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save output\n",
    "model_dir = '/Users/varunn/Documents/NLP-data/'\n",
    "file_name = 'embedding_intent_classifier_exampledataset'\n",
    "persist(file_name=file_name, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = Message(\"show me some indian restuarants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'show me some indian restuarants'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr.process(test_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_features': array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'text': 'show me some indian restuarants'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'restaurant_search', 'confidence': 0.9497399926185608},\n",
       " [{'name': 'restaurant_search', 'confidence': 0.9497399926185608},\n",
       "  {'name': 'thanks', 'confidence': 0.09301461279392242},\n",
       "  {'name': 'greet', 'confidence': 0.07009059935808182}])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(test_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
